{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sy = pd.Series(cancer.target, dtype=\"category\")\n",
    "y = cancer.target\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAygAAALICAYAAACUx9THAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzs3Xl4lOXV+PHvPWtmsk3IwhYiFBVNLbK50rdqta4o5QVcwa0KFO3ia9X+2tJasQsutbVWwaWIgIqiFPe1RVtoVRCligIqCEHIRibLzGS25/798WQmmWQCCZlJQnI+15VryGRmnsfL+1nO3Pc5R2mtEUIIIYQQQojewNLTOyCEEEIIIYQQMRKgCCGEEEIIIXoNCVCEEEIIIYQQvYYEKEIIIYQQQoheQwIUIYQQQgghRK8hAYoQQgghhBCi15AARQghhBBCCNFrSIAihBBCCCGE6DUkQBFCCCGEEEL0Gn0yQDn77LM1ID/y09GfHidjVn46+dPjZMzKTyd/epyMWfnp5I/oQX0yQKmqqurpXRCiU2TMikONjFlxqJExK8Sho08GKEIIIYQQQohDkwQoQgghhBBCiF5DAhQhhBBCCCFEryEBihBCCCGEEKLXkAClO2kNNV9CNNLTeyKEEEIIIUSvZOvpHehXXr4Z3n0QhoyDq14Cu6un90iILjMMTbUvRCgSxWGzkp/pwGJRPb1bQogOkmNYHEpkvPYPEqB0l8qtZnAyYCR89T6s/ROc+tOe3ishusQwNFvK67n2sfWU1QQoznPx0OUTGDUwWy4YQhwC5BgWhxIZr/2HLPHqLv99CpQFzrgVSk6CdxZBONDTeyVEl1T7QvELBUBZTYBrH1tPtS/Uw3smhOgIOYbFoUTGa/8hAUp3+WINFI4CVx6MOhcC+2Db6z29V0J0SSgSjV8oYspqAoQi0R7aIyFEZ8gxLA4lMl77D1ni1R3CjbDnQzhqkvn7oNGQ4YFPnoPSC3p234ToAofNSnGeK+GCUZznwmGzpmV7svZYiNRyOawsvvI43A4r3kCYhWs+p7IhmLZjWIiucNisnFlaxNTxw/C47HgDYZ7ZsEvGax8kAUp3qNgM0ZA5gwJgscLgY2H722ZlLyU3WOLQlJ/p4KHLJ7RZD5yf6Uj5tmTtsRCpZRia8rog81Z/FD+m7pw2moE5GWk5hoXoqjyXnR+efiRzlm2Ij9mFM8aT57L39K6JFJMlXt2hcov56Dms+blB34CGctj3Rc/skxApYLEoRg3MZtXciay95TRWzZ14wIDBMDSV9UF21/iprA9iGLpD25K1x0J0Xcvjb29dY5tj6qaVm8jKsEnQL7pVR68LNYFwPDgBc8zOWbaBmkC4O3dXdAOZQekOlZ+CxQbZg5ufG3iM+bjjn5A/smf2S4gUsFgUhdnODr22K7MgsvZYiK5pffytnHNS0mMqHDF6aA9Ff9SZ64JcB/oPmUHpDlVbIWeIubQrJmcouAbAjrU9t19CdLOuzILE8l1aSme+ixB9Tevjr9oXkmNK9LjOXBfkOtB/SIDSHbw7IWtg4nNKQcGR8NXGntknIXpAV779iuW7xC5O6cx3EaIvan38LVzzOQumjpZjSvSozlwX5DrQf8gSr+5QW2b2PmltwAgoe9fshyJd5UU/0JWqXy3zXaSKlxCd1/r427jLy5J123lq9kloreWYEj2iM9cFuQ70HzKDkm4hPzR6Iauw7d88h4E2mpPohejjuvrtVyzfZWiem8Jsp1yUhOiEZMffDd8ZxaCcDDmmRI/p7HVBrgP9g8ygpFvdbvPRnSRAyRtuPpZ/DEPGdNsuCdFT5NsvIXqOHH+iN5JxKZKRACXdaneZj5kFbf+WPRisDjNAEaKf6EzVLyFEasnxJ3ojGZeiNVnilW61TTMomUlmUCxW8JRA+Ufdu09CCCGEEEL0UjKDkm51uwEF7vzkf88dJjko4pBiGJpqX0im4oXo5eRYFT1NxqA4WBKgpFvtLnDlgdWe/O85Q+CLNRBuBHtGt+6aEJ3VlUaLQojuI8eq6GkyBkVXyBKvdKvdnTz/JCZ7EKDB+2W37ZIQB6srjRaFEN1HjlXR02QMiq6QGZR0q99rdoxvT/Zg83HfF1A4qnv2SYiD1JVGi50lSwOEODiGoQmEI912rAqR7HzdndcL0ff0mhkUpdQwpdQ/lFKblVIfK6V+1PT8rUqp3UqpD5p+zu3pfe0UfxW4PO3/PR6gbO+e/RGiC2INtVrqaKPFzogtDZhy/1omLvgHU+5fy5byegxDp3Q7QvQ1sWPn8wpftxyrQrR3vrbbLDIGxUHrNQEKEAFu1FqXAicC1ymlSpv+do/WekzTz0s9t4udZBjg3wfO3PZf48wBuxtqJEARvV9XGy12lCwNEOLgxI6de9/cxoKpo9N+rArR3vnaZlHdcr0QfVOvWeKltd4D7Gn6d71S6hNgaM/uVRc1ekFHwbWfAEUpcxZFZlDEIaC7GmrJ0gAhDk7s2CmrCXDXq1uYN6kUj8tOcZ6LwbkuWSYpUq6983UgFJUGjOKg9ZoApSWl1HBgLPAOMBG4Xil1ObAec5alJsl7ZgGzAEpKSrptX/fLV2k+7m8GBcxE+X1fpH9/RK/SK8dsB3SloVZH80piS8laXvRkaUDPO1THbF/X8rhSSnFmaRGvba5g4y4vs5duoDjPxaq5E/vljaGM2fRzOawsvvI43A4r3kCYhWs+p7IhiMNmlQaM4qD1ugBFKZUFPAP8WGtdp5R6AJgP6KbHu4GrW79Pa/0g8CDAhAkTesdCdV+V+bi/GRSArIGwez1obc6oiH6hV47ZNOpMycnYUrLWr5WlAT2rv43ZQ0Gy42rhjPEAvLa5ot8fOzJm08swNOV1Qeat/ig+/u6cNpqBORn9dsyJ1OhVAYpSyo4ZnCzXWj8LoLUub/H3h4AXemj3Oq+jMyiZBRAJgr96/yWJhTiEtJ4t0eik65RXzZ3Y5hu27lpKJsShLtn6/znLNvDU7JP41flajh2Rcq1n7FqPv5tWbuLZuSfLmBNd0msCFKWUAh4BPtFa/6HF84Ob8lMApgAf9cT+HRR/0wxKxgECFHdTUFJbJgGK6BOSfau77HsndCqvRJYGCHFg7a3/11ozNM/dQ3sl+qrW5/aVc05KOv7CEaOH9lD0Fb2pitdEYCbw7VYlhe9QSv1XKbUJOA24oUf3sjNiS7wycvb/usxC87Fud3r3R4hukuxb3e1VUvZUiFRTSiU9rpQsFxZp0PrcXu0LyXldpEWvCVC01v/SWiut9eiWJYW11jO11t9oev6CFrMpvZ+vChzZYDnARFVs1qRWAhTRNyT7VvfeN7exaMZ4KTkpRApZFW3KCS+YOhqrxCciDVqf2xeu+VzKWYu06DVLvPokX+WBE+TBXAJmsUNdWfr3SYhukKwKV2VDkMGeDMkrESKFLBYLS9Ztj5cT9gbCLFm3nd9MGd3Tuyb6oNbn9o27vCxZt52nZp+E1pLzJFJHApR08lWZjRgPRFnAnQ91X6V/n4ToBu1V4fK4Dnzh6mgpYiH6i/0dE/mZDm74ziipeCe6RX6mg8euPp4vq/24HVb8oSiH5bsZlJMh52mRUhKgpJOv0gw8OiKzwEySF6IP2F8Vrv3dbHWkFLEEMKK/MAyNNxBij7eR2cs2xI+JRTPHU5DpwGKxkJ/pkIp3Iu0iEYOKhiCgCYSiCWWFH7p8Qk/vnuiDek0OSp/krzpwgnyMBCiiD2kviIgFIFPuX8vEBf9gyv1r2VJej2GY7QmSJddf+9h6qn2h+OduKa/n56s28dFXdXxZ7aOsxk9EKsaIPsYwNDuqfXj9YQLhKPMmlTJ2mIeymgCzl27gg7La+PEDUJjtZGiem8JspwQnIqXC4SgVDY0EI1FCEU1FfZDCLLPCYutztBCpIjMo6WJEwb8PMjwde727AHasBcMAi8SN4tC1v1mQ9gKQWC+U9kqmxkoRV/tC3PP6Fq44eQS3PLMp4RvlowflyI2Z6DO8gRDVDUFueOrD+DhfMHU0d726hY27vHhc9v32EhIiFQxD82WNn8r6IDet3JR0LO6vXLwQB0vuhNMlUAPoA/dAicksBCMMvoq07pYQ6ba/WZADBSCxBMyWWpasDEWiTB0/LB6cxN4/e+kG+QZP9CmBUDQenIA5zm95ZhNzTh1JcZ4LbyAcf15uDkW6VPtC7NoXiAcnkDgWQcoKi/SQACVdYl3kO7PEC6TUsDjk7S8IOVAAEkuub69kZWy5WGcaPgpxKIpqnXSc52c6eOCycSxc8zkgN4civUKRKG6HNelY9LjsUpRBpI0s8UqXeJPGDi7xijdrLAPGp2WXhOgOyUoMx26i2qvuFbu47S+5HswAJhCKtPv5QvQVGfbkx5HH7aDGF2LjLq/cHIq0c9jMSl3JxmJxnotVcydKUQaRFjKDki7+WIDSwSVe7qYZFCk1LA5xeS47i2Ymb8jYMgBZe8tprJo7MaFCF5hBSnsJvxaLYkiuq93PF6KvKMh08tDMxNnEO6eNxgKMLMps9/gR4mAYhqayPsjupnyTWOGS/EwHh+W7uXNaq2aMMycwONclRRlE2sgMSrr4OhmgOLPB6pRKXuKQZhiabZUN/OmNrcybVEp+poOibCdDcl3xi1gsADlYNpuFowflSFlV0adZLIojirJ44toTCUUMLAqqGkKgwONyMCBTxrtIjQOVdx+en4nHbWfFrBOJasiwWyjIlMBEpJcEKOkSC1A60qgRQCnIzIf6PenbJyHSrGWC/GubzYIPsWUAsaAkFX1MuhrkCNFbtTw+lFLMf+Hj+LEEbY8nIbrKGwixt7aRu6cfizcQZuGazxOqw1ksigGZTsjs6T0V/UlaAhSl1B3A7UAAeAUYDdygtV6Wju31Sr5KMzixdGJdvEu6yYtD24GqdHWkEaMQ/VWy42PB1NFU1ps5JyAFIURqGYZmj7cxofFirISwjDPRk9KVg3Km1roOmATsAA4HbkrTtnonf1XHl3fFZOZDnVTxEr1be2uVof0ywS6Hlcr6IHtqzZLDhVlOFs0cz93Tj2VvbSPegJQIFv1Py2Npny/I3rpGfMFIQlPGluVcQQpCiNSIjb0yr79N48VbntnEPReNQSmVcH4XojulK0CJzcycBzytta5N03Z6L19l5wMUdz7U7wUtJwTROx2oE3yyMsGPXX085XVBpty/lrKaAIVZTn5y1iie2bALbyBMdoaN+saIdIMX/UrsWPr5qk1sr/JR4w+zZW89v3nxE+a/sJmfnDUqHqQMyskApCCESI2W5/Fv3bGGJ979krsvPJZVc09m0czxFGY5Ka9r5MJF/044vwvRndKVg/KCUupTzCVe31dKFQKNadpW7+Srau5t0lHufIiGwF/d+fcKkSL7yxE5UCf4ZGWCNZrL719HWU0AbyDMD08/giXrtks3eNGvVftC/O39XVx32hFc9/j7bZbX3PLMJuZNKmX+C5spyHKw9pbTpCCESIkqXzB+Hh87zMMVJ4/g8r++Gx+Dd04bjdHUh6fl+V2I7pSWGRSt9U+Bk4EJWusw4AMmp2NbvZavEpydnUGRUsOiZx1ohuRAOSbQtkxwOGLE37NwzeeU5Lvb7Qa/t65Rvq0TfZ5haCJGlIuOPywenEBih+5YU8YFU0djs6ikZbeF6CzD0PiDzefxOaeObHMuvmnlJqJN52HJeRI9JS0BilLqcsz8k8ua/j0NODMd2+qVohEIeMHVwSaNMe5881ECFNFD2pshqfaZOSIH6gTfmmFolFKsnHMSi2aaDUj3eAPtdoP/yhs44JKC/eXACNHbGYZmR7WP2D3f/jp057rsLFm3HYtFWpaJromdN/fUBqisD7L4yuNYMetERhZmJh2Ddqs55iTnSfSUdJ31jmvx8z/ArcAFadpW7+OvBnTHu8jHxAKUeglQRM840AxJshyT9tbEx2ZjLlz0b6Yt/Hd8Xf0/t1ZQmO1MGujkuuz4gpH4TErrYCQSMfY7wyNEb+cNhCiva+SSh/7DtoqGpMeBBhZMHc2dr37KDd8ZJTknokta5jtFDU1+ljmefv/yp+zaF0g6Br2BsOQ8iR6VlhwUrfUPWv6ulPIAT6ZjW72Sr9J87GySvCsPlAXqpBeK6BmxGZKWQUrLb9AsFsURhVk8NfskwlEDu9VCUVbyZSfJZmNueWYTT80+icJMB4tmjmf20g3xdc/3XzaOO1/9lNc2V8ST64MRI6Hk6qKZ4/nTG1vbzYHpiFT0YRHiYBiGJhCKctNKc0nNwjWfc/f0Y7nx6Q8T1v8XZDnwuOz8ZspoGZ+iy6p9Ie55fQuzvjWSSx9+JyHfafXG3dw5bXR8TBbnuVg0YzwFWQ5WzZ0o40/0mO5q1OgDRnTTtnpePEDp5AyKxWoGKbLES/SQ2AxJ6z4lsW/QYp3iWwcNo4qysdkSJ2RjszEXji/m2m99DZtFYbVasFmg0heiOC+DJ2edSDBsgIIFL38Sb0hXVhPgy2p/vDZ/7LnZSzcwb1JpQuO6wiwnoUiU3TX+hIAjWSACSB8W0e0MQ1MTCFLjC2O1KB6+fDxupx2L0igUj151PFYFe+saueOVLVQ2BCUxWaSEYWjCUYOfn1fKF5U+CrOclNUE4l8YzZtUyh2vbGHZ905AA5lOq3SJF71Cuho1Pg/E1lxYgFLgqXRsq1eKBSidzUEBM1FelniJHpKsCteBqnjNXrqBx685geI8d8JFzWGzMvt/hnPesUO56tH3EmZKXvxwN+ePKeYfn5SzYkMZS64+PiHoAHA7rEmXm8UCjbHDPNx45pEM9rjYWt7AvW9uo7IhyEOXT+CIwqw2gdRDl09gYI5zv1XIhEi12PIahabGH2bxWrOC3R9e39qmkt2CqaMBSUwWqWEYmi1767l2aWLjz7te3cLGXd54vlNlQ5At5fXMf2Ezq+ZOlOBE9ArpykG5C7i76ed3wLeaKnv1Dwc7gwLgHiDNGkWPaJlECTA419WmalB7OSoV9cF4In0kYvCVN0A4GmXmySOYuzyxStHc5e8zbUIJc5Zt4IKxQynMcrKz2t9mHbQ/FE26Nroo28ns/xnO3RceS4bdymcVDTzx7pf85KxRFGaZAUhFQzBpIBIIHbgKmRCp5A2ECEcNsjPs3LRyU7yCXbJKdrEKXpKYLFKhsiEYD04gcYyBeT71h6IsmDqahWs+l3Oh6FXSlYPy1sG8Tyk1DHgMGIg5A/Og1vpPSqkBwApgOGZn+gu11jWp2ds08FWCxQaOzM6/150P5R+nfp+E2I/Yt7wHWvrUXo5KtS/E4NwMIhGDT8vrmbPMzC35x09OSRoQWC2KshqzmsycU0dy75vbeOCycfz579uYOn4Y+ZkOBuY4WTRjPLOXbUjYp8E5GVwwpjihbv+CqaN5e0s5d0wbzT6feUOYbLtRzX5zbIRIJcPQ7POFyHXZiRiaeZNKGZKbEf/mur0ZwoUzxpPnsvfQXou+IByO4g9F9lslbuGM8WRn2Pjxkx+wcZdXzoWiV0npDIpS6l9Nj/VKqboWP/VKqboOfEQEuFFrXQqcCFynlCoFfgq8qbU+Aniz6ffey1dpzp6og5gmdRdAsA6CDanfLyHacaDywjH5TcntLat4LZg6mmc27MJhs1LREIwHJ2OHebBaLElnQaKGjgc2sSUGhtZcd9oRzH9hM9MW/ptLHnqHiGHw9OyTWHvLaayaO5FRA7PxNkbiQUtsX5es2865o82lZBc9+B++qPQl3W6G3dLhKmRCdJU3EKLGF+Kyh9/hlDvXMP+FzRgaziwtildJailWye7eN7dSEwj30F6LvmCf3zx3Jxtjg3IzmD/5GAqyHPzmxc3x4ETOhaI3SWmAorX+ZtNjttY6p8VPttY6pwPv36O1fr/p3/XAJ8BQzCaPS5petgT4bir3O+UaKjtfwSsmXmpYKnmJ7tORBozQlKNSlM3j15zAyjknMW9SKUvWbY+XQo3NXIwd5uEnZ41i+b+3c/9l4xICgvsvG8fK9TvjgU1siUF9Y6RN07rrHt+IoRNLCCfb16njhyW89943t3HntNFtApGCTGc8x6Zl0CNrrkU6NIYNbnjqw1Zj+n1+es7RPLNhFwumjm4T7N+8chOvba6QpTbioAWDESobQlTVh9qcB++cNpqKuiCF2U6cdsXtU74h50LRK6V0iVfTUqx2aa33deKzhgNjgXeAgVrr2B37XswlYK1fPwuYBVBSUtLRzaSHrwsBSmaLZo0FR6Run0Sv05vG7IHKC7dks1koznPjctgYlufiG0Nz4xWIbBZFcZ4roTtxjT/C4iuPw2pROG0WAuEo44bns2Tddn5w+pHkumzsrPYzxONi3qRSFq75nI27vEBTha6oQUVdEH8oymH5brIybG32tXXjx427vNzxyhaenHUiqum/r2Wyf+uEeCk93DG9acz2do2NkXaXGlotinmTvo7NAitmnUhDMMKOan88eVmW2qROfxuzwWCEKn+IOcs2cPf0Y7njlS3Mm1SKx2XHGwhzxytb+NPFY8jLtJLpkGpdovdKdQ7KBszcEQWUADVN//YAO+lgqWGlVBbwDPBjrXWdarFUSmutlVJturJprR8EHgSYMGFCz3Zt81UefHAh3eT7jd40Zg9UXrg1i0WRn+lgR7WP6oYQHreNsppGCrKdLL/mBLz+cPzG7KkNZTy1oQyA1ddNZG9dIx6Xnanjh/HCB2VcMKaYnz773zZVZgBuPnsUMx9pzjW5c9poBmRmt9nXWOPHljeDlQ1BtpU3MCg3g1EDXe1eiDuafyN615jtzcLhKGV1AZztBP6G1tQ1RrAAg3Kd1DXC/Bc2d+jYE53Tn8ZsOByl3BfE0GYg7A2EqWwIMnvphvhrivNc2K0WCU5Er5fSAEVrPQJAKfUQsEpr/VLT7+fQwWVZSik7ZnCyXGv9bNPT5UqpwVrrPUqpwUBF+5/Qw7Q2A5Ti4w7u/dJNXvSA9soLA1TWB5POLMQ6YtssFqoaQvGeJcV5LpZfc0LSG7O9dY0JF8tFM8e3ySeJ1eZ3WC3x5mGxv920chMrZp3IwBwnK2adSFRDht3CAFfbACsW6Byop0R7+TdSelgcjEjEoMoforI+iM1iadME785po8mwW3nynS+5YGwxeW4neW5nu6W9heiIcDhKpS+I1x+muiFkJsGv+ZwFU0cnlLK+9+KxWBQyvkSvl65GjSdqra+N/aK1flkpdceB3qTMqZJHgE+01n9o8afngCuA3zc9rk7x/qZOsA4ijQdXYhjAlgHObJlBEd3OYlEJN+QHmlmIdcReevXxzGyqqAXmDf5vXtzMwhnj4wnzse7Ef3pza8I2Wy/Nir3/iKKseKWv1n+LGpraQJiooalqCFGU7YznlqyYdWL8m8PYchlgv+v5O5p/I8T+GIamyhckHDEwNNy0chOFWU5+dUEp8ycfg9thxR+KUpjtpKo+yLjh+RTsZ9mhEB1hGJqqhiARwyAYMZi7/H0Ks5zxwOSuV7cwf/IxDC/IxGlT3Prcx/xmyuie3m0hDihdfVC+Ukr9Qik1vOnn50BH7rgnAjOBbyulPmj6ORczMPmOUmobcEbT771TXVOqTCyX5GC485s/R4gecqDKXlGtm0r36jY3+K9triDHZWPepFL+fuMpPH7NCQzyOPnB6UcmJGzGlma1VJznYltFA1onr0AT1ZqZj7zLGX94m588/SHVDUG8gRAWi8Jhs3Lj0x8ye+mGeHByoPX8sfyb1tuRHADRUbFg/her/mv+3nRMbNzl5dfPbSYUNQA4vCgLp83C+fetZf4Lm7FY0nUJFv1BfNz97b8EIwZRo3nc3fWqmXvy03OOYniBm/rGMFGDeEETIXq7dJ0dLwEKgVVNP0VNz+2X1vpfWmultR6ttR7T9POS1rpaa3261voIrfUZnUm273axJovugoP/DNcAWeIletyBZhYy7OaN/d7axqQ3+JGoJj/TgdNmYXBOBrX+CFprnp5zEv+65TSWXXMCtYEwj1yRWPY3Vt1rT22Au6cf26YC2O9e+iQhaLrhqQ8JhMx9iuXSdKaM8MG8R4iWYs0Yfz35GDTEi0WAWbBh9tIN3Pj0hwD8+c3P4jOKMsZEV1T7Qtzz+hZ+eX4pNoslnvMEbced22Elx2WV3DpxyEhXo8Z9wI/S8dm9XmxpVmYXApTMAvjqg9TsjxAHqb3KXkopdtf4sVstLJo5nj+9sZW7px/LjU9/GF/O9cBl4/j9y5/w2uYKivNcPHrVcdT4Qjz0zy+44uQRCWui/3jRGJ649gTK68xu9EvWbeeKk0dwxytmonysAs0QjwuvP8RrmxNT0GINGKH9XJr9XZAP5j1CxMSaMWqt2VHliy/tuufCY+MlhovzXCyaOZ4sp4W5px3OOd8YzGBPhowx0SWGYfCjM45k175AfNy1znlaNGM8ORk2cpx27HaZFRaHjrQEKEqpQuBm4OtARux5rfW307G9XiUWoLj2W3F5/9z5ZqJ9NAxW6SYsekayyl4LZ4zn1uc+orI+xA9PP4LhBW5+df7X8YfMUsKNEYNcl53bX/g4HkiU1QTYtS/AvNUfMW9SaTw4if3txys+4K7pxzIkN4PBuRl8Y+gx3PrcR/ElWrOXbmgKco6nqin5s3XQlGFvngxunUvTEQfzHiEAqnxBanxh8jIdhKM6Xir7ty99yvzJx/C1wkzCUU2Oy8bCNZ9z4shCBuVm4HHJ7InoGqUUTpsVuzUaH3d3vGLmnIwozMRhUTgdFvJcUrFLHHrStcRrOfApZlnhXwM7gPfStK3epW43uPK6Fli48wEN9XtTtltCdFbLmYW1t5zGU7NP4t43t1JZH+InZ41i3uqPOO2ut7jowf/QGDZw2S247BYMQ1NZb+apjB3mYdHM8RyW72bepFKG5GYkXTY2xJNBcZ6boXluBuVkcMN3RrVpLhYxohRlO9o2YJxpNmAUortFIgaGocnLtLNrn5+ooXFYLfzqglIArnr0PSrrg/hDER55+3MuGFvMscNyZZmN6LJgMEJdY7jdcac1uCQ4EYewdFXxytdaP6KU+pHW+i3gLaVUPwlQvmouFXywWvZC8Qzr+j4JcZBi/U6qfSH8oQivba5g0czxbWZBvr/8feZPPoarHn0vHlAR4Zm+AAAgAElEQVSsen83k8cOTVjO9ZdLx3FmaVHCMq3iPBc2iyV+EY0FRk/NOhF/OIpVKfbWNfL/nvkIgIUzxiWUGC7IlAuw6H6RiMGn5fUEQlEihpFQZvvOaaO5+exR3LRyE4NzM7Ao+N63RspYFSkRDEaoDpilrNsbdw6rItspS1XFoStdMyjhpsc9SqnzlFJjgS6seTqE1O3ueoCSWdj8WUJ0kjmDEWR3jZ/K+iCGcfC9yWJVYqbcv5avvAEWX3kcRxRlJZ0FcTus8X/ftHITs04Z2SaQue7x9/nZuaVtEt8d1iQXUaXw+sNsrWjgjlfMksGVDUEsFgtD89yUDHBTlC3r+EX3MnNOgpTXN7J+exWDPRlJ+/UMys3ggRnj8frD2KxWGasiJRobI3gbw/FS1snG3cIZ48l2WbHZpEqcOHSlawbldqVULnAj8GcgB7ghTdvqXer3QN6Irn1GLMG+tqzr+yP6lU51RTcM8FdCJAQ2B7gLoVXZ01ip4cIsJxal4nkkyfJAvIFw/PeymgAOa/I+JsFIlGXfO4H6YISvvAHu+/s2br3gmP3+NyyYOpol67a3WyLTMDTVvpAkuYu0MgzNjmofvsYQR2YHufRoG1FqKcyyJ4z1spoACsWf39zKD08/kjyX5BKKrgsGI9QEg+REa3HoMH+aNITb11SycVcdYI47i1IMznGS5ZAcJ3FoS3l4rZSyAkdorWu11h9prU/TWo/XWj+X6m31OiE/BGq61gMFwJ4JdpfMoIhOO1DvkjjDgIrN8PAZ8MdjzMeKzWAYCTMwgXCEspoAc04dGa/SFetO3DpHZOGaz+MfX5znwtKi1GrL510OGzMeeYevvAFmL93Aa5srsCri29xb18g9r29p013+1guOSRpotZzlmbjgH0y5fy1byuu7NHMkRDLVvhA1DUGGRXbgfPRMHH8ejWvJmSw+N5Oxw3LirzMDeD+vba5gzrIN1LQI3oU4WL5QmCL/F2Q+dhb2P49m/OvTefjs5rFXnOfCohQ5GXb5gkYc8lIeoGito3Sg50mf5N1pPsaWaB0spcxvs2UGRXRSh7ui+yvhyUuax6x3Jzx5CdpXmXCz/3mFj+I8Fx5X8zfELZuAvX3TqTxx7Ym4HVYqG4IA8TLDStEmkFkwdTQKs5mYp+lb5TNLi6jyheLbvHDRv7ni5BGMHeZJ+G+IGJo9tYE2y9Y6HJT1kFQuuRM9KxSJMioniGf1FQnHjmf1Fdx2xiCAeOnsu1/bCrRz/AnRCYahKa9rJCvqxbri0oSxl//8Ffzi1MJ4KetB2U4pJyz6hHQt8VqrlLoPWAH4Yk9qrd9P0/Z6h+rPzMecoV3/rMwCmUERndZe75I2XdEjoeaLXIx3J0YkyLWPfRx//71vbuPOaaPxh6IJn7txl5f5L2zm2bknU5DpxO20sGLWiYSiZi+IX67+mDunH8uSddvjfUy8gTBL1m3nprOOojjPxYBMByvnnMSg3AwufvA/bWZM5k0qZfbSDYwd5uGHpx9BKGLweWUDz2zYxQ3fGRWfTelwUNYDOrXkTvRqhqGx2yxkhMNJj52RA+ysnHMSHreDyvrGeJnspMefEB1kGJrdXj+GBotKft4+ZqCLJ2edyKDsDMk7EX1GukbyGMweKLcBdzf93JWmbfUe1dvMx1QFKDKDIjqpw13RbQ7wlCQ+5ykhohLX0m/c5eWOV7ZQOjibRTPHt1nW1dAYadpuBg6blZmPvMNVj77Hxl1eHnzrc67/9hHMf2EzFz34H+a/sJnrv30EK9fvZOGM8YSiBtW+ENUNoaQBRn6mg7HDPNx8tlnS+Iw/vMX8FzZzxckjuOf1LXibqthEtWbxlcclzLj0lpvC3j67IzrGMDTl9QECoSiN2pb02Pl8X5hgxOCmpz/EbjUvre0ef0J0UF1jiNpAhN+8uJmIaue8bbEzMMspwYnoU9LVSf60dHxur1f9mdkDxZHZ9c/KLDSbNUaCYJMeD6JjOtwV3V0IFz/RvMzLUwIXP4HfntdmBqayIUgwYuAPRpk/+RjcDiveQJg7XtlCZUOQVXMnUpjtxDCMhNmSNzeXo7XmyVknEjU0VovCZoGZJ49g6brtLPrnDorzXDx29fFJZ31yXXb+cOGxzPzru21mV+6cNpo93kZmL9uQUGIztk+95aawN8/uiI6rbQxS3RBmzrINFGbZWTx5SfMyL08J4enL8TV6uKtp/A3OzWDtLadJwQbRZf5glHvf3MoVJ4/gY69iZKuxF5q+nKA9j0xZ1iX6mHQt8eqfqj5LzewJgLupklfdbhjwtdR8pugXOtQV3WKBolK45o2EKl65qDbd4xfNHM/tL27me9/8Glc92radUSgSJRIxqGoI8eyGncwan8PoLAvfPL2QP//7K97Z4Y3X5m9ZlevdHV427vLy+5c/4S+XjuO6x9+P//2By8Zx88pN/PSco5Le4A/KzWDmI4mBy00rN7Fi1om96qaww0vuRK9lRKO4QzXkR+rjVZOuesnHbec8y5H5DiIWO//3Qhmvbn43PmMyONfVK8afOLRFwhHyqeWP5xTy+b4alm/0ccmJw9l1zrN4HAbekAUrBQyyyMyJ6HskQEml6m0wdHxqPiurKdG+VgIUkSYWC2QNTHwK2szAGIbBa5srmDp+WNKbbbvNwle1Ae59cwu/nWgj//np8W/3fnLh4+w56Vg+q/BTmOWkrCbQJsfktc0V/OiMI1l85XHs84Xwh6J43HYqG4J4A+Gk27SoxBLGY4d5mHPqSCKGpufnTZrFlty1zkHpDbM74sCMaBQqNuNccSmDvTsZ7Cnh0e8+xpUvNnD+4q2snHMSRTlOfnm+h5+dp7FbLQzKkX4nousikQiq6hOcTUnx3/CUcNP5S7jzPzs5rXQQfrudoUUuQhEDj0vOJ6LvkbA7VQI14K+GnOLUfJ5bmjWKnhGbgRma56Yw24nFYqE4z5W0vPDSq48jz/AyUFdy99lF5L97V0KFGdtTl1Kxp4x5qz/iJ2eNiueJtKziVZznwmW3cvPKTdz49IcMyHSw7N87mDeplCG5Gfzl0nEJ21w0czwuuzX+3NhhHn5y1ijmv7CZU+5c06vKDLdccrf2ltNYNXeiJMgfIgxDE26oxtKwB777AFy0DLKKyP3b5fzurMHxQg8KsCj4zYubsVst8v9WdFk4HCXs24e11djLf/4KppdmMHvpBm58+kOsFsXw/EwZc6JPStsMilLqZGB4y21orR9L1/Z6XMWn5mNuigKUWC+V2l3xp7TW/HNbFet37MNpt3LCiAGMPywPpQ58cjIMzTvb97Hhy300hg3Glng4bVSRnNhE+5oaORZEQ7x27VHc8EIZd726hfmTj2FEQSY5Lit59Z+hFpt5LE5PCVxwH/jKoWy9+RnenRS5VZtZk+I8F0XZThZfeRzDBrjIz7Lz50vH0hg28PpDnDN6CNc/vpGymgBnlhbx+DUnYLWo+PItID4zMefUth3rr31sfTw3pqd1aMmd6HUaGoNkR33gzDHzAT94Ar79S/j7bRyWa+Mvl45jxbtfctrRZnnh9pqICtEZkYhBfWOQvHbGXpFbxb+oGZgts3Wi70pLgKKUWgqMBD4AYtmgGui7AcruphuygiNS83m2DPPkVGvOoNQ1hpm77H3+9VkVFgWxL4cPL8riqonDmTJ2KG5H2/+djeEoz76/m4f++QXbq8yKz7H3Tzgsj4cun0CeXFRFTKy7vGGYF8YVl6G8O3F7Slh40TLCmcPxWXPIdTlRvgpU614qz10PZ/0WVswwn/OUUOE3B2ts1iSW0P5/T31IZUOQR686jq+8wYRlUHdPP5Z7LhxDYbYTt9NKQaazzYU4NjPhD0UkEV2kVjRCdu1W1IrLmotITF8CH62CU25B2Zy8tGk3px41kIIsBwMyHXhcvSPvSRy6jGiUaMBLXkMZasWM5rF3wX3wziI45RbycrJYfs0JDJU8J9HHpWsGZQJQqrXu+TUW3aXsPcgebFbxSpWmXijhqMFVi9/jg11erjp5OKeOKiJiGLy3Yx+vflzOz1d9xIKXP+Xi40uYPGYI+ZlOvqz28fJHe3nuw6/Y5wsxoiCT6087nHEleditin9uq2Lxuu1c/td3eeb7J+OQ8oQi1l3+yUvMIOPVnyUEH2rFDBzn3Y0jewgR51EYoUYcSWryxxuVekqoPn8Jt79SCZjLs4Z4XMyffAx3vLIl3idi174A81Z/lDADcuPTH/LU7JP2u54/NjNRWY8koouUMaJRVO2u5uAEzMenr4AZzxK1ZlCtc7j6mx5sNkWeq23wLESnGQZ63xc4lTK/4EnyxY8eMBK/LY9cu1VKCos+L10BykfAIGBPmj6/9ylbn7rZk5jMQqjdxR/f2MqGL2v4wbcP5+SRZnUvBxZOObKIbx1RyLaKBl79eC8P//MLHnz7i/jbbRbFuMPy+M7RA/n6kJyEpWCnHVVEltPGH97Yyj1vbOWWs49K7b6LQ0/L7vKuvKQNwbC74clLMK58jS1VIb7hKUl8naeEhoxB+L63HpfLzc9e2s3GXXXxyl1ef6hNJTC3w5p0BiTSwRwSSUQXqWIYmlBdBRkN5cnHv78acoby6+c388vzv87ATFfP7Kjoe/yVWL3bzZUT7XzxE7G5CUY1BdlybhN9X7oClAJgs1LqXSAYe1JrfUGattez6vaYyeyjzk3t57oLMMo3s+itL/jWEQXx4KQlpRRHDszmyIHZXHbCYXy6t45AKIrH7WDUoGyynO3/Lz5uxAC+dUQBD//zC2aceBhDPXKx7XNiS7ZalBKmdUnK2GtC/uYLY6DGXFrQKvggUGPOphhhfvnGXh4+fwn5zzfX5K8+fwnXPL2Ljbvq4pW1bjmnlG0VDdz16hbmnDqyzWxH6y71YM6AfF7RgC8YOWBSeYd7vwixH4ahKa9rpNCiIcOTfPz7Kqm1FvLa5gp+Mam053ZW9Dk6aqBiFTuTjD2dNZBa5WGoR/JORP+QrjnCW4HvAr+luZP83WnaVs+L5Z8Ujkrt52YWYgnVk2tt5JLjSw748gGZDk4eWcDpRw9k/GF5+w1OYi6cMAyA+/7+WZd3V/QysSVbD58BfzwGHj6DaPnH1PgamytctXxN+UfNXYrX/tFc9xz7PbYOeu0fwVOCYbFT2RDmmld81Fz6MlXXbKDxitf42doIG3fVAWYX+vkvmJWN3t9RzZxTRzIoJ4Pl15zAmaVFgBmIFOe5uOfCYynOczF2mIfFVx7HY1cfj92quOf1LR3qut628tjBX8ANQ1NZH2R3jZ/K+mCvqAYm0sswNFvK63ls7edY/ZXw5m1tx//0JfDBE2z3mqWvM2QJoUiRUGMI/BWwdAqsmg2T708Ye8aFy6i2FTEgS4IT0X+kq5P8W+n43F6r7D2w2GHAyJR+bJUlnwLgf78GHnd6pnTzs5x88/AC/rZxNz89+yhy3fa0bEf0gJZLtgC8O7GuuJSyc55lb85Ac2ai5WtiQclz15tLFt9ZRGTmaiyAZd9n8PfboKECffET1Fk8LP3e8eyo8vO7t/Yw59SR/Oz5T7ji5K/x8Z6GhIaMy/69nUnHFvP95c1d3/9y6Th+dMaR5GTYmf/Cx1TWh7hz2miynDa+v/z9hPcrzIChO2ZHYjeqrZeLSWngvq3aF+Lax9bzxjWHo5aeZx4PvnIzFyuz0MwvfPdhqo//CQ+ureNPF4/BZpXxILqusTGCvbG8OSneuxPevBXOuxvyhhNRNspVIZk2m5yDRL+SlhkUpdSJSqn3lFINSqmQUiqqlKpLx7Z6hbL1ZjNFa2pv7p/fa/aMOKM4ktLPbe07pYMIhKOsfL8srdsR3SwSSrqW2eMw+OPrn2L4qiAShMtWwsy/mX//+21mMuYPP8B/1l1c92IVU5/czX+NEYSm/BX9vTfYYT2MKQ/8m9Pueot5qz/i8pOHY7UoXttcwV2vbmHxlcexcs5JzJtUyl2vbmHc8Px4cAJmfsl1j79PnttBOGo2gdy4y0tdYyQenMRed8szm/CFoky5fy0TF/yj0z1OWs6G7PMFqahv3O/MSOxGtXXJ4o7M4ohDk2ForERZM+sInCrafMyUrTeTlf96FlobBI77PnudI/h/536d21/4hEBIqsSJrolEDPzhMBYjkniuLlsPy6ejLXY+rMvFYrWRkyF5J6J/SdcSr/uAS4BtgAu4BvjLgd6klPqrUqpCKfVRi+duVUrtVkp90PST4kSPLopG4Kv3ofDIlH5sOKpZWZYLQEG0KqWf3dqIgky+VpDJqo0SoPQpNkfzMoEYTwm+qJU7TsvGVvUpPHoe/OV4eP6HcMZt5mte/RkfVUYIZwzgvsklPH1JCUcNysbuyoZomMOsVfz9mpG8cm0phVl25izbQDiqObO0iDmnjsRttzDS7WdMTj2/ODWf4lxnmyT4wiw7A7SXwVTx/FVHMnZYDh6XPXmyfFTzwGXjeP2Gb/HnS8ayt7YRb+DAAUNsNmTK/Wu5/vGNbNlbz//ev26/gU4oEpWSxf1IJBIh2lhHXrgcWySAQsNlT0PxhOYXeUqIKhuf+d3MXr6RLeX1VDYEpUqc6BrDgIiPvGhlu+POF1FMXfQfDEPL7Inod9JWp05r/Rlg1VpHtdaLgbM78LZH23ndPVrrMU0/L6VyP7us8hMIB6Agtfknb5dF2NLoQaNw+r5K6Wcnc9LIfD7aXRfvlSL6AHchXPxEwlrm6vOXgLKS27gLVs9NLGX5tzlw5u1EZj7P0flWcoIV2Gu3Y3v2auwv/R/K+yXq0XOw3DsGx9JJHOWq45lLh/Pw1BIKs2z8afJhfHtwiCF6L3m1mxn4/h8Z79hJqWsfb84+irHDcgAYOyyHxedmkrHkTFx/OZZvvPy/rPhuLhk2Fe8OH1Oc56KyIUhdY4SrHn2PKfevY97qj9jjbTzgLErL2ZA5p47kppVtmzm2nhlx2KxJ90FuRvseIxLB6tuLPViDAnjjVrh3LLx4I5x+q3mz6CnBmL6Uu9fV4nLYWDB1NM9s2CVV4kTXGAY6WI+tfg+q8lOoa7rGn/eH+LjTFy3j9rf2yflH9FvpClD8SikH8IFS6g6l1A0d2ZbW+m1gX5r2KT3KmkqmFqa2TO8zW8NkOmyEnR4c/m4IUL5mdq5/6b/9pzJ0n2exQFEp+ntvEPrBJv57zrNc84qPXFvYLBccC06KJ8BFy+DCx9D2TGxLz8d231jUkvPMGcKz74Djr4WnLk8MaJ6agWXvBxz1whRyvFtwNZRhf+w81J/HwfpHYcI18OKNqD+PY+Tq77L43EzGDsvhtjMG4Vl9RcJnOZ6+jCOzGnngsnHxACGWPJ9pV+REa3jiwqE8M3MkhVl2Zi/bcMBlVy1nQ9qbnWk9MxIrWdxyH+RmtA8yDFTlJ6iXboa9/4WGcvjOr2FsUx7A6rkwbTHRK17kZ+sMHvjnlzhsisMLM/nNlNGSkyS6RDd6UdEgKItZVjhQY54zG+vgwmUwcxV11gH86wuvnH9Ev5WuMsMzMQOS64EbgGHA1C583vVKqcuB9cCNWuua1i9QSs0CZgGUlBy44lXKlG2AjFzIGpiyj6wNat7YEeGswyDiz8fRDTMo+VlORhRksmZLBdeddnjatye6Ycw2lQ9W0RB2RwbFxQO579KhFFjqoGKHObMy4lvwPzeagYjNiXrl/yUGIavnmsma+Uckr82fPcgsO/zUDPN1WUVmYnHR0WZFmhaf5Vl9BU9c8RpWHUoMjib+GFx5WHWYobl2Xp91NDYdwh+1UqvsDA7twP6y2TRvmKeEh89fwjWv+AhFohiGptoXSppAH5sNKasJ4A2EO9TMUUoW71+PnWdTzVeB2vQUfOsn8PZdMOYS8/n/udF83LgMjChf1YZ4csMes2qX3UpRdkbP7bM4KL1uzBoGaA0BL3h3mF8W2Zxwys3w1h1wzu9h3xdkFH6DVXMnyvlH9FtpmUHRWn8JKGCw1vrXWuv/a1rydTAeAEYCYzAbPyYtV6y1flBrPUFrPaGwsPAgN3UQyt6FgiNBpe4E8sr2MCEDTi+GcEY+Tl/3zGqMLs7l/S+91DWGu2V7/V1ax2yrEsPqkTPIq/+MobkZ2LOLIO9r5qzJcdfAY5PhL8fBkklwwuzEddCx5owWa9J8FhzZza/LLIRv/9LsQF+/N2lAs6+unk17AuZ7iyc0v/7R83AsnURe/TZcr9yI/d7R5C4/hyGWfdifSuzonf/8Fdx2xiBcDguN3j1E9n3J3q928otVHybklbScDVm45nPunDa6QzMjqSxZ3Nf02Hk2lQwDbURg/OVmcHLCbHMM/vUs81iY8D0YdR5YrFT4NcV5LhbNGE9BprOn91wchN42Zs3Zk5A5a/fijWYe4Is3QrAevnkDGFEMzwicOQPl/CP6tXRV8Tof+AB4pen3MUqp5w7ms7TW5U15LAbwEHB86va0i0I+qNpmfrucQq9uDzPIDYfnQtiZj9O/x/zGJc3GFHuIas26z9KblC+6QZISwzx5ifm8xQJ5w81mdK2XbT13vTmjEeMpgbDfDMCT9UWJBJp/dw0w3+/d2dzosSVPCWHs3L6mktD05XDKLc2vb9q+empm87fZWUXYjHDSQOeoQgeeum24HzuLwYuPY/zr07nvdCeF7CNaXw6GkTAbct+lYxk1KJtn557M2ltOY9XcibJMpz8yDPBVorRh3hCOuaTNGOTpK+Cs36CVhYzcgcyffAyDpTmeSBWtwQi3zQFcPReyitAWO5Gc4W0b6grRz6SzUePxgBdAa/0BMOJgPkgpNbjFr1OAj9p7bbcr/xjQKe1/4g9r1u6OcvxA854wnJGPJRrEFmyzqi3lDh+YhdthZc2WyrRvS6RZOyWGiYTMm7SGvRBNfvNPZtO3jJ6SpoZhh4EG3llkLt+68kXz8Z1FUFvWHKz4q5s/b+0f4bsLEwOayfczzBVm4XeLqbYMwGhv2Zgrz/z3xB9DzfakgY7NasWy5ncJ+2N/ewEF3v9iX/wdc/aoKUiJzYYMyHRSlJ0hMyP9lWGYgXNstjvDA0Wl5rLElrw7IRygvD7C7OUbGZSbgcclOQCi6yKRIATrwIia565Ws9VaG0QyCnA4pR+ZEOnKQQlrrWtV4rKnA04BKKWeAE4FCpRSZcCvgFOVUmOa3r8DmJ3yvT1Yez40H/O/lrKP/NfuCMEonNCU0hLOMJPXHf49RDIGpGw7ydgsFo4ZmsuaLZVorVEpXLYmulmsxHDLAMBTAnaXefP+j9/C6b+Eq18FX6UZUJStN1+TWQA/3AjKCvVfmd/sDTsZTr3F7Avh3Wm+7sJl4B4A599r9k+Z+GNzacyYS8wgI2cIfPcBcGSCzQWheiy1Oxhoy4DV1xGZsRpLsn0MNAXjrjx441fNzSNj271oOcrqMJfmtHz+gvsgI6d5tuiaN1KaGyYOcaGm2b76PYnjePL9ZmO8svXm3z0lYHMwwOXm2bknU5ApwazoOiMSwVq5FbXi0sRz1t9vaz73WmxYHbKUUAhIX4DysVLqUsCqlDoC+CGw7kBv0lpfkuTpR1K9cymz50OzAoe7IGUf+eaXETLtcIwZl8QDFKdvD/4BX0/ZdtpzbLGHd7fvY2t5A6MGZad9eyJNYiWGY8u8PCXm70bUDE5OmA2PT0+8UL6zCE78vvkahxMWn2t+u3zOneDymEmdlz4N0RA4s8BXDXW7zS7bhUeZMy9n3gb7vjADi4YKmPYohPzNS8k8JXDhY+byrdd/QXj6MuxPN98s6ouWoVBmfgyYn9HUPBJXnrncLGcIRBrbLs157npz/2K/R6S5omgSaYSoH4INzcEJJBaCWD69KQBeBv+6B8cpP6XII0nxousMQ6P8lc3BCTSfs876rZkDNeVBsMtSQiFi0hWg/AD4ORAEngBeBeanaVs9Z+8mc3lXimYaDG1W7xpfCLamxXeR+AxK+it5AXxjqNkc8j9fVEuAciizWMyg4aqXzaVcVjtkDTK/PU627v65682lUlYHrPsLjJthBidn32G+5rHJzQHGdxeCEYFnrjafG3UenHJTYhAS+2YwUG0mgCaUJ77cDCT8VWh3IV9OWc3gTLBbraj6r+C1X0BDBZFLVxKcspTMVTPNm0pPCfriJ1CuAWZglGx5WKje/HfTt+BCAObsSThgBt/Jxo3nMHM2MWsg/Pdp2P62OcMoRApEQkHskcbkY2/g1+G8u9HuApQrv2d2UIheKC0Bitbajxmg/Dwdn98rRCNQ8QkcNSllH/lhRZTqRh1f3gUQceRgKFu3NGsEKMx2UpDl4N3t+7ji5OHdsk2RBoYBlZ+2nUHJLDR/kuanNJo3cOMvB3+VmcSeLMD42xzzG+fYc2MugbfubJ7lCNSYszETf5zYb6Xlthq9ZuWuUedRcsrNqGUz2wQ3tsen8dn5z+H7ztMMzrIwICebap3DYBSW9pawObKbbzRbX+ybyi4TCZnBi7tQElH7ASPSiKrdhVpxmTlGk4wbbXOhsjPg7Ttg+9tmIOzu+YpPog+IRrCHvGa3+GTnLIsNXVRK1F2ITc5HQsSlq4rXBKXUs0qp95VSm2I/6dhWj6nZYS518RyWso98uyyKAsa1zNlUFiIZA7qt1DDAUYNyeGd7NbobKoeJNGmvipfVbt68JysZXLMD7h0Dy/7XvJkvGNV+gGF3N/+eW9xcqvXR88zHE2abz4f9ybeVWWAupTn+WrNyV7JKYt6d5DWtsIkamohhsLe20WzQGFvC1jIJ/6Jl5jHpq4RXf24GaIZh/r1V2WUePiOeSC/6LiPSiPJVm8GJd6eZa9WqGp2+aDm7o7kElYPI/9yC/t4bqKJSCV5FSuhgPcoIw6u/aFsJcfoStMVO1F2AzS65J0K0lK4lXsuBm4D/An3zDqDyU/MxtzhlH/nPsgiHeyCn1cqUsHNAtzRrjDlqcDb/+qyKHdV+RhRkdtt2RQq1V8Ur5IP3/grTl5jlVOJlsyQAACAASURBVFsnCsde97c5MOPZ5gCj9bd+YX/z7zYXPJekXPFlz0Buhplz0nL515QHYdVsM79kxrPtV/IadR4DqGPg6805KqOnL6fGOBwsTrMC0zVvNFUmi5hByZYXm2dh/vFbOP8eMyBrL2CTRPq+KxactFxaU7a+Oaep8GiwWKnDxcUPbWD+5GM4ZugACrPlRlGkSDTSlP8UNM9NvvLEmebMQrC7sdkl10mI1tIVoFRqrQ+q78kho2qL+Zg7LCUfVx/SfFAe5X+TVCwOuwpw1X6Rku10xFGDcgB4d3u1BCiHqvaWQOko/Pte2LXODA781eZsxqrZzVWMoHkZlivfDF5iNfubqmjFPy+ryGzimCzIsFjBtw/W3mtelDMLzYIS0VDza2JlhJMEQJHv3I799V8kXNDtb/+evHP/ALjNb7izBpoNzx4+t/kzsorMG4IzftVcVnl/ZZdF3xMJQSiA0lFz2eJlT8NbC8wxXrbenOW78kWo28vOQB5/vmQsuS570sadQhwUw0A3VJjVMA0jacVEffWrRK0ZabsRE+JQlq7j4ldKqYeBNzET5QHQWj+bpu11v8qt5o2dw33g13bAf76KENEwJsmy57BzALmBf5sXWos1JdvbnyG5GeS4bLyzfR8XHVdy4DeI3idZFa8Zq8BiMy+U4YCZEP/Xs8ylUQ0Vie/3lJgVj5zZMGA4XPGi2VwMDRYHYMA1b5rJ6u0FGdEwPHut+fyWF5ufP+9uuOAv8Nx15k3jRcsSyr7qi5YRzRpiLotIUkrYplotPWwZfMS607cqS0xWkZnMH9uP2L5IIn3fYxjoSBhVuwtiS7tazhI2VJhjzu7CsLsY7CnGYrXgcTmkgpJInWAdCsOsdtiyrHWsYuKpt4DNjU3KCguRVLoClKuAowA7zUu8NNCHApRPICc1sycA/yqLkGGF0ry2fwtn5KN0FEdjJSH3oJRtsz1KKTMP5Yt9ad+WSBOLJXEJlN0F9Xth2ZTmC+X0JeZNe2xdfsub+gsfM/NMlk8zb+7PvN38veWF9qJlZnK8r7zt+6cvMWdK2stfafSagcQ7i8z9a5ol0Z7hRK1OCPuxWCxJq42pq15O/MyWs0UTf9z2PSsuM4OiU242n4stA7v4CTOQE32KDvlQ/qrm4ASaywnPXGWWwXbnowGj8GgKbPL9tUg9bURQ0VDbstZNy1+1M5uowy2zJ0K0I13HxnFa61Fp+uyeZxhQtQ0OPz1lH/nPsihfHwD2JBMk8WaNvj3dEqAAHD0om3e37+Mrb4AhHle3bFOkWGwJFJjLoGKzKcUTzBv5aAjO/h288v/Mdfnn3Q15I8zu8MF6M28kq8gMJIL18OysVjf+M8zAYsWMxF4lucPMakgTf9x+/kokaC6zmfGsme9Stt4Mlk79KbbYjeW1/0ge4LQu3tBytsiV135Q9NRMs+zyOQukildfFevU3VCefBz4qsxjwuYkaoBNghORDpFG8zzXUJF8HCrzvGOTGVwh2pWus/M6pVSp1npzmj6/Z9XtNk8+ualZ/rS73uCLWoNrSpP/PZxhNoJ0+PcAY1OyzQMZ1ZSH8t6OfUweM7RbtinSKBIyg43z7zULO9Rsh9d+bl5AL1xm9vLxfmkGCwCTHzAvpGf91vzG77sPJL/QZjbNQJStj/cqYeYqs4iEv7r5G+u3Fpjbmny/GSy8cnNz/soZv4awH11Uilp8TvN26nYnD3BsDgxDU+0LEYpEcdis5BcejSU2W9Red3rvTjOZPrdEApO+KBKCuj1mOVdfZfJxkD3ELOsK+KzZ5PbYzoo+yzDQAS/KiLY7DrXVhnJLzxMh9iddAcqJwAdKqe2YOSgK0Frr0WnaXvdKcYL8v3ZHABjbzmqTcMYAgG7rhQJw2AA3LrtVApS+wu6C029NTHaf9qj5bXMkYHZnD/vg3LvMGQrvDvM1sRmJQE3yGz53fvPzsfXVNre5rVj+SWw5mGuAGRi90rTU6vRbExtAXrQMRnwLNjZ1kU+29OziJzBcBWwpr+fax9ZTVhOgOM/FQ5dPYNTAIizotrk3saaRnhKo2mrm1kgZ2b7FMMzZkUij+e30B08kX3ZosWA43HgjDv4/e+cd32Z1Lv7v0bAl7xGvxHFIAoQGCCvs9gJtaegNBVpGmGGPUtrya2/LvfS2pePSUsptb0uZLWWEvUdaQsNsoQ0kbAIhJCGJszziJS+N9/z+OJIl2a9syZYsOX6+n48+kl6959Wx/fg573OeVVYosf9CBvB3oQK9ZvPFTg4X3Wt0oXhPBGFYMmWgHJeh6+YGzWEDpSw9BsrfG4NUeGBGgsbtlquQkNMzrgaKw6HYs6aI1zdIHsqEJbYxoXJEjRMw3otADzz9rfgk4lAXPPkN8/kJN0bLDNsZCyfcaI4vuNZ0rW/+yOSUfOmn8d8VCQeLdKo/8kpj+Dzx9bhz1INnm0T+5o+iFcVceejzlqK0ZcoZF1bR2h0YME4AGtt6ufjulfzlm0dQEmgBb5kJ5Qr2w851xjjxNUUNFV+TlBfe1egOF3noboZ/3WyKK6y4Nb56nMOJduaxocvBjAqPJMQLmcHfYzZ0Xvn1UDksqkF7K1B5RdmepSDkPJnqJL8xE9fNGZrXgKfUPMaIpTWvbglywBQTZWOLUgQ8FeT17Bjz96XCnNoSHlq5mfYeP2UFstszoYg0Jox4Ei5YFu/9OPLKoUbEk5fD4iejuSShgLmxi1TZis1T6W0Hvw/2O8MYMV3bTE7JoiVhD4xNOFjnVhNWdtRVxmNjd06kg/3L14W9MJegYrwnFFbhD4YGjJMIn51VRnH7RybPJHL+mQ8bw+mLPzE3DC/8NGr4SHnhXYdgH9oKmZLCTjfsfwa895B5Lqwysqbc0N9On7OQ8sJ8XC7xngkZINiHBlOk4fDLQVtwyMXGg+0pQzvz0G4vSry3gjAi8l8yGprXpK1B4+oWi7Y+OGDK8OcF8yvI69melu9Mljm1xqWzamPbuH6vkAYGNyaMxEJHSJRMrnW0I/zT3zLliPNLjPfjpFuhfJZZdIN95vM7F8LS75pzTvyDMWiceSbhfdESM27REvM+0GsS7pd+F3Z8YN9hvrsZq3IPrNOWmNCzk24244uqzc/T00yey0l9eXzhhh8eXTm0I/19pwLKeGoePDtqnEh54V2HUBDt70H5u03OUuEUmDIHPvdd+PhZU0Y72AehPizlYrvfS5lX/vZCBrAsdDBoyqOXzzK68OO/Ge/1cz8wlQuVwiG6RxCSQgyUVNHahKCkKf/k7+H8E7v+J7EEPOPbTR5g96oiXA7F659KmNeEI9gfb4BEQrQiRkEkdCuWOQtNr51Yo+CJy8yxOxfCjQeZMsWu/KHel4fOiXZM3vAKHPW9qKGz7Gr4t+8Zj2MkRGzwfCIhY5teRznzUJFO9ct/bMZ//kdmPkE/lYV53L54/oCRUl/upcBp2RtcvW1Dv0fKC+86BHpRWoPbY0L2LAte+z+T23TwRTBnIdqZT8hdSFfhdGZUFklol5ARtL8X1dMMgT5AG8Pk4Atg7XI46ip0UQ1a9I4gJI3UWEyV7hazE5IuA2VzkN1KoMIz/HnB/Ary+ppNFSLH+PzZ8lwOZlcV8YbkoUwsQkFjVMQmtTeuNLHQ5/3FNFx0ekyyZqSk75yFpk/IvScPTS7vaYk3Rrq22xsDlbubMK5ZRxnDJHbMw4tNRa/Y+URKE1d9xvQVWrsc9j0Z9efjhs7hqStMeJnLNNObU1PM01ccQUGgDZcOmPhIuyT+9o3RPJnCKuP5LJ4qCfK7AmHPCB1b4kP7Tr3bfP7QYlj8FHgrUXmFlIphImQIy9KoYLe5N3hocXw/qcMuNRtGnjIcLne2pyoIEwZZpVMljRW8egKaN7aHRgzvgkizRou83uYxf28q7FlTxLtbOugLhMb1e4Ux4NsOy34w1HNw9H8a4/a9R0xS8Uu/NDfuFyyDL/8yepMH0YZiR11lwsNiGRwuFrn+jg9MOJUVtDdgUEPHufJNtZu6/eGwy+zncOSV5nXF7AHPhwNNedcn5N/5JZz/ty9qxW1w2j3xP+8pd0JekclBARNmoXVyxollhXtpbDbPljXyGGH8CPZB53YTNhjsM3JcPz9qDB/+TWjfhFaK9qBTvCZCRrH8/eFKcs3xHuiHFoO20M48VH6CKjiCINgiHpRUaf7IPKehgteKbUECFhyYhNc3kG9KDef1bMdfWDfm706WvWpLePrdbby9uZ3DZknd9glBKGC6pXfviCa897ZBQYW5Od/na3BPuKP8mqVmzBWr7I2Kitnw3A+jx+rnmzj/sx4x3gl3gQkX81aa8sHtm4wHx86b4XSbSmFPXm4W78Flj895wn4O3vJw3ognalwMzrH55+/M83l/MQaSywNtnw6tUuZOouno4AIDkbAwKU2cG4SCpkhDX1v8bnXE29a40hi9ZQ1o5aLYI+WEhcxhhUI4Q92o7h0mv26wPFohU47dKbdbgpAKstqmSvPH5ianIAm3xwi8vDlEngP2TuK+P9ILxTRrHD/2DCfKS5hXjhO74+9wmQUy0jwxkgcCxpBQzqGGgBW094q48o0XpazBGCdfuAZe/b0JdVz63WiSfLAvOi7QbZ9fEugz33P2o3DKn4fmsexcZz8HMN6R2LLAQf/QnyFipFTMNPHfj18ytEqZlYQncLDx075pIEFfyAH6OiDkjxonEO9tK2sArdGL7qXXM0UqdgkZxQr0ogLdRtfaeKC1w4VyF2R3koIwARHNnSqRBPmENYGT55XNQfadAvnOkc8NeowVM96VvIryXTRUeCVRPpeJ7Pj/8Yvw233gr1cNDXc67R4orDHy27p2qCHw1r32Y/51K/zlOyb/4+SwUbH/GSZ5PnYxfuIyc3MIJswhUvv/vKXmee1yU5b46W/BjQebXJXBBsbL15m8mNg5nHgTFFab/7ne1miolSsvgUEVrpATsjFg2jeZ4yNhZ/y0b5LSxLlAsA86Nif+GxVWwWn3oAsq2ZY/C2+eeE+EzOIM9iQOa62YbUK7xHsiCCkj/zWp0vQh1M4b82U2d1ms77C4eO/kzg+5CrEceeSPs4ECsGdNCa+tayEYsnA5xabNOQbv+EfCtiLhTk43FNWaG/wHzog2YYxturjnlyC/yPQOcbpMqeAVt0e9EveeGu2lkqhEcSQUy1tpEu5jE5cXPwV3nzC07HHsdXxN5gZz4Q0mdKy3DZ6/xhxfcK3xAkVCrQqqhnaMj63OFTFgBoeZJVPicyxjhcxhWeBrNnJ15sP2f6OSqWhPOV2Wm9qyfMk9ETJL0B8u1pAgrNWVb0qwC4KQMnK3mQo9O01y8eCd21HwymZTXjiZ/BMg3Kyxkrzu8TdQ9qotpscf4qPtXeP+3UIS2O0mR4yUipmmcpXTFT0vtoLWeUvNw1MKO9fD8z81VZE6t0aNEzDhXZ4yY6QUTDFVv2Ipa4DiOjj7MfjgCZOncvbj5vwF10J/1/BljyNhYN3Nxhi6c2G0d0msURQJtXI4jKFy0XL4zkemc7yn1HxmWVEDZjTlhccyVsgc/l5AmyTk/i4bj9/daJcX7fJSUiCd4oVxINiPAlMZcdGSoR7o/BLJWxOEUZJTHhSl1B3A8UCT1nqf8LEK4EFgN+BT4DStdXY6Bw4kyKfHQKn2wvSi5McE8svHPQcFjIEC8PqGnewzrXTcv18YgWR3/N1eOOvhqHfi1d9GvRNv3w/H/y8s+B/j6VhwbfSa9fPhuF9BxyYztmMTfOFH5pprlkaNi8cvMdc7d6lp5tjTYgyOV38bzQ0YXPb4zIdNaU5PGTz1jaHnRX6W3vC/fGyolcNhjIZECe0RAyboN7+LgqrkbhZijZ9UxwqZIeiHniaTZ9XbZuT1qO8bIxjAlYe2QoQ8xbhcObWsCbsqoaDZyOkIFwtxOOHcp02emxVCe8tQeSks8IIgxJFrK+6dwHGDjv0n8LzWeg/g+fD77ND0oXkeo4ESCGle3RLkgKrUUlmCngryx7lZI0BlUT5Vxfm8IXkouUkyO/6WZfqXRBLbl11tEt5PudPc7B1ztQnNQg9tpPiFH5tKXbFJ8d3NJhTrouXGmIlUTyqqNqFkdy40XbwjTRY/fhZOuiV+jodeajw2wX5jnDSuTOxZefW30fexhtdwCe0Oh0msL5tunlMxMMYyVkgvwYDZHLr7hKhMHXopvPwraFtvcptcHiiowOUaoaGUIKSLnlYTURHRi49dYoqUdDdBoAflKhS9IQhjIKe2mrTWryildht0+ETg6PDru4CXgKvGbVKxNH9kdqELxxbq8VZTCF8ADkrxMoH8Ckp6/2V2aBxJZNankc/UFvPaulZClsYpoRO5RTI7/nY38k9eDuf9Fb7yG3N+11Zo/SRaAey9h4yHI69gaOPFJy+Hc5+BRy6I93YcdZV9daWv/M787yy8wYRr5Zea9/9+Pfzle+b7IOpZOf+vpmeJFTQ9XRpX2htektC+axMKGrmMNBSFqEwtuBYq9wB3ITx9Jer4/zVhfoIwHoT6h1YifPJyEzLrLoK8JEqaC4KQkJwyUBJQo7WOxDVtB2qGOzmjNH0IpQ1jruD1yuYgDgX7pWqgeCpx6CDuvlYCBdVjmkOq7De9jFfWtvBOYzsHNpSP63cLSRDZ8U/E4Bv5+vkmnMoKAOHuxqGAqaR1wo3GSNj3NLjv1HDPkwQVsU65Eyy/MdotyyTk251bOg2WnDw0dOvC5cZ7s+O9aIjWMVdHu71bljGgvnydveGVKLxNKTNWdjAnLpZlSlZrK3HFLocTXvs9HPNfptqbIIwHwT6zUWgnl1YI8gtE9wjCGJlQ/0Faaw1ou8+UUpcopVYqpVY2N2eoX0HzR2lp0PjipiCfKYcid2rjAh7TeyW/u3HMc0iVedPKcCh4aY30gkgX4yKzYG70Yvuc1M83YVfLrobf7W/KEzetNh4NX5MJ1/rCj6JVvjq32pf09fvMDWIoaBo/Pvl1szjblv/1JDZyIt6fK983z7ENEUcKtbILbzvhRvjL983PJB3g08q4ySyYpPi2T+3LYpc1mGINznw47FKo3ltuCAVb0i6zlmX6QCllL5dOt9F3giCMiYmg0XcopeoAws9NdidprW/TWs/XWs+vqspAtZ2endHSqGOgscvig1aLQ2tTH+v3mh1CT9emEc5MP0UeF7tXF/HyGttfvzAKMi6zEXqaTZhUJLfjyCujxgdE8zbAVKLxNZkE98jnL/58aP7IV28Db4WJt46EORx5JfztR0NzSBYtMTeasQbSoiWmwlfEGzmWXJHquXDh3+Cbbxpvj7age4c0V8wA4yazwX7o22nCt8pnDa3YdepdpiGpFYp62wTBhrTLbHeTaXZrBeGcJ0zhkfr5YV13rxgngpAmJkKI11PAucAvw89PZmUWAwnyM8Z0mb99GgDgsFEEqgW8VWgUHt/4GygA8+rLeHRVI62+fiqLpAHahCHoN9W2uneYuP3qz9iHewV6za70+X+NekIiZYmX/8jkj5TNgJaP4W//DV++3lSviVzLWx7/Pd5yU3EpvwReus7cZL78K5PgHNuD5fT7xr4D7muK5ilEvCgv/FRyUSYioaDRt7F9dE67B06+w8T9l043RqgzD4prxTgRxheH2zQLjZXPRUuMrimoNPl1giCMmZzS7Eqp+4F/AnOUUo1KqQsxhsmxSqm1wBfD78ef7e+Z5/LdxnSZ5z4NMqMYpo2i+qB25hHwVODp2jimOYyW/aeXoYG/r23JyvcLoySSpxGpkqV14nCvPx9nqtO4vMZLEjnP12TCaZ68PNqfxF1gqntFzulti37Pg2dHq4V1bjFGibccvniNjffmzLF5Onqa7ZOoj7pKmitONCwLurZFb/7APD90DuQXwxNfN2GBy34AaDFOhPElFDR5UYPl88GzTYEGrcGVYuy2IAi25JR211qfobWu01q7tdb1Wus/aa1btdZf0FrvobX+otY6O7Vut71jbrC8FaO+RFufxevbQhw2ivCuCAFvNflZCPECmDmlkBKvi+Uf7sjK9wujJDZP48gr4W8/Hj7c68GzYfvbZnf6nMfhipWmYtfa5dFqW2BuFL2VcOJN5lqv/jb6GqIhD5W7m6T7kN+ER6S76laiSl4Vs6W54kTCskzeUKjf/u/pcMFpS2DV3XD0VcMXhRCETODbnljfOF1QNCU78xKEXZCJEOKVG2x/BypmjamC1wubgoQ0HD4GA8XvraFw5/ujv8AYcCjFIbtVsvzDHfj6gxTli/hMCAbyNJab3b/YMKyqvewXW285dDRG80vKGkzJ4TkLzDmBHigoh79cBYddZgwZrcGVb8psBvtM/kBxnTn/mKvN5xGPy0hNJVMhUSWvvCLZYZ9IdDfDOw/CoRfb/z2dLvCWmc+Lp5r3gjCehALRgiOD5dMhyfGCkE5k9U6GQB80rzE7smPgmXUBqryw+xhCVP0F1eT1teAIdI9pLqPlc3tMoS9gsez97Vn5fmEMKMwiGlvoQVvRJM8IZQ2QVxxf47+o2txAPvH1aLPGji3wue/Ck9+A3x8ES74GOzfAI+fDjQebazsc5lG1lzFoShuGellOv29sno5EjSrH2K9IGEcsy2z+HHyB0bdDkuLvhtf/BC1rjBEsxokw3gQDxgAJ9Q+Vz9PukR48gpBmRMsnQ9Nqs2tSMWvUl2jptXhlc4ivzR5bGxW/14Q1eHyb6Cn/zOgvNEr2qC6ipiSfJ97ewskH1Y/79wujIBI688AZxtA49S7wd8d7R068CZ6/xuSanHAjBHvjdwiPvHJoU7LHLoav3W7Cv7q2GQMm0lE+1itiWaZE9wNnwMx/g899z3hZrFC48Wn12DwdyTSqFHKbvnYjQw+ebWTr8G8ZGQkFzOOte2HPL5mwLgnbE7JBbyt0bIJHLzJ67KxHjaHscIGnzHhsBUFIG7KCJ8O2t81z5e6jvsQz60x41zFjvKfvL5wKgLd97dguNEqUUhwxewqvftJCU1dfVuYgpEhsF/nGldDXYd8B+eQ7TKWuF346tKS2t9w+FKygAp79L1MWdtnVUeNk0b3RG8nenaYb+Ek3w57HwWMXGi9MXkH6qjCN1C9FyF0sy/TUiRgnAP/8nZERbRmD85CLoGKm2SSSv62QDUJ+Y5y0b4K3lsAfDoa7TzShq/1dIpeCkGbkPyoZNq0AT/mYkjIf/9jPrBKYUTy2qfQXTsNyuCjcuXpsFxoDR+4+BUvD429uydochBQYnNSZqNt7T6sxNL74ExPKEFvFK7ZaV4RIA8Y1S41Rs+Bas+u94Npwl+9wJ/jOrSYkLFLV6/M/Mp4cKQEsWBbsXJ+4K3dPq3ntcEJhjYR2CdkhFEwso9oyXmBBENKKGCjJsHkFVO816tisDR0h3mm2ODodEVEOF31FM7KWKA8wrczL3Lpi7nrtU4Ih6dSd80SSyCNEygHHUtZgvCTLrjaGxJPfMDX9z/sLXLEKKveAk/8YH3d94k2mF4VdaeHIbqKUABaGo6cZ2taDDtnLZFGN6aNTVCvlW4Xs4dsOaHsZdeaJbApCBhADZSR8TdC2AapGn+/x4EcBHMBRU9Mzpb7iBop2fmBcy1niy/vWsbWjj2fe3Za1OQhJMjiJ/O37hyZ5nn6/aYB30XK48n3zXLk7lEwDT9jtF+yHr/wu6iV5/hojg3YJ6pHwrtGWALYs8O2A9s3Qtd08dm4wlcVCwbT9aoQsEQqaIgv+HlNAIRQ0VeLiSlQvgZI6KJwinhMhu7jyQblg8ZPw7XfhmB+Ek+OXSDEOQcgQovVHYvMK81w9OgOlL6h54EM/h9XBFG96ptRXPJOKLS+S170Vf9G09Fw0RQ5sKGdGRQG/Wf4xC+fV4XaKrZuz2CWReyvtk8rtwhiLaoyx8OQ3bEprjpCgPpoSwLFJ/XZJ/KfdAzX7yE3rRCUUhKYPojknkb+vtxwWP2UKkmhtDBMp2ypkm2DAGNOxneNPuwcOPNeEHoonWBAygtxVjsSGV0wH7VEmyD/1SYD2fvjKbumbUl+JuVhxy9vpu2iKOJTi1PnT2djawz3/zE5neyEFBieRO12pJZUnKuUba9jYXWs0JYBjk/ohmsR/5JXRruI+KXM9YfFtj0+Ij/x9u7bCjvdNueqQ31RGEoRs49s+tHP8Q+eYcsMio4KQMWQLcji0hrXPQd08k1ic8nDNXe/72a0Y9q1M37R6i2cSdBdRtuUlWndbmL4Lp8iBDWXsN72UXz+3hmPn1jC9oiBrcxEyzGhL+Y5mXKKwMG959HUoMLafR8geoYD939ddYHKdLlweLbIgCNnGCtrLqxUSL64gZBBZAYajdR20fQrTDhrV8H9tC/FBq8Xxu42t98kQHE58lftRvuUFoySzhFKKC46YiQIuW7KKvkD25iKMA6Mt5ZvquMFJ/WDe97ZFX49iw0DIEZxu+79voCdcelrKRAs5hMNlL68OMU4EIZPIKjAca58zz9PmD3+eDVpr/veNfio98PnpaZ4X0FV1EO7+Nopb3kz/xVOgusTD5UfvzgdbO7n6sffQWUzcF1IkNhHdt8O8zwXswsJOvAle/W00/ruoNrtzFEaHZRlPyWlLhv59y2dJE0Yh9yiqse8cLzpIEDKKbAEMx3uPQPlM00wuRf7eGOKN7SG+vi/kO9M/Nd+UeYScHmrWPkBX9cHp/4IUOHBGOaccVM8jqxrJczn4n6/ui9ORTpeRkHbsEtFPv9+EY2V793pwWFjEW/K1P5rXRbUSWjERiZW5mf8G5zwOymkSjd0F4K3IvuwJQiyWBS0fw8Z/wblPm54nDpeUvRaEcUBW+UQ0fwxbV8H8C1IeqrXmhjf6qPbCggx4TwAsVwFt045hyoan2DzvSvqLM/RFSfK1A6YRsjQPvLGZ9p4A1586j2KPKPCcxS4R/YEzjFEwhoakaSNRRTFh4hIrc28tMY+yBiNzhVOyPTtBGEqszC67yhyLyKzoht1c9wAAIABJREFUJ0HIKLJdlYi37wXlgJlHpzz0oTUB3mm2OHNPcGfAexKhdcZCtFI0vHV95r4kSZRSnDZ/OuccNoPnVm/n+N//g/caO7I9LSERiRLRpbu7kClE5oSJhsisIGQNMVDs6OuElXdAw2FQUJHS0KYei//5Zx97V8AXMuzUCHoqaNntJKZsfIayxucz+2VJ8u/71vHfC+fS1RfkxD/8g589s5quPqm4lHMkSkSXmv5CphCZEyYaIrOCkDXEQLFj5Z+gvxP2OTWlYVprfvyPPnqD8K39YDzSMFpmnkBf0XRm/eu/cfbnhsfiM3UlXHfyPI6ZU82f/rGBz173In948RN8/dIBPGcYrq+JIGQCkTlhoiEyKwhZQ3JQBtO1A/5+A9TPhyl7pDT0tnf8/HVDkPM+A/VFGZrfILTDxZa5lzDrjWvY/Z9Xseaom9Nc03h0FOW7uOhzszhmr2oeXdXI9cvWcNsr61l08HROPaiePWqKsz3Fyc1o+5oIwmgRmRMmGiKzgpA1xECJRWv46/ch0AcHX5zS0Oc3Bvjlin4+NxVOmZ2h+SWgr3Q2O/Y4ndqP76XuwzvYNvfC8Z3AMMyuKuL7x+3FJ00+nnpnC3/6xwZue2U9DRUFfG6PKewzrZTZVUXUlORTU+LBk8mkHSEeSUQXxhuROWGiITIrCFlBDJRYXr8dVj8BBy6GkmlJD3vqkwDfebGX2aVw5X7ZcWC0Nvw7Be1rmLHqWgKeSlpmnTT+kxiG3auL+M6xc2jv8bNiw07e2dzOY29u4d4V8QmIJR4XU8u81JcX0FBRwP4NZRy8Wzl1pd4szVwQBEEQBEEYT8RAifDOg/DsVVB/COxzSlJD+oKaG9/s58a3/OxTCf89HzzZ+o0qReM+32DGW79ij1e/Q3HzSrbsczn+wqlZmpA9ZQV5LNi7lgV712JpTXNXPzs6+2jr8dPWHWBnj59WXz9rdnTyytpm7nh1AwD15V6OmVPNMXtVcfisKXjzxNMiCIIgCIKwKyIGSr8PXvoF/PNGqJ0H//Z9U154GHoCmmfWBfjDW/1s7NR8oR6+MS8zDRlTQTvz2Xjgf1Kz9n5q1j5Azcf345uyPzsbvsTO6V+ir2Rmdic4CIdS1JR4qCnx2H4esjSbdvawZnsn72/t5OFVm7nnXxvJczk4fFYlx8yp4ug51ew2pXCcZy4IgiAIgiBkislloHQ0GoPE74O2T6HxDXjvYehphT2/DIdcDM482vosdvZpegPQG9T0BE354M2dFm81hVi1PURPEGYUw88PgwNyqKCHdrjZPmcxrQ1fpmzb3yluWsWMN69jxpvX0VO6O+3TjsZXsS/+wjqCeaVYLi/+ghq0I/eaKjodiplTCpk5pZDj9qkjELL4aHsXb29q453GDl7+uBmeXs2MygL2nlrCzCmFNFQUUOp1U+JxU+xx43E7cDgUTqUoK3BTViDlIQVBEARBEHKZyWWg/H4+BHuj751umHEk7H821O47cPiGfzWz5J3OIcMVMKvcxbGzvXxxppd9q92oHKiYZU8tvdP2pRdwdm+nYMtreLf8g9qP7sZhxTeZ+vT0lwmUjHNm/yipK/VwzBxjEW5t7+X1T3fy5qZ23m3sYNn7OwhpnXDsZUfN5j+/vNd4TVUQBEEQBEEYBUoPc0M3UVFKNQMbx/lrpwAt4/yd6Wai/wyjnX+L1vq4dE8mFbIks+PBRJepTDHW38uuJrO5JCe5NBfIrfmMZS4TQWZz6XedDBNpvhNxrlmX2cnMLmmgZAOl1Eqt9fxsz2MsTPSfYaLPf1dE/ib2yO8lnlz6feTSXCC35pNLc8kEE+3nm0jzlbkKqSLdhgRBEARBEARByBnEQBEEQRAEQRAEIWcQAyV93JbtCaSBif4zTPT574rI38Qe+b3Ek0u/j1yaC+TWfHJpLplgov18E2m+MlchJSQHRRAEQRAEQRCEnEE8KIIgCIIgCIIg5AxioAiCIAiCIAiCkDOIgSIIgiAIgiAIQs4gBoogCIIgCIIgCDmDGCiCIAiCIAiCIOQMYqAIgiAIgiAIgpAziIEiCIIgCIIgCELOIAaKIAiCIAiCIAg5gxgogiAIgiAIgiDkDGKgCIIgCIIgCIKQM4iBIgiCIAiCIAhCziAGiiAIgiAIgiAIOYMYKIIgCIIgCIIg5AxioAiCIAiCIAiCkDPskgbKcccdpwF5yCPZR9YRmZVHio+sIzIrjxQfWUdkVh4pPoQssksaKC0tLdmegiCkhMisMNEQmRUmGiKzgjBx2CUNFEEQBEEQBEEQJiY5Z6AopZxKqbeUUs+E389USq1QSn2ilHpQKZWX7TkKgiAIgiAIgpAZcs5AAb4NfBjz/jrgN1rr3YE24MKszEoQBEEQBEEQhIyTUwaKUqoeWAj8MfxeAZ8HHgmfchdwUnZmJwiCIAiCIAhCpskpAwX4LfB9wAq/rwTatdbB8PtGYFo2JiYIwtho9fXT3R8c+URBEARBECY1OWOgKKWOB5q01qtGOf4SpdRKpdTK5ubmNM9u4mBZmuaufra09dDc1Y9lSaW8XGUyyex7jR0c9ovnmXfNczz59pZsT0cYJZNJZhMhOnZisSvIrMicMBlRWueGoCulfgGcAwQBD1ACPA4sAGq11kGl1OHANVrrBcNda/78+XrlypWZnnLOYVmaNTu6uPjulTS29VJf7uX2xfOZU1OMw6GyPb1cJuu/nF1ZZgMhiwW/eYX23gBlXjcdvQFe+t7RFHvc2Z7aREZkNguIjh0TWf8FTUSZFZnLKvILziI540HRWv+X1rpea70bcDrwgtb6LOBF4JTwaecCT2ZpijlPa7d/QIkBNLb1cvHdK2nt9md5ZsJk5u9rm1nf0s3iw2dwwWdn0trt58E3Nmd7WoKQMqJjhfFGZE6YrOSMgTIMVwHfUUp9gslJ+VOW55Oz+IOhASUWobGtF38wlKUZCQI89fZWivJdHNRQzuyqIhoqClj+4Y5sT0sQUkZ0rDDeiMwJk5WcNFC01i9prY8Pv16vtT5Ea7271vpUrXV/tueXq+S5nNSXe+OO1Zd7yXM5szQjYbLTFwixbPUODp1Zgctp1M2BDeW8saGN9h7ZARQmFqJjhfFGZE6YrOSkgSKMjsrCPG5fPH9AmUViVSsLpbelkB3e3txOrz/EAQ3lA8cOmlFGSGte/nhiJqwKkxfRscJ4IzInTFZc2Z6AkD4cDsWcmmIev/xI/MEQeS4nlYV5kkgnZI3XN+xEAXNqiweOzZpShMft4K1N7Zy4v1QNFyYOomOF8UZkTpisiIGyi+FwKKqK87M9DUEAYMWGVhoqCyjKj6oah0Mxc0ohb29uz+LMBGF0iI4VxhuROWEyIiFegiCkRuc2WP8S9HUOe1ogZPHmxnb2qi0Z8tnsqiI+2NpBvyR6CoIgCIIwCDFQBEFInrfuhd/tB3efCL8/ELa/n/DUT5p89AZC7F5dNOSz3auKCIQ0H23ryuRsBUEQBEGYgIiBIghCcqz9Gzz5DajaC475IVghePQiCNpX4/pgq/GwzKwsHPLZ7LDR8u6WjszNVxAEQRCECYkYKIIgjEx/Fzz1TSifYYyThkPh0Mug+UP48CnbIR9s7SDf5aCu1DPks8rCPArynHy8XTwogiAIgiDEIwaKIAgj89qN0LUNDr8C3GGDo+EwKKqFN+x7p36wtZOGigLbajNKKerLvazZIQaKIAiCIAjxiIEiCMLw9PtgxS0w/TAT3hVBOWCPY2HTayZxPgatNau3djLDJrwrwvTyAtZs70JrnamZC4IgCIIwAREDRRCE4XnvYehrh31OHvpZ/cHmed0LcYe3dfTh6w/SUOEdOiYytLyAjt4AzV396ZytIAiCIAgTHDFQBEEYnrfuhbKGeO9JhPKZ4K2Adc/HHV7X7ANgWlliAyVivHwkeSiCIAiCIMQgBoogCInZuR62vAGzvwDKpnOxUlC3H2x4BWJCtdY3dwNQN4yBMq28AIgaM4IgCIIgCCAGiiAIw/HRX8zzjCMTn1M1B7qboaNx4NC6Zh8FeU7KvO6Ew0o8LgrznGxo6U7XbAVBEARB2AXIKQNFKeVRSr2ulHpHKfWBUuon4eN3KqU2KKXeDj/2z/ZcBWFS8NFSKN8NimsTnzNljnnesmrg0LpmH3WlHpSd1yWMUoraUs+At0UQBEEQBAFyzEAB+oHPa633A/YHjlNKHRb+7Hta6/3Dj7ezN0VBmCT0dcLmFVB/yPDnle8GDjdsWTlwaF1TN1NLE4d3Ragt9bK+RUK8BEEQBEGIklMGijZE7lbc4YfUIBWEbLDxNdAhqBvBYel0Q8VM2Gr2Dbr7g2zv7GPqMPknEepKPWxr76MvEErHjAVBEARB2AXIKQMFQCnlVEq9DTQBf9Narwh/9D9KqXeVUr9RSuVncYqCMDnY8Ao486DapnrXYMpmQNOHZlhLJEF+aAf5wdSVetDApp09Y5mpIAiCIAi7EDlnoGitQ1rr/YF64BCl1D7AfwF7AQcDFcBVg8cppS5RSq1USq1sbm4e1zkLwmjIeZn99O+mtLAzb+Rzy2ZATwv4mgeqciUT4lUXPkfyUCYGOS+zgjAIkVlBmJjknIESQWvdDrwIHKe13hYO/+oH/gwMCYrXWt+mtZ6vtZ5fVVU13tMVhJTJaZkN9MKOD+x7n9hRPsM8N61mXXM3DgW1pSN7UGpLzDlSyWtikNMyKwg2iMwKwsQkpwwUpVSVUqos/NoLHAt8pJSqCx9TwEnA+9mbZZJYFvh2QPtm82xZ2Z6RICTP9vdM/smUPZM7vyxioHzIumYf1cX5uJ0jqxdvnpPyAjcbJFFeSBXRsUIuIfIoCGnFle0JDKIOuEsp5cQYTw9prZ9RSr2glKoCFPA2cFk2JzkilgVNq+GBM6B9k+nCffr9UD0XHDllEwqCPZGSwVP2SO58bznkFUHLGtY1zR0I3UoGKTUspIzoWCGXEHkUhLSTU/85Wut3tdYHaK3naa330Vr/NHz881rrfcPHzo6p9JWb9DRHFRWY5wfOMMfHgGVpmrv62dLWQ3NXP5YlBc6EDLHlTSiYAgWVyZ2vFJRMRbeu59OW7mE7yA+mrtQrIV5CamRIx0YQXSukxBjlUeRNEIaSax6UXYOgP6qoIrRvMsdHiWVp1uzo4uK7V9LY1kt9uZfbF89nTk0xDkfiZniCMCq2rITK3VMbU1xHqOUT+oLWQG5JMtSVenjhIz8dvQFKh+k8LwgDZEDHRhBdK6TMGORR5E0Q7MkpD8ougyvPuHhjKWswx0dJa7d/QIEBNLb1cvHdK2ntHvuCLAhx9LbBzvVQlWT+SYSSqTi7tpBHgJqS5CuBR5LpPxUvipAsGdCxEUTXCikzBnkUeRMEe8RAyQQFVSb+NKKwIvGoBaOvIOIPhgYUWITGtl78QWlwJ6SZrW+Z58oUDZTiqSg001UTNal4UEpMOJiEeQlJkwEdG0F0rZAyY5BHkTdBsEdCvDKBw2GS4y5ably8rjyjqMaQLJfnclJf7o1TZPXlXvJczlFdz7I0rd1+/MEQeS4nlYV54k4WDOGO8ExJMcSrpA6AmY7tVBYlv5NdXZKPQ4mBIqRABnRshHTpWtGxk4gxyGO61/YIIn/CREc8KJnC4YCiGiibbp7HuHBWFuZx++L51Jeb3eZInGplYeohDZGY16/e9CpHXvciX73pVdbs6JLEPMHQtNrIbF5RauOKpwKwT34LrhTk3e10MKUoXwwUITXSrGMjpEPXio6dhIxSHtO5tkcQ+RN2BcSDMkFwOBRzaop5/PIjx7wjkijm9fHLj6SqOPncAWEXZcfqofHUyeApoYtC9nCnXkmpttTDeumFIuQA6dC1omOFZEnn2h5B5E/YFRADZQLhcKi0KBeJeRUSEgpAy8cw98RRDd+oa9hNbaczxXG1JR5eW9eK1hrTj1UQssdYda3oWCEV0rW2RxD5E3YFJMRrEhKJeY0lHTGvwi5A6zqwAtHO8CnQ2a/5xKqlLrQ15bF1pV58/UFafFK5Rpj4iI4VsonIn7ArIAbKJCQTMa/CLkLTavNcnrqBsqnT4lNdS1mgCRXqT2nsQKnhVslDESY+omOFbCLyJ+wKZCzESynlBRq01msy9R3C6Bgu5lUqf0xymj4E5YDS+pSHbuqy+NSqxYGFx7eZ3tLkq4DVhQ2UDc3dHLxbRcrfLQi5hOhYIZskm9cisijkMhkxUJRSXwF+DeQBM5VS+wM/1VqfkInvE1LHLuZVOtoKNK2GkqngTH2nbWOnxUZdA0B+16aUDJQpRfm4HIoN4kERdhFExwrZZKS8FpFFIdfJVIjXNcAhQDuA1vptYGaGvmviY1ng2wHtm82zZWVlGtLRVqBplBW8gE0dFu3uagA8vs0pjXU6FDUlHjY0i4EiZADRsUI2yRH5i0VkUch1MhXiFdBadwyqxiMFuO2wLHNT+MAZ0L4p2oG2em5cHfXxcMVK5Y9JTqAXdm6A6YeNavimLou8ghJCAQ/5XZtSHl9TIqWGhQyQpI6NH5IZfSs6dhIygvxlK8xKZFHIdTLlQflAKXUm4FRK7aGU+j3wWoa+a2LT0xxVXGCeHzjDHA8zXk2XpPLHJKflY0CP2oOyscOitlAR8Fbj8aVuoNSVetjY2iPNxIT0koSOjSWT+lZ07CRkGPnLZkNFkUUh18mUgfJNYG+gH7gP6ACuHGmQUsqjlHpdKfWOUuoDpdRPwsdnKqVWKKU+UUo9qJSamKUo7Ny8QX9UcUVo32SOhxkvV6xU/pjktKw1z6NIkPeHNFu7NXUF4PdW4xmFB6W21EN/0GJbZ1/KYwUhYRhNEjo2lkzqW9Gxk4RYWQz6oag6/vOw/GUzzEpkUch10h7ipZRyYhLi/wP4QYrD+4HPa619Sik38A+l1F+B7wC/0Vo/oJS6BbgQuDmtE880idy8xbXmdewCWtYArqiSGC9XbCY62goTiNZPAAXFdSkP3eKzsDTUFoJfVVO4833QGlJouhip5PVpSzfTyrwjnC0IMQwXRuPKG1bHDg6xyaS+FR07CbCTxRNvguevgcaV5pyw/PkD2QuzElkUcp20e1C01iHgs6Mcq7XWkSB0d/ihgc8Dj4SP3wWcNNZ5jjuJ3LxWyCykkbCayMJaUAWYxTNk6XFzxUYqf0wrL6CqOF+U1WSiZa3Z6XOl3tF4Y4cJSagrgEBBNc5QH+6+lpSuUVtiDJT1LZIoL6TIcGFcBVUJdaxdiE2m9a3o2F0cO1l88nI46irzPkb+3C6Hray5XePTok5kUchlMpUk/5ZS6ingYWDgbkNr/dhIA8MemFXA7sAfgHVAu9Y6GD6lEZhmM+4S4BKAhobRxdCnFcsyiiroj+4ix+7g1c+HI6+EQA8UVsHFL5okZVeeWVDDyZut3X5+vnQ11508j6sefXegHOCt5xyUEVes1EUfP3JOZlvWQsmQf62k2NxlwmnqCsGvTThDftcmAt6qpK9RXphHvsvBp2Kg5Cw5JbOxOhaMcR3RsRH96u8BmqFqL7houTk3Rse2dvUPCbH5+dLV3HrOQVx6z6q48quibycm4yazlgXdzWZNX3AtvPpbc/zIK8FbbgyT73xk1vaw/LkciutPmcf3Homu7defMg9XGmVAZEyYqGTKQPEArRjPRwQNjGighD0w+yulyoDHgb2S+UKt9W3AbQDz58/PbpatnYv31LtgzkJYs9Qsnp//ETx1xYhVZfzBEM+tbqK5y88Pj59LmddNe2+AKRlQMlIXfXzJKZnVGnZ+ArM+P/K5NmzstMhzQEU++EORUsOb8FUflPQ1HEpRW+phgxgoOUvOyOxwYTSQkn4dHGLz3OomfnbiPhkPfRF9Oz6Mi8zayeNJt4DLA4+cl1AOe/0hfvXsmri1/VfPruHGMw+AwnRMS2RMmLhkxEDRWp+fhmu0K6VeBA4HypRSrrAXpR7YMtbrZxQ7F+/D58LZj8GO98yOyopbzS6Ltxx62+DFa+Erv4GimrhLRSptvLW5nUvvWQUYF/Djlx+Z9mknSth7/PIjh234JOwCdG0HfzeUjs6DsrHTorbQOAsDnilo1OgS5UvEQBGSIFEYzcIbINhvjJOi6qiO7dpq8v0Kp8RdJqJfY42U+nIvDocj4zpP9O0uhJ08PnGZkcdh5DDP5aTZ1z+wtkN6wwlFxoSJTKY6yf8Zm74nWusLRhhXhemh0q6U8gLHAtcBLwKnAA8A5wJPpn3S6SRR1Zi+dqOoph4A+SXxO3wn3GjbvClSaWPwDkg6wg3GMzlUyHFawxW8SlKv4AXhEsMF5rV25hHwVJCfYrNGMJW8Vm1sIxCycDvHJw5bmIAk0rFlM0A5zE3hYC/KonvBWxHnRcmkfh2M6NtdmETy6C0fVg4zJX8RWevxB0XGhAlLpkK8nol57QG+CmxNYlwdcFc4D8UBPKS1fkYptRp4QCn1c+At4E/pnnBaSVQ1pms7LLsazvtLVGGBeX7qCjj/r0MulalKG3au3/suOtR2N1Hqok8CBkoMp+5B0VqzucviM9Ojx8bSCyVoaRrbepk5JQ0xDsKuSSId2/KxeX3UVUN17INnmTyUGC/1eFUyEn27i5NIHvNL4ZELEsphJuQvVtZ+ePxckTFhwpKRLUqt9aMxj3uB04D5SYx7V2t9gNZ6ntZ6H631T8PH12utD9Fa7661PlVr3Z+JeadMorr7dlVjTrrFuHXPeQJ0yH63xQrGXydMJiptxLp+D5hexg+Pn0tXf5Bbzz5I6qJPRlo/MfHSBZUpD23u0fQGTYJ8hFH3QikxsrdBOsoLkJqO/ept4Ck1XpSK2fY6NtA7Lvo1/kfQbO/so7s/yA+Pn8sB08vikvFF305QYmVTOeH0++Ll8bS7we0dUQ7TLX+xa/stL63jhlP3ExkTJiSZ8qAMZg+gesSzJhLD1d13OMxzpGqMcoDfB/eeYs4962H73ZZAHzx7NRxztW1Cp/007Ct0jFS5IxJecMD0Mv5jwZyBCmFfmlvNfRcditOhpOLHZKJlLZRMNbKaIhs7wxW8CqLH/N5qyre+jCPYh+XyJH2tqWXm3E+afHx+r5oRzhZ2aVLRsU439HVEdex5S+11bNAPO9dDxayk9KuZRmJdOpKetfOcXHfyPH69bM24JeMLGcBONs9+HC78mzE+0LDsv+GQi+3lsKMR+n3DrvNjXdsj5Lsd/OzEfSjIc9LjD5E/TiWMBWGsZERSlVJdSqnOyAN4GrgqE9+VNXp3moS3k26GRUtMzHOk7j4YpVNUA2XTAR1dOAFevs5UnIndbTnhRnj+p3DopSZhvmvr0F3DQdjV8F+zo4tg0LI9blnRtKBIcuhlR88eME7AVLA5848rUErJYjmZaB19ieEBAyXGgxLwhksNp5iHUuxxU+p180mTeFAmNZZldKDfZ/L26ufH9zaBeB1rBeN17PIf2+vY5ddA1zajv+08M0OmYa9jLUsP+1kEuyTlqx59l8uOnk19uRclfSgmJnbr/5KvGjlsXQt3n2gqdr74cxM9ESuHJ94ELm90nbeRvXSs7QCXHT2bK+57i/PvfINFt/2L8+98g8V3vM72zr64MYKQi2SqildxJq6bM1gWdG6Fpd+NLymoQ6buvm9HXC8TQoH4HZTGlaYc5nlLzXW6m+GFn5rjO94zC3JHI9yxAMoasBbdB9VzcTjj40YTVeh46NLDR6zcEUnO6+63T6Lb2t5LR29AyhFOBkJBI2/1h4xq+KZOCwdQPciDApDv20Rv2R4pXW9amZe1O8RAmbTY7U6fcCO89xDseVx6dOy5S+GuhQPXtxbdR0vBbJQjfkd6uCpIwIh6NlEifGVhHtefMg9fX5AphVp07ETCbv0/7W7o7zKfuwuisti40twXLLzBHO9tM3LpaxpY50M9bXSW7EGpN39EuUtlbb/47pWUed2yvgsTlkx5UJ5P5tiEpafZJLoNLikY7IPf7Q9//KJZYCM7I053dAclgq/JdJG/YwE8eLZRZJFrFVaZBTX83vHgmfR17Biy45Fo8QuGrBErd0SS86aWeW072UYUZGu3fxS/IGFC0bHZ7PwV145q+MZOi6oCcMdoE39BuBdKV+qVvKaVe1nb5ENr2eGblNiVbH3qClOefdnV6dGxob646zsePJPNjZuG7EgPV2krmSpcsbvZEerLvZR63fzq2TUsvuN10bETDbv1/6HFZv3v2QkOZ7wsKgfceyrcuTAqhzHrvPPBM2ls3JyU3KWytj9++ZHUl8v6Lkxc0mqgKKU8SqkKYIpSqlwpVRF+7IZN9/cJS6KSgu6C6OsHzjBGhm8HOPPgtHsGJdDdYxLoBi+qcxZCcZ25WVy0ZCC0ob3LN0SZJFr8HA7Fn887mAOml8UdH1y5w+FQ1JZ4uH3x/Lgkuj+ceSDPr94h5QgnC20bzPMYDJTagvhjIXcJIad3VJW8ppV58fUHaerKjVoYwjiTSL/2tMbfFD5wRjQU1l2Qmo5FGe9KjI6tLlADO9IRXZtIxyql8ObZfxarZyO72bH69eazDqTFZ2RbdOwEZLj1/8GzoHhqfHhhoCf6un6+kbkLlpl1/uNnoX0TZXlWUnLndCgeuexwbj3noIH1PdHaXlWcT12pV9Z3YcKS7hCvS4ErganAKiDiO+wEbkzzd2WPRCUFe9ui79s3mfjpe04yrw//Fpy3FG2F0A437c4KlMNB2en3oyK7hXMWwlHfh7uOj3cdB/voDLgpHqRM7GqoX3/KPL5531s0+/q5/pR5/OrZNTT7+hNW7nA4FHtUFXH/xYfhD1o4FLT4/Jx5WAPtvf4RyxGOlLAnTAB2RgyUulEN39SpOXhwCQyl8BdUj6oXyrQys5iu3eGjpiT5BHthFyGRfo14lSO0b4oLheXcpaZUeyiAdrrpck+hL2AxZdF9OB48M17HLvlaVMeeehe8/zhNPWb3OvbGzU7HXnfyPK556n3+37FzuPuCQ1h8x+td+E9HAAAgAElEQVQJe1hEdrMfu/wIevwhgiFNrz9IX8DixyfM5aYXPxEdO9EYbv1v3wTdTeB0oc9dCtoi6CrAueg+HC/9wuSYxvZEOe1uOOBsugN5I8rdLWcfxE+e/oDnVjcNyOFdr23g/x07Z9iqXNUleQPr+7aOXv7w4lrOPWJmUut7BJFBIRuk1UDRWv8f8H9KqW9qrX+fzmvnFJESl7Ex0ifeZGJLI5Q1wM51USX2z9/Bh0/Qu3gZX7r1owGlc88FB7PbhctRIb9pw/3nLw91HS+8gWl5HoJ58TeQsa7c3kCIdU0+fvXsGt7a3A7A9x55lwcvOWzEyjNuJzS29fC9R96NM3SuOWFvLMuiuavfViHZVai59ZyDmFNdjEsqhUwc2jaYEJlRlBj2+TU7+3RcgnyEgKdqVKWGp4V3+z5p6uKze0wZ4Wxhl8NOvy66F176Zfx5sUZL+yaTU3LRcqziaWG99BqNbb0smFvFbxYvw+sIoex07MPnohc/xcPPmw2m2B3piI596NLD2dreS2u3n18vMzp29bYuHrv8CNsqXIN1bJ5Tsa69d4iO/ckJe1PuddPc1Z+wSthgHXv74vnUlOTT65ebxaxgJ58n3GhynCL9zh48G1XWwHtffoyv/PkNFsyt4qYTrsN557/bru91bg8L5lYNkbuIbCmluOap93ludRMQLbbw0KWHU1visZW5cq+btc2+IcZ1c5efqx59l/suOjSpcsOyzgvZIlPSVR1utgiAUqok3F1+1yC2xOWV78OFy83us88oj4EF9eXr4seFQ7ViE9zOueMNWigzlWi0Tug6Ln58MWVWh81UjCvXqeD8O98YME4i1wfiqsPYVQfp6A0OLJyRcd975F38Qc2hv3jBtlII2CfyXXrPKrZ29EqFkInEzg1QVJu2EsMR/N6wByXFXJIyr5vCfCdrpZLX5GSwfr1ouXl/zNVDq3K9+tvouPZNEPQP0UvLVjfzpds/osVZnVDHKt8OTp1bkNALorXmlFv+yaX3rBrQsY1tvQSC1pAqXKnoWEvD2mZfwqpMiZKl39nckbCKk5BhIvJ54XL45psmAf6Fn5r1P1Ymw6FbYGSwqaM34fpe+sRifnN8/RC5i8iW1nrAOInQ2NaL1jqhzG3p6E1YQa6xrZdkJUbWeSFbZMpAcQKvK6XmKaWOBd7AhHztOsSWuCyuMXX1YxfUkqlRgyVCWQPbfPElBePiQCOu40FjIq5jFUqc0JYoZnWwC7fF1z9E2TR39dsn5IUTUAfHZUdIlMjX1NU/puQ7y9I0d/Wzpa2H5q5+UYKZZueGUeefbLIpMRwhUFCNM9SHu7d56IfDoJRiWplXSg1PZmL1a1ENOF3xRsv5f4UVt0YT38HoSlfe8MnriXRsdzP71RXw+OVH2lY2Sla/gv0NXSIdG7K0rQES0Z+JfpaCPKft+aNB9O0ocDjMul8+E+r2g5P/FDVUIjJZ1jAQNgiw1WcNu757HaGEnrCR5C8VmSvzuqkv97K+uTspucnUOj8WRGYnB5nqJH818H1gBXAXsFBrvevkoNgxeEH1VgzpdGwtuo/bVnXGDYtb5Oy6I0d2ZMKLbyLskjEH7wRalqbbP7SscGu331b5be/oG3hvl1CXSGlG3MyjIZneAkIa0RraPx1Tgjwk9qAAo06UFw+KEEfcptDUoR6V0++Hgqrhb+YKqox3e7COfft+XHn5CXuRJKNfI9jd0CXSsS6HGrYqU6Kfpb03YHt+qoi+HSMRmSxtMDIZE0XRfuJd/Pyl6ObMbas6TcuABOu7GsP6norM9fhDXHfyPH73/Nqk5CYT6/xYEJmdPGSqzPC/Ab8Dfgq8BPxeKTU1E9+VsyQIU7jy2L0SL3LDuI6tRffRFCpOuFsQG7P66lXH2O4Etnb7+bSlZ4iyeXTVZm49+6C4ef3mtP244bmPB86x2y2sLMzj1nPix1138jweXbU56eS7wSQKaZByiBmiuwX83aNOkN/YaVGaBwXuoZ/5vaYT/OgS5QvY2e1np/zdBTsShYE5HMPfzEXGLX7KVFJacC2suBV9zNW0qdKEO7LJ6NcIdjd0j67azE1nHThkTp4RKoHZ/SzXnzKPW15aZ3t+qoi+TROD5FFfuJy2oj1o9hlDsr7cy5XH7pWx9V0pZStztwxa128+60A8bge/XmaK5yQjN5lY58eCyOzkISONGoFfA6dqrVcDKKW+BrwA7JWh70svoSD4tpvmX063ic93juJXFdldibyFuMQ32wTHiOvYqgJPKfqUO9FWkA0dFv/x2CqafQFuXzzfdnF0OBTlXjdNPosef5BAyKK6KH8gkc0fDPG759dy3cnzBrrH15d7+ebn92BauWdgXm6XA19fkOZwKcxEu4UOh2JOdTH3XXTogLs3maoiw5FMbwEhjbSNtYKXRa1NeBdAwDsFjRploryp3vVJk49DZlaMam5CDmNZpht3oNc0snN5TV8IRwp7ZoP0a/SwGl7POl1QNgPyCqBsBnrqAQRDIRobN/Oj5dsT6liHQ1FZmDewc7y9sw+nAkfYKIqcG7mhu/SeVQM69twjZrLknxv54fFz+UxtMd4814COHFytKVbXDv5ZktXNySL6NkmSuSeIkUcFzCjU9jI4ivXdsjQt3f30BUI4lcI56N/EqRiyrp97xEzyXYqfnbgPu1cXobXm50tXD1QBS1ZuMrHOjwWR2clDpgyUw7XWA9KitX5MKfVyhr4rvYSCsON9eOicmFKA90DNPskZKZZlGjkF/SYkqyB+0Y0kvsWd72uyP79rO+qBM1Dtm5hd1sAfv3IXFz3bzcV3r+Sxy4+guji+BGswaPHRji4uWxJdGG85+yD2qjHVNvJcTpp9/fx62Rp+ePxcyrxuevwhphTlUeKJN5SmJFKug3C5HNSXF+DNc1FX6uHAhnljqioT2X2MVUBj2SEURmDnGHugdFjsXmr/mXa4CXgq8XR9mvJ1p5WZmLG1TV1ioOxqWBbsXA9d2+DJy6N69vT7B7wgI44fRsdCjJ6NnNs56FyHAwqq0E2rUQ+cgbt9E/sO0rGDKyDaVTOKLfUauamM3NA9cMlhbO/oi6v89dr61riu3zDyptXgNSNZ3ZwMom+TYKR7ggTyGPd3syzoiVnnvZVJr+92cnf9KfOoKfGwW2VhWOYc3PXahoF1vb03wF2vbeCMQ2ZQW+oZKN3+P1+dx4+/krrcpHudHwsis5OHTCXJT1FK/Ukp9SyAUmoucNJwA5RS05VSLyqlViulPlBKfTt8/Bql1Bal1Nvhx79naM4G3/aoIoJwKcBzzPGRsCzT3fiPX4Tf7jO02/FgQkFo32huEre9DU//P0I7PqCtuw/dPbSbcuXT5/LfR1fR2NZLrz80xBXc5OsfME7A7CpctmQVTT7jNnY64NazD6LZ18+l96ziT/9Yz+7VRQRCmm0dvezsNuelWvM8ttpIohjuZEkl1ltIA20bAGW7Ez0S/SHN1m49pEljLP7COrwd6xKfkIDKojzyXQ5JlN8V6WmGtvVR4wSijRd7RiiokIqOtdGv1o4PaOowYVy6uznagyo8h1gd29jWGxffbhdactWj73LyQdPjQkwsS9PWG6Agz0FVcT61JR5+dco8ln7rs9x30aGUe+PjISOemTyXE38wRGu3f9h4etG348xw9wTJyKONHOqOzcOu7z390fXdTu6+98i7bGztGZAVpwO+/YU9+dkzq1l0279489NWfvSVvZlVVUip141l6ZTlbDDplLuxIDI7eciUB+VO4M/AD8LvPwYeBP40zJgg8F2t9ZtKqWJglVLqb+HPfqO1/nWG5hpPKGBfCjAUsD8/lp6hRgUPnGHiUguq4ndZvJVGkT14VlwtdedLv6D18F9QVhy0nUd1gYk1DYTMghm7sxYIWfbVuELWwA7MEbMqufP8QyjIc9DWE+CM2/8VtytTX+7F1x8aEnKQKN46WZI1ekYMzxDSS/smKKgAZ+rKfVOnhaWhvijxOf2F0yjb+jJoK6Uyxg6p5LXrEvSbrtt2ejY4Qhx5sjrW7YXOrUP0q+OlX7B53o/49jNbefGi2bht5lBf4hxIRI/Et0f0UaKqSJEQk8hu92/+toYLPzuL7z78Tpy35f+Wf8y3v7hnXA+JRL1OxqJzRd+mkeHuCezk8cVr4d9/ZQqQJJBD1d9le82I7G1o6aYw30VVcf6wldz8wdCAvJ1/5EzuufAQCvOc7Ojyc/pt/4qLpJhTXcQnLd1pX9sjjFczR5HZyUPGPCha64cAC0BrHQSGDRDUWm/TWr8Zft0FfAhMy9D8EuN0GyVSPx8WLYHzlsJZDxtFMxJBv70is9tl6dgcVVqR8566AvY/g5mlCrVznW1Jwna/g+tOnkevPzgk5tLtdNgmXDodiovvXklVUT5nHtbA5p099AesgRhpiO7K9AeHL3s5GlKtupErOzWTgvZNUDS4DXxyrG83u4TThjNQCqbiDPaS15OEB3IQ08q8rN0hBsouhysPHE6jV89bavRs/fwRKxUCyevYrW8m1K9713j54fFzCWllq2OLPHlcd3I0ET1ifAxXUau+3Ivb6WB7Zx/d/UGu+vJnBoyTyDUi3pbBPSTSnfQr+jbNON0wZ2H0fmDREvPe6R4qj/XzTbf4P395eDn0lCSUvZvPOjCuwlYiuevxm89XrGvmP7/8GdxOBx/v8OHrD/H1BJEUmUouH+/KWiKzk4NMGSjdSqlKML2AlFKHAUO7DCZAKbUbcACmTDHAFUqpd5VSdyilyhOMuUQptVIptbK5ObW+C3EU1cKZD8MXroFlV8OdC2Hpd03H4kShWhHsauzPWWieu7bCSTcb5VZUDb4d9gttYRVKh0yTxxNujCtJqBctodtVxl2vbaDF54+LubQsTciyuHlQpZhbzj4IgF9+bV+uOWEuvf4QP3zyfZoS1Eh3KGyPjyUBTapu2JM2mR0L7RuhcHQGyoaOsIGSIEkeoL/QFO8bTZjX1HIv2zv76OpLwnspjAtpkVlvpfGgLP2u0a/Lrjb69uzHjRdkOBLpWCtodq4XXGtuIitmDTW8w/r1gx29PLpqMy63a4iO5YQbyXe7B3JGIBrfbhdacvNZB1JRkMcfzjyAzr4A1zz1Pqfc8k/auv3Deltie0ikO+lX9G08Y5bZwho46vvR+4FlV5v3hTVD5XHBLyHUH13rC6vs13kctrLndLrw9QfjKmyVe91DqnHddNaB7F5dyOa2Xo7co5pf/vVDTrnln/zsmdU4EpSuDlo6Y8nlInNCJshUiNd3gKeA2UqpV4Eq4NRkBiqlioBHgSu11p1KqZuBn2GMnZ8BNwAXDB6ntb4NuA1g/vz5ozfbnS7wlsF9pw4KIzjThBEMF6tfUGUW2bb1ZgEGKJlmYlWXfjfOxUvIb17HKq+yBoIF1XyyM8ReviZTgnDBteAth0APa3pK+I9H3htIkIuNuWzt9nPG7Sv4w5kHcOf5h+B0gEJx7V+iVTtuOfsg/vzqJzS29Q7s+g1ONLM0o0pAG869K1U37EmbzI4WK2TCD+oPGdXw9e0W5flQaFNiOEJ/oXGCejvW0TH1cyldvz6c2PlJk48DGmz3JYRxJi0y29sKD54dr1+fvNyUXx0pQd5OxxZMMc3uDr3U7E5H9OyJN8Hz18Q1zgsUVPPwik7OPWImH25vY983b43q2N42WHErweNusK2S5XAoakry+fWp+1FTks+nLT386MkPaPb1c8Op+/GHFz/i5IOm89zqpoEeFIP1aETvtnb7qSs1SdDJJP2mEj4j+jaeMcts3077HJRIWOHp9xvj+MhvmXDZvjZY/mNT/ObUu4wBvWZp9HplDViAY8VQ2fMfez1/fnVDXE5FW2+A3z3/Mb/82r5MrzAy/z8x1biuP2Ue5x85k+dWN4VDurWtPLkcKm3J5YPl0bLsw8snq8wJ6SFTHpQPgKOAI4BLgb2Bj0YapJRyY4yTe7XWjwForXdorUNaawu4HRjd3VQqJIo5HSk+GiDYF90ZfOLr5r2/O9578tQVoP4/e2ce3lSdtv/Pydo0aZtCW7a2ggwiVREEAeH9jSgqKCgqArIvo6C4jL6O4qi44fiKDjMObuDGjgIqojKKiqIzqKgsLiCI7BXoRtMtabZzfn98c7I0SduUBhDPfV1caZNzTg7wXPdznu1+dCgjl0RkUHzDl/DApw7+uvYwZVcuFAS3fCy8fTOKrTVtWrdmxZQ+tMu0YDHqKamqDeqmq06p1itzyT8+4+eiasa+spEPt4vFUWqZd1iPPADmrt/N7OHnRmUD01P0vFhH87yhAbSGyruJbGHWcBxRdURknpswIA+wp0KmbT3VEwC/KR2fwYqlMvEKSm6mcMY/F1U15fY0nKyI16blb2S2tS7Hep0iqWQwiwe+3J6hoOfC6eIcez7e4UuY8amDiwpaM/3N71n6fRXeP06PyIzL/f+KIS2b1bf25b/TL2LpDb2xB4aMAVwePxUuL+Ne+ZpJC75hy0EHheUu7lr5HcN65GEPDMDPXb+bWcO6Ru2O2LyvjEWTe9E6PQVJEspgDQ39Jto+o/FtMyOevfo8IqDOPhP63wtv3QirpojPr54LV86Bz/8Olz4a4ee9w5cwd1MNFRfcHWV7ToOdh648i2ybKTjE7vH5+XB7MWNf+RpZVhjzcqRfv/uN72mdHlL0fPGz3bxQp+Iyd2wPUow6Xhp37MPlseyxtMbDZQWRFUvN5jQcK5JVQflSUZTzEIEKAJIkbQbOi3eCJEkSYoj+J0VR/hH2fhtFUQ4Hfr0G+DE5txwGtWxbp7rRYH903YE5W45YhBcupXnVs6IyYmuFtOFpGDwbpUVHXJKFR9cdYXhBKjk9rVQZLGSMexdDzRGoKUFa/wSpf7yXe9bVsnZ7SYTE5b2XdwlmR9QMndpKEI7CcleQjLYcdPDE+zt4dlR3MlJN7CutCWYDnxvdnddu7INOolEDaPHKu6qcpuqA42n9azhBCLfTJmCPQ+b8hk6VJDzWtlgq9iR8/Zx0MyaDjp1HtDmUUwpN5VeIzbGeGlg5IZpjC7+FzA4weS2yrRWvb/czvMDH2a3cnDYsnzZmL8bPZ4qgxpqNbGvFU1/WMLxXLU6PP0quvXOODX8goIjHrWpLy5aDDhZ+sZcFk3pRVeslLcWI1awjLaUd41/9OoIHO2XbMBt0zBx6NqkmPU6PH7MhlDtsiF/rQuPbZkZD9uoqE3Mmthy4+MHIKt5Vz4rA+co5YLTgs7bmV6eOYZ3dWG1ZInEJ4HVSIxu4eelmSqq9MX17YbkLvxK7TcuvhILVL/aUcedlnVgxpQ9uv8K+0hpmvP0jJdVuFkw6n7du7ovXLzd5uDyWPU5dvIllN/Rm++EqzeY0NBuaNUCRJKk1YrDdIklSd8S+IoB0oB4xUgD6AeOAHyRJ2hp47z5glCRJ3RAtXvsQFZnkQi3bqo5Q1ehvqD+6bqal3x3RUprv3Co2yFYUwpYlsGUJkj0fw+R13NsT7KuHx21RMBb9wJRLV7J2e0lw6HLGkAL2lzl57ev9QVKbNawrTo8/Zjk3J81MbqaFbJuZ2wd0oqXNHFTyUnHLsi0smNQLk6FxxNVQS4GmunGSoiKw4b0JMygVboWjtUq98ycq3Na2WI9ua/jAOtBJEnmZFnYWVSZ8roaTGE3lV2g8xw58XGSnS3bA8rHoOg9mTP970QUGls9U+bWmSFSpAZ09n0suXcn+MiczVv8YNWS8fEofHluznXsv7xKTW7PTzLz4+e7g73de2pksmxGLUYdfUZBlmLZ0c1Sg8fqUPuwvczJn3a6IuRc1AEm0ZUvj22ZGQ/aq2uTAx0PBCYRsccJ7sPhqyO2JYcDDtI+VsCz8ljR7Pg9cupJhi3fH9O3T3/yeIxW1MW2vtFoExpcV5HD/4AJ8fgWdJDHulY0Rx06c/w0rp14QITWcqG3Es0e9TtJsTkOzorkrKAOBiUAuYlZEtc5KRLARF4qi/Dfs+HD8uxnvr3HQ6cTCsBs+rncZWBTqZlosmbFLw5kd4O2bIt4z4sfuKxUZFVc5bHhaON6BjwcdqCozrKKw3MUfcmxIwLAeeaze8ivDeuSRk2bCYjKw+E+92FcqHF9JtZuXxvekbYaFd27tx2FHLf9a9zN/vbwLs4efi8PlZe763cGWBYfTw3Vzv2yUFGFjeqijFlRqOPFQbdPaiAfDOggOyNej4KXCbW1L5qHP0Hsq8ZvSE/qe3MxUfvy10foaGn4LaCq/QuM51podevgD6HMTuuojDfJrbrqeKrMldpZaVrj38i7oJHh1Yk8mLxBZZPWhUK+TePiqs5k5VGyXz7QYOVDuZH+Zk1STnpx0M9k2c8S1C8td/FruYsbqH5k1rGtwOD88AGnKYjqNb5sRDdmrapPxbNFVLn4e8BD4XJE2qAbTgZks1b8XlrvolGNDr5P40/+cjtcv89R1XbGZDbw6sSe/ltcGq22ntUjFbjWy8a8XU1rj4W9rtjOsRx6dcmzMGFIQ9OvqdWt9MuPmfdlkqeH67FGzOQ3NiWYNUBRFWQgslCRpmKIobzbntY87dLrEe/MtLWHk0pCsoNcZuzRcXRQa3AQxRFdTHD1I/8mjgvTCzi12hkq5uZkWDpQ5mbTgm7Ae56O0Tk9h0oKQBvq8sT1oY0/BbhEZDb8L/rXuZyb07cC4sHYD1UGWVIcUZtQsX6zN9Sq0loLfKBwHIMUOxtj/r/Vhj0M8PDW2ggJiUL46u3tC35PfIpXPfi6htNpNlk1zfqcMmsKv0HiOtbSA1TcLns3tCeY0WH1Lg/xqTbWyvcQV8wFsd0lNkGvnje3BvHFCIREFxry8MeqBz+HyUFRZG6zGqAPNT34QqRCm7ltRs+ZTF2+KCEA0fj0JUJ+9qjZZfSS2LVYcbJwNhvl3dU4k3K5mDz8XWVFw+5QIm3ppfE/sFtFe+K+PhV+f/ub3UX59y0EHuZkW9pXWRFXxVky9gNbpKY0KUjR71HC8kJQh+d98cNIUyLJoJ1j/hMiITF4rsi7XL4uSCpYz24fkh+35KAP/hlRX1eadW8WAp9cZPM47fCkvbhLtLqqzm7NuFxDS2b/6vFxuWRbZRjB1ySb8MhGqWsN65AVJLPz82wd0itgBoH4Wvtm2LsJbCjZMv4hV0/o12/InDUlExcEmVU9AVFB0ErRuVICSC0Cq4+eEv0dVrfn5iDYo/7tHIzmWkUtRjBbBn7k9xeuK8Q3yq2PoQnZWmpmzblfUgHtdrp26ZBM2s4H0FCNT6+ycUOVVXR4/d78RybF3vyE4Vr1u3X0rdosx6oFP49eTGOE2aUiBEYui/D3WLLjogfg26CoP2t9j68V86XOjz+OJ93+KsJ27Vn6H1WyM2nGi2lt9fv2m/h2DQjiqHasoLHdxyOFq9N4SzR41HC8ka0j+94fw4U1VUtCeDzd+KuQzPdVwdDfSmv9Fqi5GGbkU/xWz0Sl+JDn21ngluwsyEtLtW5ElA/vdadw32Mjk/6klJ83M/674LpiJA0E08bbJh/crGw26uIOeHbKsPP7v7RHXrbvZNha0loLfII5xSWPrVDA2IsXhtWTj16eQWv5Twt+TF3hI3HGkir5/yEr4fA2nEOrjWLX9RvbB2vuRdq4JPSCaM5Bi8Kuc2ZFqnRXzn39Chx/Z6eEMcy2dc6zoJFg8uRd+RcFi1HPrsi1RXFtS5SYn3RyXb+MNNHfIsrLufy/kwFFn1L4Vdfakbv++xq8nKeraZG5P0cKV1ibo76kuRhn3drQN2nJQss4ExY888d+4acE/Rir8XFSNTiKo1KWioT1l6txHrM/Vdi91x0o4VNnrO5ZvjSu8UBeaPWo4HkiWzPDvD7E2yg58HNxVIHvhwxli+WK/O+DqF5Cqj6DHj27B5UhFP8RcPuavOYp+weXo5nTDsPAKslx78Pp8PPH+Dn4uro5JNH5ZqVdiUpYVqmt9tLCaYh5XWO5kQt8Owc8uK8hhyZ96k98iFbfPz9Ead9K2w2o4jlCUQAUleRLDQUg6atPysZY3qDQehQyLkfQUgyY1rKF+jgUwWmDt/dBtlFjWOPBxpPWzQGeIubXbrbNw93sHqD56BOPCK2jx0nmkLRnEzL46FmzYw0WzP2Pi/G8od3rJTotsX1Ef6vSSFJNHjQYdErE/Kyx38peV35FlM1FS7aZ7np35E89nyZ96B6SH5aDErIaTHOE2mdtT+HdrNlQdFhWVSx4Rdlh1ONIGc3vCgIeRFg5GmtMN/YIrSK34Gb/fz9z1uzkUGIYPR0P+PdNiJDsgglP3813F1UxdvIknP9gZrA6qdrdwci9y0kw8ce05OD2+4OoCDRpONJIWoEiS1FeSpNGSJI1X/yTru04KhG+Uze0p5AbX3gdzusH8y+GCW2DQkxHb6aWaEpHB3vB01FZZ32WPYVgZ2fZlXz2BNjoHD1xxJnPX7+ap6yLbEOaN64GCErV1dt64HsF2gbIaD+Nf/Zq563fz3OjIrfOzhnXlna2HRPbwT73YMP0ibru4E2Nf2ciAf3zG9S9+xc4jVewrq4kgMFlWKKly82u5UyO33wpqSsDnblIFRVYU9jrkRs2fqHDb8kQFRUnMNiRJIq9FKju0Fi8N9XHsy5eIpaP/767Ijd+9p4rlu0Ofj2wDG/o8ZS4/U3qk0/LdCRE8a1w5hjv7tgBCCl73Dy6I2ivRMduKUa+LucPEoJOY+d62mLum9DqJ+wd3Id1i5JlR3Xh06FnMWP0j/f++nhHzvuSXkhruX/U9O4uq8PlkjVtPZqg2GW6Pz/US+3mQxMLGtfeBwQIjFods8MLpUepzaavGIzlLuX1Ap5j+/fkx52FL0Uf5d7UdsNzlZdlX+6L8+vNjzmPd9iLmjevBvZefiUmv462b+wbt7i8rvqPWK3PvWz9w4VPr4+7Z0fy8huMNSUnwgaFRF5WkxUBHYCug9hYpiqLc3uxfFgM9e/ZUvv322+/R1REAACAASURBVIYPbCpkWZR2wxU9AIq3i42yAx6M3EQPgpgGz4alwyPfUxU8uo+FC24T1/P78EpGjM92i/7uyWvxGazsIo+MVBO1Xhm9TsKo1+GXZUa9tDEoIXxay1QMeh02s45ar4JJL1Hrk/k1sEl+3fYiBhS0wm4x0tZuYcmXe/lj51bBHtb5E8+PkNsEQXgzh57NWe3SyUlLCS5tqjsw19Se1EQ2JjcjTnjzbNJtti4KN8HLF8NFMyC/d0KnHqyU+X+vVXNbVxh0WuPOyTz4MW13vMqma/+DJ7BdvrFY+MU+PttVwraHB2p9ziGc8H+IpNpsc3LshPfg8ycFv+r0IPvhy2dw972L4moveQujd//6b93MdcsPseWgmPlb/5f+6HUSHp/M4QoXi77cx4S+HVj4xV7uGXQmqUY9PlnBoNdh0Em4vH52HKmK4FivX6ZVuplJC0JcuWhyr+BeFBW5mRZmDClg5nvbeX1KHx59d1twa/ixcGvon/aEcCycSjYbbp+yD2odkTMmEOnfVTv0uoQN6gwisK4D/62bOaxrjYzI5SgK6CSRqJn5nrCDywpyhJqcTsJs0KFDfO7xC9/u9YtnAp0k4XB5OTc3g72lNcF5qNxMC8tu6M3owAD+vHE9mPne9igbDG/3am4/H/uf9ITZZX044Tfwe0ayZlB6AgVKMqKfEw1ZFk4yXBN95FLI7hLaKFvriC03aEyNfi+tNYx7GzJyoXyvaAOrLsYw/p3YiiA1JRjW3og0ZBVlchY3B3T11UHObJuZLQcdQbWZmUPPxuOXeXPTQW69uFNQhz9cFUSvE+Q2oV8Hdh6pDsoO21NjL3tMNenF0LxVqXeJmLq4rLGEczxIUEMAFU1f0rirXOQc8tMaf05tmsgcWo/uSDhAyW1hweXx86vDFRya13AKo7k5VvZDvzvBWSoqh1tfgz43ozOmUOz0kBeDZ/Xlu3mgfz7DFlcGZ/DC1RJLqjxB1a2J879hxpCCejkWIL9lKocctRGy7kdrPDE5Vl20e6Silgl9O1BS5WHLQUfcBY2NfbjTOLYZIMtQvA1eHx1mn0sEl4bbkeNASKHLcQAUWdhtTYl4P47dpdtT2O+2Rvn2kiqhrPnh9mK2H65i5tCzAZizbhf3DOocEYCodtfSasLrV1i/o4gZQwqwW4w4XF7Knd6g3cVb6hw+t9rQstBjDS40u9QQC8lq8foRaJ2ka59Y1N1k7DggJC8rC4UDXD5GZEli9DwHFWPC37PmiBmVt28SMsMXPwi2HKS19+MdviSyLeGqZ0U7mOMAp2UYggQGIYWYm/p3DF5eDSbsFiPDeuRFLQm7a6Uo7V4390tmvf8TZTVeZqz+kZEvfsXM97YHFWXCkZtpwenxs7e0JkhIschNlmV2FlVxzfMb6Dfr07hl43DEI0FV8lhDM+IYtsjvKhc7UPIasQNFhduWB0CqI/E5lNMCQcm2Q9rCxt8FGuLY9U8IeezGcqzOAOV74MP7Q21fX72ADpkXN1XiHxGDZz+bRU6qFFct8ab+HSMCCbvFyPgL2sfl2Cfe38Gv5S7+svK7IL8+OvSsuLOADpc3OOeifp96zboLGtWHu8ZwrcaxzYCa4lBwAgH7HCvatsJhzw/tQLHniwBFDZDrtnyF2Z1Z8jXat6ea9Nx12RlRanHhvn3me9sY0i2Xme9tD9peRphvV20tHLmZFiQpFBjUtyw0EfuLB80uNcRCsgKULGC7JElrJUl6R/2TpO86vqg7qAmBLJ1XBCa2HDCnR/c8D3tZ6KXX6YPmzckRgQnv3CoG7XauQW/Lwj/x30JOc+DjwY2z2PORDLHVY+wWY/B3NZhwuLzkpMU+PtUkhueH9cjj5iWbyLaZmTeuB7OHn0tptSeq3/Wp67qSaTUyZ92uYLYk5jCfQsKEk+jGZA3HAMdBMFnFnwTxi0Mm0wxpCcjeywYLbkurJil55bcQqkrbD2kLG38XiMuxftFO03sqrHs0am6PEUsgNSvyvWtehDcnRXNst1HIPi9T/l9HfNY2ojUsMFjPJ49CdTHZ9nRen9InYm8JRMoBqw93sqLQxh57wWOqSc9N/TtGPERm28yUVnsw6KUojp01rCtvbjoYlCAO5/VYCxoTebjTOLYZ4HXFX8Bc1/Y2PB36edWUUID8n9mQkhHT7hR9bCWuWL69IbuD2L79SGUtCyadL+apYsy7zBrWFX1Y4SKenzcZ9M0SXGh2qSEWktXi9XCSrnvioPacQuzWq/J9Yuj4wunwxkThCAc+Lkq5Xqf4bN194j1rtpAh/PzJ0MLG8I2ygfKvrngbFSl5IJvJWHtfsJzsH7mMMiWdywpyIqQILyvIoaXNzPIpfXB6/LS1mzEbDJQ7PbS0mWMuH3O4vOKvYDGSbTPzl4GdI5Y8LZh0PiunXoDT40cnwZHKWh55Zzsl1e5gKTfW0iYljsRmfYTTlI3JGpqIioOietcE7Cr3k5dAe5cKty0P69HtCZ9nMuhoZ7doFZRTHQ1xbMlPkNVZcKXjANQUhfg0vS28f2/oPZV3dYbYHGvNxli+i3NSW/PyNyZGdsii5dsTIji2QpdOrVuOUPDqnmfn9gGdaGkzs2hyL5Z/vZ9Zw7rilxUOlDlj8pfXL0e00XTPs0fw7GUFOSy9oTeGQCuLw+VlWI+8iOV6aiAUayFeIg93GsceI2RZzJDEXMhYGGl7aa2Filc8O3RXodhahXagBeyu0JPK/Innk2rSB1sBs9NMQd+uk4TEr4KCrEBJlbtJvv2FsT146+a+1PpkiipqeeLaczDqdThcXhZ+sZfHrjkneL36ljMernAdc3Ch2aWGWEhKgKIoymfJuO4JgRwoywb2mLB9teg3DSMVhj4P6x4Wx189V7yvln1VTFwjCEp9b/JaOGOQWPLU745QT+rENeLBcexbsOFpMvZ+zo4r3+HwkFWcnmlkZ6mHB986Qkn1Rl4Ycx5AcHDutgFnMHH+10Gnd9uAM/jTwo3B358fc15Ef7S61RiEU3zs6rMxG/UsmtwLv6zw0ud7mDj/G167sQ/FVbURPa4qOYUvbQrvPy2r8SRMONqG2uOIJu5AURSFXeUyFyU2RgKIOZS0kk3ovDXIxsQqN+1bWvnhV62CcsrC7xNBc3UR+D3RHKtu3R70ROjBMJxPb9sU2o0Szru3fB2Sfw3nWFsOeF2YPn+C3l0f5IYPanjg0pWc2yYVDwbufK+QtdvXB5S3xMb4kipPVK//3LE9eHdrIRd3ac3sD3cwe/i53LXyuwiOtZj0uDx+cjMtZNvMPHldVzx+mfkTz6fa7aO4ys3f1mznb9d0JdNipNzpDQ4tqw+SbdLNMfejQGIPdxrHHgNUGzWkCJ+vqnCF22dh2AD+xDVCTQ7g9q3CpjNyRXuX3gg6A9KO96ke/S7lNW4y021UG+w4yt0Rm+L/OeJc7FYTE+d/TbbNzD2DOjP2ldCG+WdHd49pd+G+/b4rugCh3T5HKmp5Zt3PPHTlWeTaLdS4fdy5aGvE+dW1PrKsCjqdFNfP63RSswQXml1qiIWkBCiSJPUBngG6ACZAD9QoipKejO9LGmINa171LJjTYPxqqC4WW2JXTQ0RU0Vh7OyK2ouq/l5TIshq0JPgKhOtNjWlQp5Q/a4Ri6DrSDJNsL0qnRtejlTUunnpZhZM6sWf/ud0WtrMweAERFn3va2FzJ94PilGHQoSTo+PZTf2ocLpocLlJdWkD+5SOVxeQ1u7JXgNVZ4QwOOTWbX51+CQndPjp1W6OegoYy1tagrh1EeCGpoRiiJs7PSLEj61yKlQ46VJFZTatA5IKFjLf6Iqp2dC57bPsvKfX0opqXJrC8JONag8u3xMiPsm/VtwrCyLyon68Fd1JDa/+n2x3zeni6SRs0wEPhW/RnHsOZZ0oIQ/v3eI16f04foXv4poV7l56SbmTzwfg15i3CtfR3z27tZCxvXtgE6COaO645dlFkzqhcvj41BFLU9+sJOSajdPXHsOz43ujtPj56m1O5jQt0NENnvWsK7Isky5y8ucdT9HDDQ/s+5n/nZN17h2nwjXahzbRITbaIc/CkW4cW+LfSepWaLlMDw4Cff5nQcL27MGFs2q6nP2fBi+EA8GRq3Yw9IbemNUdNyxfGuEjd254jtmDj2bwnIXM4YUMH/DXp66riut01PQ6yV8fgXJAvMnnk+t10+Nxx/csQOweV8Zw8/Pp6TKzbhXQ/599vBzkSTRItgmw8zMoWcHqzaq3YYLMsRbztgcwYVmlxpiIVktXs8C1wMrEYpe44EzkvRdyUOsYc13boVxq+C//wzIVhpFoKLi08eisysjFsFnT4nP1SBn4zxR5q04KPqjBz4u+lPDv2vFeLj6BXIs0NLi4l9D2vLY+pKg9GVhuYuyajcjX/yK5VP6RAQvp2el0iYjJa4zXPTlPm67uBPLp/TB61cw6iVG1nHM0wIB0IGjToZ2bxex9XjD9IugniR4UwlH21B7HFDrEBVBW3bCpzZlQF6FK70DANayHxMPUFqqg/IV9O/ctNY0DScpnCWh4ATE6/wrYPhCSG0RyYsbno6dvd6yVPCsKvUaePijpjhUiRmzEtbcEsWxpqtf4M1Rucjo8CgOsm2RqkaF5S6OBvrpw98f0SOXwee249F3t8Xk2Lnrd7PloIPueXbaZ1lRFLhl2VfMGFIQPFa95vQ3v2fF1Avw+fx8uL04aov4Q1fGb5dJlGs1jm0Cwm10yxLx3h/vEcGuLUfMNxX9EN1VYc8PVP32g88Fb02JtL+VE8icuIb1N3ZEZ6iiSE6rd56kbUYK0y76Ay6Pn/97/6cou1Mrdu9/f5jXbuyNXwaDXmJXUXXEygB1kH7+xPOZtOAr5o3twZx1uyJmrYBGtWk1V3Ch2aWGukjaokZFUX4B9Iqi+BVFmQ8MaugcSZLyJEn6VJKk7ZIkbZMk6c+B91tIkvSRJEm7Aq+ZybrvCNQ3rLn3c3i+txhyDx/WrC4WUpfXzYfbt8DEf4OtNQx8TJR5x78jKjCXPQoGc8jRWjJjf5etFdKiqzDM6UqPj4bz8iAr3fNEIUrtM+2eZ6eF1cQbN13AvHE96J5nx2w0MG3pZob1yIvpDO8eeCbvffcrR2s8jH1lI26fHJMYjXqJOet2RSjJNFS+VRc6Ha4Q12uTYSE7zaxlQ04WOA6KV1viW+SbIjGswmfOxGuyYz26LeFzT2spomFtDuUURDye9XvgzT9F82taa5jwLtz4KYxeKbizz02Q2hLGvCl4d/y74v3qolArozE1LsfqAhybumgg868Icay6bbulzUQLq4nLCkLB8Y1/PL1ejr2pf0e659m5Z1Bnrn/xKw45XBHKX+EoLHfhl5V6h5E1nEDUtdEtS4TvH7FE2OQnj4qB99sCPr9FB/EMMP5dsUnjqxfi2p9UeQjDnK7oXr2UHNceBhaIxFH3PDvzxvXgjZsuoKXNTPc8OylGPeU1Xu5+4/uYdnf3G99TXevjh0MVOFw+xr6ykV8DAU4sm6t2+ygsdzF1ySZuH9Ap4vP67K7u0kaA7DQz7TJTNV+vodmQrAqKU5IkE7BVkqQngcM0LhjyAXcpirJZkqQ0YJMkSR8BE4F1iqI8IUnSvcC9wPR6rtM8ULfExthFwojF8NmT0G2UcIQT/w2Vv4rPPrhHlHsvuB26Xhd7XqW6OFI73VUeZ/h+b0TGpeW7E3jg0pX8+T0v/xxxLiu/LeQvAzszacE3ddoFlHqdocmgY2K/0xk+78ugc4zVR+qTlWBWRVWuCS/f1tU/z7QY2VVSremZn8xQ7akJQ/K7ymXSTGBvSmuwJFGb3h5b2Q8Jn2o1G2iVbmabpuR16iEez9pagbUVSJJop5F0YLSIFsVXLxPc2e8O0SpbXQQrJ8TmWXU+oJEca189gUcvf4sHP9ZFzZw8Hzb3p9dJ9XKs3WLk9gGdguerg+7qa12u1UnR7TKXFeTwwOACPD4/JVXumJlpbYfEcUAsG60uFjY4eLYIPoypUFsBK8OqeGq3RO+p4PfGf54AcBxAv3w0/xi/ljuRYlZH/LIcDDbi2V1ei1TuH1zAmMAiRofLi0mvi2lzxYHgorDcRYcsa/CYWG1a4b7eLys8tmZ7sy4Q1aChLpJVQRkXuPatQA2QBwxr6CRFUQ4rirI58HMV8BPQDhgKLAwcthC4Ogn3HI3UbLj+tUjpwJFLxbKwnLPEwrC198GaO4XGuewLnZvbE3qMDwUnIF5XTxNOta52+oanY8hmLhKLG8PhOMA5rcUCxoxUI9f2yI2ZvTPopQhnGI7cTAt7SmqoDVN/eenzPTw/5rwIqcHnx5zHvPW7g7/nZlp4a1pfWqWbOVzh4miNO1r/vLiKf36085gkBzUkGRVqBSXxAOWXcpk8m3hmbApq09pjqfwFnc/V8MF10L6llR9/1Soopxzi8Wx6O8Gxq28REq2Vv4KnRlRWOvxRtNWsvU/M/anBCUTzrCrdrraHNYJjz2hp4h8jzo3aLzFt6WamX96F5VP6YDbo6uXYVukpnJ5tDZ4/d/3uCAnhuvLtFpM+ol1m418v5s+XnMHolzcG+fWnI5X4fHLEd2k7JI4DYtnoiMWg+IRCp8EsKnifPxXdEt5tlHjV6aN9vLrbTIXjAF63ixlDzopZHbGaxQxoQ75dDlPRnLt+N5lWY5SU8Ozh5zI3zL+nmvSsmHoBn9/dnxVTL6BTti0YcNTddTL65Y1M6NuB7nl2zd40JA3JUvHaL0mSBWijKMojTbmGJEntge7ARqCVoiiHAx8dAaJ6UyRJmgJMAcjPz6/7cdOg00FOAdzwsSjxGkyCqHQ6kbFb/wQMexX0Jlg4JDJrIkliMDNWS0H4dllVO73wW5FpGfc2IMHRX8BdFTnfAmDPx+NXmLTgGy4ryOH+wQVk24z8a0hbclIlip0Kj60vocbt44WxPXhm3c/MGtY1qj/672t3cvuATsGMyYpNhQAsmNQLo15Cr5NY/MVeVmwqDGZIWqWlRFRH5k88P6qvderiTcwYUhDRQ63pmcdGUmy2MXAElGjMiWlWCAUvP70T7wwLwpXeAUmRSS3fQXV294TObd/Sysa9R6ms9ZKeYmz4BA3NjuPKs84SwbFDnxNtsXXnSz7/e/3tseE8a8kMiZqMWAQmm6icxOFYHX6cbrFtu3teOg/0zw7yq0kPr/x3D49dc05cjn3quq7odKCXpCDHbjnoCPJuh6xUFkzqRVWtF4fTS6v0FOwWU+CfQ/Til1S5mbp4UxS/LruhN7mZqcGHR22HRP1oFputa6OSBPs3itewFQBc9ayQulYH5sPtU28Sy0IHzxZ+H+Cjh6KG6w16PUdrPDFtz+n20iIQbMzfsLdRvn3LQQePvLOd+67owutT+uCXFXSSxMz3tgVlrF8a35Nqt4/xYUP04VWRWEHw9De/Z8aQgqCNavamobmRlAqKJElXAluBDwK/d0tkUaMkSTbgTeAORVEiUqaKoihA1IpSRVFeVBSlp6IoPbOzEx/+jQudTrQa2PPEqy7wTybL0O92qC2HFWOjsybp7UTpNta24/DtstUB3f6Ja6DXjWKoU5GF3r+ttXDE4RmXq+dS41XonmdnQt8OlFbWMv8KKz0+Gk7ewl70+Gg486+wkm7Ws/6nIu4Z1IUOWaksn9KHd2/7H2YMKQgOu89Zt4sXwqomX+wpo6iyljte38qj725jfN8ObJh+Eaum9aNzqzTKXd4IkorX11pXvUProY6NpNlsQ6gISAwnWAYpdio43ND+GLT4atMCg/JHf0z43A5ZYg7lh0KtzetE4bjyrCzDBbeArzYUnEBwuJhuo8TvautWOOrybFobuHJOQDK+RvBuztliI31djh25BONHD9DWVMPAgmxeHhTJr61ce5g+qDPlNR5kWebBIWeR38LCshtDHPvkBzsprnRz4KgzInMt9kfpePqjXRw86qSlzczZ7TJo39Ia1R4TL/AornJHZKu1uZX60Ww2G26jehPknhfdIaFW61SodmjPF7Y26AkRnOiMIki+8O5I27v2JTyyRFqKIabtdVQOkGHRYzUbmBGwu9en9GHVtL5Rvj18AWhJtRuPX+bRd7exp6QGq1nP367pGvTvrdLNweAEoqtw8WyxvgWiGjQcK5K5qLEXsB5AUZStkiR1aMyJkiQZEcHJUkVR3gq8XSRJUhtFUQ5LktQGKI5/hSRDXSYm++CtG+HqF2Jn7yQJtr4mMirqYjE18/f+3cFgA0MKrA0ogQx4OKTyoWZjflgRWkZmaQGealyy2Ew8/c3veXlYPvb3Itsb7KsnYBy/lk92lvDJzpJAxs5KaZU7Qlnm9gGdyEg18vqNffjVIXpVw5W6HhhcQH7LkFRXXZKK10udkxZaCqnpmZ+EcBwQ9pQgdhwVrSXtmzAgr8Kb0hKfMQ1b2Y8UJXjuH3KEdNjm/eX0+0NW029Cw28Digxv3xSfY1UbVttjw3k2XEXp6rlifkVniFT/uuZF+OgBcY1wjlUUqCnC43bx1BXtSF8yKIJfTSvH4Lz8LQ750nhz00FmDDmLokoRNKj8mptpoazGQ06amUff3c7iyb0ornJHcOyKTYVsmH5RXOWiePslhCxsSvA9bYfEcYYsiypKdVH9dhle6Ru+SCjNdboE3pgcssHrFojAWW8MLHdsixkzT7z7E3+PYXuGFaNpOfYD7ly9j+w0E/cM6oJBDw6nN2h7IAKSDIshQq5atbs//c/puDx+2mWmBm/713JnvVW4eLZY3wJRDRqOFckKULyKolRIkRnaqKpHXUjihFeAnxRF+UfYR+8AE4AnAq+rm/FeG4/wvShj3qh/uB0J+t4GXzwTcn7WLDBa4dqXoWwXfPygOH7ocyKr4nUJCePKQ7DukcjNxyCqLG/fTPa4D6jRC7Kw6v0xSbKiqpp7BnVGVohqPVi1+VfG9MnnaI2Xw45a2totvPLfPRFtWbmZFgz6yAJbXZKau343T13XNWqBY9sMi6ZnfjLDcQDyeid82s6jwlkdSwVFDMp3aFIFxWo2kJdpYdOB8oYP1vDbhiyLWZP6ONaWE9keO/Yt0bJlyRSJn8v+JiomHz8Iw+aLpNGYN8QsQOUhwc397hD8WodjufoFCiv9nCa5YvKr3STjs5iZ0LcDo176KqLFZuEXe5nQtwOb9x2loE0u9w/ugk9WYnJsQ4tr543rEWyhCb/+efldg8dpOySOI9RnAL0p1CFR1y5T7GIRs62VGIzvNgp++Qh6T4n28W9MjPTxk9eis+VTUuXB5XKSHsP2qmpqgr49fG+ZupwxO83EfVcUICtg0ut44v0dwcAlN9OC0+OPsruGli3GCoLnjetBltUUd4GoBg3HimQFKNskSRoN6CVJ6gTcDnzRiPP6IQbsf5AkaWvgvfsQgckKSZL+BOwHRiThnuuHLEPVodBeFJ1ekFGs7N2IxcJBtugIlzwsHKPsF7tT9n4ueqBTs0MqIDojLLgiMvs36EmhBqb2UaulYscBXC4XaamZQoXDqZAXgyQPV8u0zkkJLmaC0KDdGzddwN7SmohttS+MFduSVVWOuWN7kGOrf/liSbWbVukpvDWtL16fHOEYm0vPvK5KmEaExwh3lbAjW+uET91xVKZFCqQfY6LMldaelvv/jeR3o+gTs5MzWqXxzb6jyLKi2cGpBrU6rfb4K3L9HGuywcT3wecUFRL1oa+6GCasEQ+Ha+8THOsqj1zQOPR5+H93hVp2IYJjlbQ25GW2Ru+OHRw5PDpsdgO3vbYlqi9/2Y19WPrlXq7o2i4ieAlXAGv04tqcNJbd0DvY1rXwi73ceWnnqPOOlXM1nm0kagK70catit0hMXKJUJoD+GEl5F8gjrvglsb5+JoSanRZ3D6gE4WVR2mVoG9fedMFlFS5IzbNq4FLSbWbp67rSqv0lCj7aagKd7IFwZq9/j6QrADlNuB+wA28BqwFZjZ0kqIo/0WohsfCgGa7u0ShZk081SGy8LpDi8FUDfQWHUFvBsUPjn1gsMCKcWFOdQnYTxNyhJuXiIyfwQwLBkcr0AyeLRS+1OyhKpVpzyfFZOBgtZtZw7ry8c4jdB27GqOzWJDn1tco6/UXXtxQyd2DlJhlW5+sRKnT3LxkE8un9OGBwQUY9DpybGYMhsgKyvEmKU0+MwlQ7Swt8Un3nWX+Y2rvUlGb3gGd4iO1fAc1WecmdG6nVmms21HMLyXVnNGqGW5Gw8mB8Oq044DIQJvS6ufYo7tjc6zeALIXvn8drn8dUtLic6y9vXivDsdKR3eTnpXKv75xcNe41RhqQvzq6PMXUiw51Hpj9+VXuLyM7HVaVE//tKWbWXaj4FiLydAo7jQYdORmpmIxGWiTkcJ5+V2bnXM1nm0kZDn0DOB1i/mRz54KdUjYWsE3r8KXc0K2WFkIlz/ReB+/cR4pAx4nv2Uqb3xdXI9vJ6bteX0y05ZujgpcXruxDz6/kCjOTkuJ+n9tjG8/WRYpavb6+0FShuQVRXEqinK/oijnB4bT7lcUpTYZ33VcoG6UV0u6uT3BZAmR0yWPCKnBD2dAyXb4V1eRqVYdJwS2Fo+Fc68XJeCu18GSa8X7sfpYjanCGbftJnqwP3k0qOnv8kNxlZv/7CziL90VjEuGwqsDYe19yP3v5eUdJm4f0BmLMfbwpE+OHbj4FYX8llba2i1RwYkKlaSOx0ImTT4zCSjfL14T3IHikxV2OWROa4aYILhRvgkLGzsHgpJv92ltXqcUVI5VudDvaTrHSnrB1RfPEDKwlYficqyiN6LcvlU8LIZxLJ/NwqSDu7orGBaH+FXpfy8lKafjkeFQRW1Mfk0x6KjxxBlwr6wlxaRPiDuTzbkazzYSzhIRFHceHLJNdQ9aih3W3i+CExD29dkssaOnHvujRUdofY6YQwnsS3H5weX2Rvl27x+n8/IOE7defAapRl1M21OIHbgccrgY9+rX6HS6uPZzPH37W1t/EQAAIABJREFUsUCz198PkqXi1VOSpLckSdosSdL36p9kfNdxgbpFVm01uHA6uBywc43oHV0wWLzuXBMq18bbWlxTCu7KkPpHPAUar1N8Pv8Kca0r/i4c9cZ5WM0mzmmXzs29MtCvGB3hoHXLx3BTLztt7ClYTPoIJQ+1hxmFmOSmlyR8PjliQ6wsNzg6lDRo8plJQLCCkliL174KGY//GOdPAvCmZOMz2rCVJT6H0irdTIbFyKb9WoBySqHupm7Z33SOrXUIOVd3tTgnnpqi14nkLBOBjjlNBEEDHw8GKoriw1CHX6XlY6itLMFs0HNOu3SeG31eFL96/DIphtgPkNlpZipdXo7WnFhuDYfGs42EzyOCjksfibZNZ6n4PRzdRjVofzgOwMIrwWKHPjfBxnmkmk2cluKM8u3GlWOZcr6dZz7ZBRJRu3SeG30eh+MEzU6Pn7lje2BPMZw0/r2p0Oz194NktXgtBe4GfgDkBo49+aFukS38Vjivoc8LpZdYA3KmNFFhSc0SbQo1JfDzB3DGIFEGTm0BtZWh8zY8LVRm3r4psj/VnAb/vkv0TztLRWuYwYz/kkc45LXy5NptzLkiO6aDtuj8GAKa+h6fzD9HdCPLZsKvKBypqMUny8wefi53rfwuWCKdPfxcHC4vfllh9MsbT4rSaUODexqaAMf+Ju1A2dkMCl5BSBK1ae2bNCgvSRKdcmxs2n+0GW5Ew0mDupu69UYRgMTjWBAzKmNWiuP8XjEXqDeJbPaAhwRvqomlWBybkgFfz0MqGIrSshNS2S5xbHUx/pHLKK92kxWDXzu1NHJIAYNex3Of7mLGkALaZqSQYtTj9PhJSzFi1BO1o+K50efh8fmZtOBbZg49m9YZKSdFW4rGs42EwSQqbLUOYYPhthnLFtPaioD35w9i2581W6wVuPoF8ZxgTsPf/68c8lixuYtIi2F7LpeTOy/tjEGnY+EXe3nquq60Tk/BrygY9TqWfrI/yu7mju1BVa2XOet+5oHBBSeNf28qNHv9/SBZAUqJoiiN3nty0kPdIvv6KBGkOPaLIeO6A3JXPSuOv/hBWDY8TFp4kdgwu3NNqDe182Dxe+G3QmXm6hcgva0gupoSkUHMPhPOGVFnCG8ZaalGSqo87HP4ODOGAzeaUpAChNPCYqTE5AkO1KmE9clPRyIkCF/57x6G9cgjN9MS8f4/P9rJ367pekKG3jX5zCTAcUD0Sie4A2XnUT86IK+Zxj5q09rT4uBaJL8HRZ/Y/+cZrdL4dn85pdVusmwnvidaQzMgnGMdB0R22e+JzbE+l0gCSXpYc1fkQ9+H94uHyJFLRKVFTSx9/KBoo8loJ2ZYqg4Jru0xGd6YiBS4hn/EErzWNhR5LaTrK2IGSDpDCu3Thfz6nZd25p8f7WRC3w7cHOj9Vzn2851FEVz63Ke7mNSvAzOGFHBay1T2lzlplW6mhTV5NtwYvtV4tpFQbbTqkLAh1TZVoZs1t0Ta4ttTQy2D3y2Ltj9Jgq/mBp8LlJFLqLB15KUPdvHXC1vGtL1sezqt0wQJ33t5F4oqayN8u6rSqdqd0+OnqtbLqJc2AvDnS85Iqn+PheYeaNfs9fcDSew9bOaLStIAYBSwDjEoD0DYXpOkomfPnsq3337b8IGJQA4EDp5qkfWw5oh+aLUH1VUu1DoueRiWDovO+oVLCdrzxYC8OoMSvh9F3Sprz4cJ74ryb51r/XD5W1QbMlm9uZC7z5Np+e6E4HWU61/DkfYHnB6hqqWgcO3zX0RlG5be0JsxYZmUWcO68vnOIq7o2o5blm2OeL9zKxsK0jETTFOG246TWscJTx8lxWZj4YV+QmVmwEMJnXbjWic/lfqYd1Hz3Eb6kS/J++EZvhv8Ls4WZyV07i/FVcxYvY1nR3dnSNe2zXNDvz2cejZbV8VLnTmpy7E9J4rP1A3eKsJ51p4vHgjr7j4J51l7vpg9WTo84hq7h76Ny5TFM+t28ng/QwS/eocvpdz2B7yyyORmWowUV7sZMe/LBjn22dHdcXvliMr1vHE9aJORgsvT/PyWCN9qPNtI+H1QfSSgEHe/sM2cLrD4mvptceDjwl5HLIJ//yVkf3WeC+rz7fLIZZSmdkTSif+f0hp3TN8+c+jZTFrwTcRm+S0HHVxWkMMtF3WK8u9/yLbSKiPUFtactpCsgfbjqOJ1wm3294xkVVAmAWcCRkItXgpwXAKUZkVwMaMsljPWlIg5kr3/gf7TQ7MkgS2wSLrYfdFq37T6e60jpP6R3lb0thZ+G3kMOnGM6pw3PA2F32I3ydy84ntmDj2bGz7YyaOXv0WnlkYMxhQOeqyMe1aQ1mUFOdw/uCBmv6aiwNIbelMSJl95/+CCoENVj5v+5vdRjrapBBNvuG3VtH5xMzgni3LIKQFFEdW/DhcmfOqOMn+zDMirqE1rD4Ct7MeEA5QOWTasJj0bfin9PQcopw7CAxOjRQQnFYWiQtL/Xlg+JpJj/R4xXFwfzzoOiDaxjx8SQUhme0AKciggMt8tThe7T8L4tV2ankte2RTY2p7OA5eupI1NR4v0NPY4U5gyNyQdvGhyLww6KSbHVtb6eOLac2iXmcq+0hqqa33c+9YPEfw3dfGmiAfK5my5SYRvNZ5tBGRZtA0Wb4fsLiH/f92rDdti9pnCDv2+SPvL6RJhfzmpcPOySN9+RpYJv2TkzvcKWbv9Uy4ryOGBwQVxB+JPz7by+T39QYHH1mwPLg6N599XTL0g7K/YvAFFU3x+Y6DZ6+8DyQpQzlcUpXOSrn38oEpffvo49J4a2Wpw7Uuij3/wbNF36nWKMq8kxe6bdpVH/u6uDnyHTzw4OusM/XYeDLXlIR3/C6eLPtbqIgzmVArLXeS3SOXeywso9fixKqnYdAbGvSqCk+55dib07cCekpqY/Zp7S2s4q206p7W0BuUr3XGGz0qq3M1CMNpw2wlGrUMMBNsSkxiudCscrFK4qF3z3YontRV+Q2pgDmVkQufqdRJd2qTzn12lKIqClGC7moaTCOHywrYcGPBwZMVj9ErR/gohjv3wfrhufv08a88XWe5+dwh+NqTAxpdgyxLxeW5P8V1q5rvzYMGv7irMBh3/c7qd1ze52HKwkmGLKwH45K4LmbIkJB2cbTNTVFlLrVeOybEpBh03v/UD79zaj7PapeOKo+yVatIHf26OhzcVGt82I2QZju4BT1WorbDz4NCqgIZsUa0I6gOPXKr9ffigqMJYs+HqebQ0pZFtM0b4drtkC+7TUf366Jc3MmNIQUy721NSQ+fWabROT+Fv13TloStFlSGefw/vomnugEKzQQ3HgqSoeAFfSJJUkKRrHz+o0pfdRoWCExCvb90otsEvHS5UPD6bBa4yUUEZv1qQF4RmULa+Fvr96rkiuFl7nzh34RC48J7Icy59VGRnbDlipmXNXfBsT3j7ZlpTzsCCbFICUoNnt8ugfUsrXp8cJIOb+ndk+pvfM2fdrii1j1nDujJn3S68fjlCVtBsiC1LXFe+r6kEY4pzfW247ThBlRhOMEDZXib+rzs2g4JXEJKO2rTTsDZByQvgnNwMCstd7C9zNuNNaTjuCJcX7ndHKDgB8bpsuHjQC+fYq+cCEly3IKSOpPb9b3g6NscuGAznXBfi2Aunh74rt6dIQC25Fl66CGnBYB7vp+f6Hm2Ctyl4ShfxsHVT/47c/UZ8jvX4ZV4a3xO7xUROmlBVjMV/Dpc3+HtzPrxpfNuMcJZA+R6xk0e1z51r4KOHAEkEKmNWCluqa4tDn4dVU4UtKoo45sLp8NULwu7W3iekhJdcg6HmCE8Nzsdi1NHGbuGsdunoJKL8emG5i7nrd8f17YqiRMkGx/Pv4fbQ3AGFZoMajgXJqqD0AbZKkrQXMYMiAYqiKF2T9H3JgSp9acmMr2MOgnAueTRSpWPEIrHIqaIQvn1Z9E0P/Js4vtYRSXSOA/DZkzDo/4SEoewHv1u8P/DxqOBIWj6Gf01YiynDEiy7yrKCX1aCGRW7xUhhuYvCchd/X7szOBiXk2bmf1d8R0m1O4okYg2fzRvXg399/HPEcU0lGG247QRDtaEEA5RtpcI5nZ7RvLfjSutAi8KPkWQvis6Y0LnntBM3899fSmmfZW3eG9Nw/BAuLxyPZy2ZsTl22Msw9DmRFPJ7RXb62pfFeTE5dlaIY/UmkfxRA6M6HKtbMZbHxq5mZ3ENJdVeXhrfk1RTpHpQfRz7f+//xMNXnU3r9NBSvCyrOYr/1C3fKprz4U3j22aEzxMta53bM3pD/IjFwhYVn7DFsl2w7mHR1pXbU7SIXfuSaF/scxOsviXSRpeP5fQJa3CY9EHxhOKq2ii/DrDloCNod51ybOwqrubva3fG9O3QOHtoboUszQY1HAuSFaAMStJ1jy9U6Ut1V0ndEq43kL296IGQ44TAwrDxojVBHZLrej28dQPk9RXBSl2i6z1VVFLChzk7D47rtM14kJzFgjgNJiqkDB5bsz0oMehweYNEs+Wgg6mLNwUVurLTTPzr+m54fH5KqtzBAbNY22QzLUbuvLQz2w9XHTPBHO9N9BrqwBGooCS4RX5bqUzLFGiR0ry3U5veAZ3sweL4BWeLLgmd2zo9hSybif/uKmVsn9Oa98Y0HD+EywvH41lXeWyOffMGwbELBocy1Ylw7NDnxcNjHI41OItZPq4zkiJjUCpANrF48vmMe/UbCstdOD3+mBw7c+jZ3DPoTAw6icMVrgieC+c/o0FHda2PkmqhI9PcD28a3zYjDCbh78PtM6bfHxdoSZSESpwqwJDbU3RChLeJj1gUCpJVOA6A4ifdfQSUFEjNxqCTeOq6rtz9RqRfBxGkzHxvOzOHns3UxZu4rCAnpm+HxtlDcwcUmg1qOBYkJUBRFGV/Mq573KHKCn76eLSO+YgloPjFz+ltY2f+0tvBbVvg6C+hLEq/O8Q22nCii5HBY+UEGLtKlJVjOW1FgXfvDPav2m2taGHRBzMqXdqk8fyY85gWJn35/JjzaJ2RQm6mJa4Weqzhs+YkGG247QSifD+YrGCyJXTaj6V+Tm/O9q4AnBl/ACCtZFPCAYokSZzTLoMvdpfilxX0msP7bSJcXnjD03DNi7BqSuRD3GdPwSUPxefY27dAWRM4dvU0MUMYLzCS9Jh8NVBdJMRRtr5G+4vu491b+1JV6+dXhytqn9QLY85DVhTKazxMnP9NTI4N578sq5LUhzeNb5sJqdmQeXqkfaa3i22TaW2hsjDSBmPZ34rxMVXkdGW70H02S7SBteiITZ/K6s1FwX07c8f24KYlmyJ2mOkkiS//ejFHazz17jlpyB6SEVBoNqihqUhWBeXUgE4HOQVw1Rwx1D5+tRhqV3XM9/4Hxr8jljLFdHA68NdGEpAlUyjLhOv7W2MvXETSgb290PQPVwsbsQjpm1cjBvclez6PDV/C8Ldlpi7exPIpfXji/R0RmucPBaRZpy7epClp/R5xdLcIphNArU9ht0Nm+B+a/3a8lhy8JjtpJZsp6jw24fPPaZfBpztL2HrQQY/TMhs+QcPJB5Vjb/hYDCK7q8QAfGpLkYH21MAVT4qf43Gsz910jm3RUbR71eXY6xaIe1t0VcQOFunTx8m88p/4THbufuN7sm1mFk/uRXGVG4fLy4Ort3FT/47MfG+7pp51KkGnE0pwOoNQ3fK54w/HIwu7/GxWyAbjtS+26Bi6hlrV+/71iGqLyZ7PvUMXMunfu9hysJJ3b+0X4defeH8HWw462DD9ooR9e+y/qmaTGk4OaAFKQ9DpRKWk+rCQCKyrMFN1REzYDH0+8rOhz4uWGl9tJIm5ysXypk8eDUkIW1rEJrqqQ2JewGARQ3juKjDZUIwWpPxeURkZw8qxPHr5W1w5vxKnx09JtZupizcFL5mbacGvKJqqxu8VpbuErGoC2HFUxq80//wJAJKEy96JtJJNDR8bA+fk2tFJ8OmOYi1A+S1DpxM8V10EHz8Mf/xLKDDoPFjM8vlqk8OxOoOYC/jsKZGAcjnAnAa1FdEzLO/cKq7n89AyPdQK83NxdURAEj4noELj2FMArjLxHGBIEfYy6P9i22TlIWE/4TaY1jp+++KE90QAjiLEd2JUW+yrJwR9e2m1J8LeQPPtGk5NJEvFq0mQJOlVSZKKJUn6Mey9hyVJ+lWSpK2BP1cc9xvzeUQGrq7CTMUBUe798AExQDd4tsiuqNLD6x4RWZShz4fUZra+JgbpqotFxu7tm0XgcfXcSEWaq54VWUBnmeh/ffsmeOkiqClGKvkpbkawIMfMxr9ezFlt05k3tkeEwsdL43uSYmy8qoYsK5RUufm13ElJlRtZbv6lnhqOE7wuIdiQkZvQaeqAfMdkBCiAM+MMUqoPYnSVJHyuzWzgzNZprNtRlIQ703Dc4fOIltWVE0Lc1m2UCBSOlWMVvxhOrsuxtRUieOk2Svz80kWw5FqU1JaxM97WbMHHQEubiWU39qZbbgbzxoW4Vp1NCYfGsacA1OeAFeOFvXzw12ibtOWI7fAbnhY2qdrghw+KdsW69vfBdKg6LDoziraJ4+NUWzTfruH3hpOtgrIAeBZYVOf9fyqK8vfjfzsBGEziAa8uaaiqHo4D8ME9Ic39FqcLWUF1IdO6h2HcKrHg0dZKzAGoDtZVLs61toIxb4ot9a5ykXmpLhZLyRQ/XPKIGNJTZPhsFsqwV5FiZGR0BjNHq73cuOhbsm1mZg49mw5ZVlLNerICqiCNGYJL1gZYDScIR/cASsItXttK/diM0MrS8LFNgdN+BiDmUI7mJ66t0T0/k6UbD3DI4aKtPUk3qeH4wGCKTryoD2uN4ti3xcOe1yky1uEc+86tgmNHrxQVE1c5bJwnHjRbnC7adYypotVrw9Ni30oMflVsrVAsWVHcuGhyL96a1hevT8Zi0msceyoi/DnAkilkhmuKQjbpc4tAo9so8Xlmx9AOH1c5fPOysEn7aVD6s/Dx1laQYhd7UiyZMHqFSEpqvl2DhpMrQFEU5XNJktqf6PuIQmo2eGujSSNc1aPw25Bi1+DZIceZ2xMueywka6nTw4ZnoNMlIrOnloYvfUwEIuHvXbdASGeW7Q5JGpvTIa8vfm8thjrlZXnkMip0Gdy46Iug/KW6oXjVtH5B8mnMEFyyNsBqOEEo3SVe0xOroPxY6uf0DOE/k4Ha9PbIOiNpJZuPKUD5ZEexpub1W0dqNnic0e1ajeHYC6eHFMF8bijfB6lZoWqMPR+umA3rHhUPliq/ooiWHRBcWl0sBqFlX1T7jm/EEpwpbXA7fVHcOP7Vr1k1rR/tMgVP2y0mjWNPNYQ/B6h2qdokiN+vnAMpGZDWRsyfpreDtfeHbO78G+HjR8TvnQfDgAdFJ4a67FnJRE5vh07z7Ro0nFwBSj24VZKk8cC3wF2KopQ3dEKzQqcThDNyKSwfE3J49tOEFv+bN4Teu3qucHj2fBGQXPWsyMZVHgoqwXDBLfDdMtGbas0WWe03JkH2mWLWxFkmjtXpoLYytLk20OOqXDANw6uXieurPdZeJ1J6G5weucE+1FhDcLKsUFbjCRKbtgH2FEOZGqA0voLi9iv8VCZzVYck3ROg6Iy40k8nrfjbJp3fNiOFVulmLUA5VZCSETmwvvU10RqjzoPE4tjLnxIPd95aKN8rWr6qi8Vx174IttYiMfTd62IHSr/bRWXa74lUDLvqWZHVXjUFafBscZ0wfvVZ22BLMVNV4WoSx0Ikz2ozA78xhD8HrH8iUoTBni8CW1uOUNisKQn5+wvvFn9qSkSlTrVBWyvRdlvHv+v0laIi2Ey+HTS70/DbxG8hQHkBmAkogdfZwOS6B0mSNAWYApCfn9+8dyDLULIDvl8ZGUB8/AgMeCig6uGBkp/g4wfFOQMfhzbniraulSMineCXz4kysJp5uX2rcKiF34rv6XeHCFxSW0YPaq6eJr5PbXtYHlI/ku74EZMhLeFFS7FKvstu6N2sC5s0RCOpNlsXZbvBmgXGxrdBbS/145Whc5Lnz52ZZ5K17z30nkr8psT0jCVJonteJp/uLMbl8WMxafaZTCTNZmVZtCFWHRYbttXkjTUbTGkwYY0IKGJxbPn+yEFlNdB4+yZxjK21OH7LYti1VvBrThdYfE3sIfjlY0VGOzw7Dpjv+BFJJzV5mV1dnp0/8XyNY48Dms1mZVlU5mSf2LNjzxcD7hUHxXJGe57w4+FVu6ueFQP1qr+fuEbMlva7QwQ7dedaV08T7eB1bK+pvl3ctmZ3Gn6bOKmG5GNBUZQiRVH8iqLIwEtArzjHvagoSk9FUXpmZ2c37004S0TlJL8XLLkWPrxfvH/BNKjYL/pfSn6CtfcJYlHJxe+NHPhUnaDaowqCxGpKxc6TyQHnufU10Zcq+2IPasr+0LCdCns+GEzBRUt1B+jqW7QUq+T72JrtEYOf2gbY5kdSbbYuSneJdoMEsKVYZNTOTHKAUt2iK5LiJ+PIl006v3u+HbdP5ovdpc18ZxrqImk26ywRO59WTxPtL8vHCp49uhvcDkARqoaxOLbuQ947twoeVYfay3aJasjYVXDZ38RxtZWxudWSGc2tAPZ8JHU4vgkcC9E8O2fdLp66rqvGsUlGs9msswQc++Dzv4fmTbwu+PJ5WHy1UJOL5+/TWsOYlWDNEbZ54GsxTxrLBhU58r1j8O2g2Z2G3y5O+gqKJEltFEU5HPj1GuDH+o5PCnyekPOy5URvhB25RBBO3ZKvJMVXgqkpCZy7VMyeLLkm7HpLhY56bYUgNXXQc8PTUF2M5CyL/q7rX4PU7CYtWorVzvXh9mJmDj1b2wB7KkBRxENaft+ETtta7CfbAi2beYN8XbjsnfDrU7Af+pyj+QMTPr9Lm3QsRj2f7ChmQJdWSbhDDUmHzxMSHYHYm7eveVHMjbwxsWGOVQMNW44Iznv+KZJjx78jZgDUZJGrXCSGvE4xe5KaJT4PzA7II5ehSxUPt01dZleXZ7ccdPDkBztZPqUPgMaxJztUFa+w/WPBZaIgEofx/H1KJrwxOeycxcLWYvh3dIbI3SjH4NtBszsNv12cVAGKJEmvAf2BLEmSCoGHgP6SJHVDtHjtA6Ym5ctlWWRIfB4xbJmaLXpOITR86SoXw5h1N8IuHysc3tr7RYtAdhexG0Vniq19nt5WVEgGzwaLHRYMrnO9MaJ3Wpaj+lOxZsM7t4hj1TaIjFyxvTZwv4kuWorXsqDT6bShuVMBzjIR7CYoMbylyM8Z9iTdUxgUnYGalueQefAj6PWomBdIAEa9jnPaZbDup2Ieu1pBStZEv4ZjRzyeNZgiRUdibd5eNUWoIkVwbGy1LdLaiL0mOz+EMy+HhUMir/XNq3DhPbBiXORDozUwWF9dLBS/LpgWmD9phUn1BzRtmV0sni2pdmMy6DWePZmh2iyA0Rrddr1ivGj1kqT4/n7jS5HnfPakmEuJ5d//M7vZfDtodqfht4uTqsVLUZRRiqK0URTFqChKrqIoryiKMk5RlHMURemqKMpVYdWU5oMsQ/F2ePkSePps8Vq8XbwPwole/5rIsGV2iJ0l0Rngyn9Cm25CeeuZHvDGhNj7Td6/Vywl+2xWqDpT93rWbNFDXbc/1ZQqnDeIwOjVgSJDrqvzXynL4jscB8Wr+neJgaaWjjX8RhBU8Gp8i1epS+ZglZL0+RMVFa36YKotJb2kacPy3fPtHKmsZduhyma+Mw3Nhvp4NjUbMk8P7TOJt3k7I1dwrE4f4NiJ0Rw79HkRzCwaCq26ADGqLPm9QsGJeu0V40SbTuG34ndnqUgeLR2O3l8b/XdpJL+q0Hj2N4hwm31jovCz8dqy3pgU3993ukRUBVWo+31i+feSHc3m20GzOw2/XZxUFZQTBmcJvD4qkixeHwU3fCyUNnQ6obB1+SyQvaIs+9mskMyl2mqgZgOri0IShB8/KDJxtQ7R1vXJo+L9oh9EBUX2hbIuuT1DA/I6o2hPCCdDxwHR87r2vtAgaHVxcHFYECqpqn8ntUycUxBNdjS9ZUHDbwTF28RrrN76ONhaFJg/OQ4VFIDqrO7IOhNZe1ZR2ap3wuefd1omOgnWbjvC2e2StFVSw7GhIZ7NbC9EHMa/IwIQtcVKhT0fJL3gO8ny/9k78/Amq7Rx3ydJ0yZtaUsXtrKJiAKDQHFBZsFxFGcGh88B3AB3BXU+Z1E//ZxhNhy/QeTnjKMIOO4ibujouC8jOoMggiAqiogiFIEudE3Tpsl7fn+8edukTdu0Sdu0fe7rypXk3fKkOX3O+5xnC9exM26FAeOg9AuzApKlm5//GVz0z+Y6Nvc485wNf2k81srvsz4rUN/w2uYIWWlup361ED3bAwkds+X7oKYsclgWmM9v/NYcb1WHms/31nhrbfz5a83QxjjN7SDjTui5iIECLXsx/D7ztVXFK1QpzFphToTVRaax8NL/wKk3m4oipX9jKcKJ55velftnNL9+9mjY9ap5/nurmse2Wp8RaghVftOYfPfj5ab7190k8a+tG4EIdMR1LPQQDn0MzjTT8I2SbUUB7KrzOsg3xXCkUDHwFHK+ep59k2/Cn9w+y6hfShLHDerHyx8d4rozxnSSlEJMtKZnI+nYucHYfquHxKwV5ip2dZGpX+etgzd+b+rY1FxAwZq5za9fW9myjrUWegq3NOpX67PQZnPG89aiQv93OqBfLUTP9jBCx2z+FLOSXNOwLGequf3C581cP8Mfeb7PHN48r6rp+Ks4AP+81gxlTMmMy9wOMu6EnklChXh1G1aOSSjByhlAZKXw3NVm0uaMW00Fs+tF8xhPsVnRCw3fvd70dhR/Gvn6pbth2Emw+w2zYVPTmOvnrjZzXqzj/2slvHVL4/6cYyKvnLRlcAl9i0Mfm6vT7cjN2HY4wMh+kNKFSxilw2ZgD9Qy8LMHOnT+iSP680VxNV8UVcU8fMFhAAAgAElEQVRZMiEutKZnI+nYpy6EM/8Pfva+uRhjLdZYeXp2R6OOvX8GVB6IfP3kNNM4iaRjrYpfVrKzK9PU6W/+3tx2+RuopjpW9GvfIXTMTvtF5LDr2gpYM8csdvPidVDUwnzvymx9/Fnze/k+M4cq91iZ24U+jRgo0JhjEho7GqycAbSsFKoOmrGiYS5ar+k5SclsLDm44S/mSknT2NS3l5qT8KR5ppcl0mdkjTD7pPx4uek+DvWm2ByR3bptGVxC38EwzBCvrOi7LfoCmg8OBziufyfKFYG69OFUDDiJwZ/ci9NzoN3nTxlhCvzqJ4fjLZoQD1rTs63p2Ooi0zNSuCV8Hyq8rKs2GnNYrOvPWgGeUtPL4nBG/owB4039+tL1sHq6qdOtakpWiG8ool/7DqFjtqW8KHuS+ewpaZzv5z7UfL73lkc+3wr3sub3zGFmeW1vaXN5ZOwJfQgxUMCcgPLGmm7SX3xsPoeumrWkFOprmm9TNrNTbG1FozIq3GJ6WRY8a177gqcgpZ+5cpKWZ17HZov8GcW7wO4Ee3JjrKul8FQL1Y7aMriEvsORL83x1T96A2V7UYDaAByf3YlytcDh0ecDMPo/v0IZ9e06t3+qk9F5abz8cfzraAhxoDU925KO9RSbsf6R9qHDb/iUDXY8burXn20xn3c8bura7WtNfRnpOjaHWZ1J9KvQlNAxmzk88vixxmdKphkGVrjF9FafdSdcs9mc95Uyqym2NI6t/j6hi5eRvCIy9oQ+hOSgWNhsLcdwWkohND76v1aCIyW8XvmsFWYlj+oicwUlNMmzcIs52Qb84fXQZ60wjZQ9b5nnNO1C+94qGDzJfJ5xa2PN/vdWoWfeQcSgnVClGqlsstB3OBBcdc45JupTNn4TQAHju8FAqXflcfC4y8j/+G6O2ngze065rV2haSeM6M9jm/fxdamH4dmpnSip0CFa0rORdOzch8ymeJ7Dzfs+zVphNssL1bHagAnnwWNzw4/zHjFDZf115vvQrvOzVpgr4BvvbqZf6364nMqquuYJxaJf+xbWmDUMM/fpiXnN5+i5D8Gbf2xMcK+rMg3fNXPCx3Ok87c+bOZTeYrMsddSgrwli4w9oY8gBko0WFW8LnnZTIDT2ux54jlshgb0H2WGIrzxu8YwhKcugvnPmNU7LGWUPggeOqt5DOt/3WPGp265zzynptQ0Zt5bZSbepw1En3ozKmTyLp/1EGW1boan6sjVOFozuIS+w4GtZmWkjKFRn/LuAT+jMiC9m6IGKgZNw+k9TN6ep/G5B7B/0vVRn3vKqGwef38f67YW8itJlu85hOrYQL15c/flevjuDWYY7L/+aOrarJFQUdhYoCRUxyob/OOq5vr1rDvhmSvN5+R+5nWS3KZnMX2Q2ZTx1JvDjKPyWQ9xyZovKK6u594LpzBmQHpzI0X0a9/CZjPDsS583jQkktxmcvzpfzDDuna92FitK9JYfOoiM8/pohfM+4XgHK9PvZmAVjis4zOH4T/nMWyunMghLjL2hD6CGCjR0LTCzJgfm8mblpKpqzTr5YdSvs8sLWw1XOo3pOVOs2Aes+1Rs4zhjD+Zxw89sWF1pDz9aAp/+AyZToOiGs0tLxVTXP0+z149TapzCC1TuAWyj466+WGt38w/OSv6iLBOoXjk2Thqj5D/8Qp87gEcHrMgqvOy05L51pAMnv6gkF/84BgppdlTiFTFa/6z8HqwhHDuseb+fywKz0WpLTcNjuzR5vvWcgTsScEmeH8yDaDQ1efgqnSgvo6dRXX89qVDbNtv9tS54uEtomcFc4yW7IK3bjULLtgcZqGbpqWCc481FzEjjUWfx1zkrKs0x9zMOyi3ZXDTuh1cefpT5LkVRTWa1W9UcsvZfnLT29e0VhB6E+IXjIamFWZ2vWgaJ/fPMBMqKwojx5ZWHTJjS51ppsGR5Go5lyUj34zLPuuOYKzr0LAEzRqfwVkPfM61L5lx0nf+KI+/zhyMtur2C0JTfDVw6CPIid6TsPVQgHqje/JPwlCKg8deQmXOZEZu/j39970a9anfOyaXb8pr2fhlhCRTITGJVMWrpsTUtU/MN40TK07fwor/Tx9s6swkd+s5ApnDW9Sv1qr0IXL47RuH+M30XP698GjWLRhFbloSPr/o2T6PNUZ3vWiOzeJPI4/Jsq9MY7iluT5tIAw63vQGpg+gxmfw6s5ibllfTFGNJs+tuLKgn8ztQp9HDJRoiFRhxlPcqIAiVek6dw0MmRKeCJqaC+c91rzKTNZRZj5LKzgddmaMzeXvZ6ZS8Ppchj50IgWvzyWnZk9UXYyFPkjhZrOx6IDxUZ/y7jd+7ArGdreBAmCzUzjhv/FmjGL0f35BWsmHUZ1WMLw/qcl2ntqyv5MFFOJGR3Xs4MlN9GuTBOKf3GUmyJ+31gznArMccQsduN1OGw/8KFzHPvCjVNxOmSr7PKFj1FtmjqtmY/JRc0z2yzfHZ9O5PnOEuVAZkjcic7sgREZCvKLBqjATOoFuX9uY8Fa4xcwXufD58NABMBM0Kw+YMaloM975klfMG0dlN5WVtxzuPTW8M2zusWaZwWAiXLY7hztm5uN+eEbYKqPtiQuiahAm9EG++rc5xgaMjfqUDQf8HJMJ7gTRDNqezL6J13PUe4s55u1r2DHzn/iTs1o9x+mwMfWoHF7++BC/r/GR6ZYSnAlPrDo2UG/qVFeWmceiDVPnKrvpNXFlRw4hS0438wiC18s0KlDPXRSmYzOfuwh92RtA64tIQi8ndIxu+IuZEG8Vr0nNNefgfkPM8Vh9ENz9G+d6bFB1AJ67Kpg71Tj2cuxO/npWPikPydwuCKEkyG1IghOpwsypN5tGREvVNAzDLPFadRA23dO8g/F5a82Vv5piePTs8NCGt26F6TeFVfuwnbcWV0o/adIkRM9X75j5J0nuqA4vqjH4sMhgfoLllgec/dg/4VpGbvkDR//nV3x26t+b5dTYfZVkHNyALVBL5YCTOH3sAN749DBr3tvHNace3U2SC1ETi46tqzRj+0Orc1n61Tq2+nB4CFlanqmbLd0bPEelZETUsSogOrbPEzpGLYM5NJ8ppb/Zv8RTHD4Wz11j9kazqs3lTwkbeypzGMkL/iFzuyA0QQyUaGittF9Lqxs1xVD2pdlZdsatzTvIPn5+4/WaKqaJ5zcaJyHHq0tebr7K2EqTJsPQlHp8+PwBnA5783KZQu+l5ohZYvhb50R9yr++9qOBkwZ2nlgdpTZjFIfGXMTgT+9j1MYb+fLkW9F2J/a6CgZ/ej+DPr0fu98DgFY2Boy/hkn5M7j/P19x6bSRuJySbJrQxKJjwdSzkfSrdW5TPTvtF403kaHntFPHhiL6tpfTVonfikKo2Nd8LD4xz7wHsAyUCGNPHdkjc7sgNCGhDBSl1P3ATKBIaz0+uK0/8AQwAtgLnKO1Luty4dpb2s/vM1euy/e13IHWUnJNFVNqbuTjlb35KmMLTZoMQ7PrcBVXPLyFwjIv+VmuyOUyhd7JF2+aYS75J0R9yut7/QxwwYj0TpQrBsryT8PhqyBvz9P0K9pCTeZo+h1+D0d9NRUDTqJ02JkYDjfZe18k/6O/8echB5nhOZv7N3wlXpSeQEd1LLS9+txUz7akk9uhY0MRfdtHaG2MBuob5/xQyveZc7pFpLH39tLmPVJkbhf6OImW+fcgcGaTbTcBb2qtRwNvBt8nPg6nWbEjtIpMKNbqSKTOsGkDWuh4bGu9430IpR5fgwIDKCzzcsXDWyj1iMu4T7DrJbOzcc7oqA4vr9W8Xehn6qB29UXscoqP+ilfT7wBv7Mf7rJdVOVO5ouT/0zhhJ/jzRxDXdpQvhm3kOIRZzHmwNPcnLuRFeu/4HBlbXeLLsQbS8daejaUpqvPTfVsS+e0Q8eGIvpWwJ7U8rgKndMjHVNdBP0Gy9wuCCEklAdFa/2OUmpEk82zgOnB1w8B64Ebu0yo9mIYZuiB32cqmLNXw7t/a94J2VodieQ2dmW3vIoX5Sqjzx9oUGAWhWVeKZfZF/B54POXYeT0YHGGtnnpy3r8Bkwf0rmixYPq3ElU505q+QClKDr6XFKq9nHZkVW8EhjMr5/N5t4Lp6AS2foSoqOpjq0pbd4hvunqc1M9m+SKWceGIvq2j2MYZiXOzOERxuJjZqPctsaeq39UHeFlrAl9hYQyUFpggNb6YPD1ISBxS1oYBhTtDFc889bBD28zl6Uvedls4NQ0djXShNharGsUOB128rNcYYosP8uF0yGx+L2eXS9DvRdGfi/qU9Z9Xk9+Ghyd0YlydSXKxoHxV3HUe79mtfMevv3pIO77TzaXf+eo7pZMiIWWdGz2ALj4RTOs0eEyQ2qa6sumetbVPyYdG4ro2z5M6JhMy4MzboGLXgQdMI2R1Ly4jj0Za0JfIdFCvFpFa60BHWmfUupKpdQWpdSW4uLiLpYsSKRmY2tmgyMJMoaYzRibNghrCUuhRXt8E7JTndx74RTys1wADXGq2alScjVR6LQx+8HD5qQYZXnhj0sCbD0c4MxhiR3e1V4Czn58M24ROXX7uaP/s9z60qe8sOOb7harR9PterYlHWtTprGSNQLSo9SXMerYUETfJi6dPmZDx2ThFrj/THjox+B0Q/rAyOMqhrEnY03oK/QED8phpdQgrfVBpdQgoCjSQVrr1cBqgClTpkQ0YjqdSBW5uqlUoM2mGDMgnWevniaVPhKUThmzJV/AV2/DpAujDu964CMfKXY4fVjbx/Y0PP3HUTp0Bj/c/zxz+5/Iz9cqDlfWcem0ERLu1QG6Xc8mkI4NRfRt4tLpY7aLx6SMNaGv0BM8KM8DFwVfXwQ8142ytI5VKSaUKEtUdgY2myI3PZkhWW5y05NFgfUFNt4FtiQYfXpUh39RFuAfu+s5YxikJXWybN3E4dHnUecexB+Mu/j20CSWvLCTXz35IeU1klTa40gwHRuK6Ns+SjeMSRlrQl8goQwUpdRaYCMwRilVqJS6DPgzcLpSajfwg+D7xCRSRa4oSlQKQlyoOADb18DRPzBLWbaB1ppbNtaSbIfzoiv21SPR9mQOjFtEsreI29IeY/bkfJ7bfoBTb1/P45v34fMb3S2iEC2iY4VEQ8akIHQKCRXipbU+v4Vdp3WpIB2lrUZOgtCZvPlH8/lbc6I6/L6PfKzfH2DheMhI7kS5EgBv5mhKRv6EAV8+w9Unn8AJZ8/k/g1fcdMzH3HHG59zybSRzC3IJzutl/8hejqiY4VEQ8akIHQKCWWg9Ao6UKJSEGLmy7dhx+Mwfk6b489vaFZ96GPZ5jqmDoSzRnSNiN1N0VGzcVXuYeTmxXhPH8Xvzyrgw8IKXtzxDX9++TNuf3UXZ4wbwPknDmPaqBwJm0hURMcKiYaMSUGIO2KgCEJPp7oInl0E/YbA8ec1bPYbmr0VBl9VGBzyaA57DA56NBu/8fNNtebbg+H6ib2rcler2Ozs/9Z/c9Tm33Lsvy7ls+/fx8ShJzBxaCaFZTW89VkR/95dwksfHSI/y8V5Jwxl7pShDOiX0t2SC4IgCEKfQgwUQejJ1ByBx84xm9X9cClFPiev7vLx0pf1bD0cwBfSu8uuoH8KjEiHy46Dkwb0IeMkiJGUxt6CXzP8g//juDcu4suTb6Vk5Czys9wsmDqC804cxvt7j/Cvz4q4/bXP+csbu5k9OZ+F3zuKo3LTult8QRAEQegTiIEiCD2VvRvguavRFQdYP+oGVm4YxOaD1WggPw1+PByO6gdD0iDXZeaZ2PuYQRIJf0o2e6csZuiHdzB6w68YuOthSkb+FzVZY/AnpXNGPx8zTqjnSLWfzV+W8Pa2fZy1ZTfTvzWSS789kklDM9sM/6qsrWfr12Vs2XuE978q43BlLd76AJnuJI7OS2Pc4AxOHNmfCfkZJEuDNUEQBEEIQwwUQUhUtIbacmprqvBUVeDzVhOoOIA+9DHpX75EZuUuim05XFN7M5s/GkN+WoDzj4FvD4ZhaX3PO9IeAs4M9k75LVkH3iL76xcZ+f7vIx73XeD6JCAJij/PZPdng1mXNBz7gONw540gNWsguPpTY9gprjH4tNTPlm98fH6oCg3YbYoR2W6GZLlw2m1U1tbzwdflvPTRIQCcDhsT8zM5YWQWxwxIZ1CGiwxXEq4kOylOG/1SkkhJEgNGEARB6FuIgSIIiYoRgKUjSAGaZkFsN0bxnHER2zN+wLEj07h8RDJHZ9ml+WA7qcs6l2/GnYPdW4yzch8qUIu2OdE2O1YVdruvAkf1QeyVBxhV+jWTatbjOvgyHGx+vVc4hV0DFjPv5GGMH5LBsQPTIxoYFd56PjlQwcffVPLJgUpWrv+SgG7eQ+6604/hv0/rxTWgBUEQBCECSkeYFHs6Sqli4Osu/tgcoKSLPzPe9PTv0FH5S7TWZ8ZbmPbQTWO2K+jpY6qziPXv0tvGbCKNk0SSBRJLnlhk6QljNpH+1tHQk+TtibJ2+5jty/RKA6U7UEpt0VpP6W45YqGnf4eeLn9vRH6TyMjfJZxE+nskkiyQWPIkkiydQU/7fj1JXpFVaC/SSUgQBEEQBEEQhIRBDBRBEARBEARBEBIGMVDix+ruFiAO9PTv0NPl743IbxIZ+buEk0h/j0SSBRJLnkSSpTPoad+vJ8krsnYSSqnfK6Wu72454o3koAiCIAiCIAhCD0Qp9XugWmt9e3fLEk/EgyIIgiAIgiAIPQCl1IVKqR1KqQ+VUo802XeFUur94L51Sil3cPtcpdTHwe3vBLeNU0ptVkptD14voWraiwdFEARBEARBEBIcpdQ44FngFK11iVKqP3AtQQ+KUipba10aPPYW4LDW+m9KqY+AM7XWB5RSmVrrcqXU34BNWus1SiknYNdae7vruzVFPCiCIAiCIAiCkPh8H3hKa10CoLU+0mT/eKXUv4MGyTxgXHD7BuBBpdQVgNU9eCNws1LqRmB4IhknIAaKIAiCIAiCIPQGHgR+prX+FvAHIAVAa70I+A0wFNga9LQ8BvwE8AIvKaW+3z0iR0YMFEEQBEEQBEFIfP4FzFVKZQMEQ7xCSQcOKqWSMD0oBI8bpbV+T2v9W6AYGKqUOgr4Umt9J/AcMKFLvkGUOLpbAEEQBEEQBEEQWkdr/YlS6k/A20qpALAN2BtyyGLgPUwj5D1MgwVgWTAJXgFvAh8CNwILlFL1wCHg1i75ElEiSfKCIAiCIAiCICQMEuIlCIIgCIIgCELCIAaKIAiCIAiCIAgJgxgogiAIgiAIgiAkDGKgCIIgCIIgCIKQMIiBIgiCIAiCIAhCwiAGiiAIgiAIgiAICYMYKIIgCIIgCILQy1FKTVdKvRB8/ROl1E1d+NkTlVI/ivZ4MVAEQRAEQRAEoQ+htX5ea/3nLvzIiUDfNlDOPPNMDchDHtE+uh0Zs/Jo56PbkTErj3Y+uh0Zs/Jo5yNm6vyBqQfKvO9+Xer56kCZ9906f2BqrNdUSo1QSn2mlHpQKfW5UmqNUuoHSqkNSqndSqkTg4+NSqltSql3lVJjIlznYqXUXcHXo5RSm5RSHymlblFKVQe3T1dKrVdKPR38zDVKKRXc91ul1PtKqY+VUqtDtq9XSi1VSm0OyvcdpZQT+CNwrlJqu1Lq3La+Z680UEpKSrpbBEFoFzJmhZ6GjFmhpyFjVuhK6vyBqZ8frn7+3NUbp35v2foR567eOPXzw9XPx8NIAY4GlgPHBh8XAN8GrgduBj4DvqO1ngT8Fri1jev9Ffir1vpbQGGTfZOAXwBjgaOAacHtd2mtT9BajwdcwMyQcxxa6xOD5/1Oa+0LyvGE1nqi1vqJtr5grzRQBEEQBEEQBKG7KKnyLb/q0a05hWVeAArLvFz16Nackirf8jhc/iut9UdaawP4BHhTa62Bj4ARQAbwlFLqY+AOYFwb15sKPBV8/ViTfZu11oXBz9oevD7AqUqp95RSHwHfb/IZzwSft4Yc3y7EQBEEQRAEQRCEOOI3jEGWcWJRWObFbxiD4nD5upDXRsh7A3AAS4C3gt6Ns4CUOH1WAHAopVKAFcCcoNfl3iafURd6fEc+VAwUQRAEQRAEQYgjDpvtYH6WK2xbfpYLh812sAs+PgM4EHx9cRTHbwJmB1+fF8XxljFSopRKA+ZEcU4VkB7FcYAYKIIgCEJvQWvw1XS3FIIgCOSkO6+7Z35BiWWk5Ge5uGd+QUlOuvO6Lvj424D/U0ptIzoPxi+AXymldmDmt1S0drDWuhzTa/Ix8CrwfhSf8RYwNtokeWWGrPUupkyZords2dLdYghxwDA0pR4fPn8Ap8NOdqoTm03F+2PifsH20pPHbBf9RkI43f4HTsgx+9w1sH0tzPgTnHxVd0sjhCNjVogLXTjnxHzROn9gakmVb7nfMAY5bLaDOenO65Id9o3xEC6eKKXcgFdrrZVS5wHna61ndadMHYoLE4SuwDA0uw5XccXDWygs85Kf5eLeC6cwZkC63AAnCPIbCQnDwR2w7VHz9Wu/gWN/DJnDulcmQRDiSk+bc5Id9o1DslyndLccUVAA3BUsFVwOXNrN8kiIl5B4GIamuKqOwvIaDlXUkpuWDJjJZVc8vIVSj6+bJRQsSj2+hokCOv83ssbGgbIaiqvqMIze5wEWOsjHT4PNAWfdaYZ6vX9fd0skCEKcKfX4uOP1XSyeOZYnrjyZxTPHcsfru+S+IEa01v/WWh+vtZ6gtf6u1vqL7pZJPChCQhFpdWTp7Anc/uoutu0vp7DMi88f6G4xhSA+f4BIVUo64zfqaStnQhez913IOQb6HwWDjoed/4Af/B6UjA1B6C0YhsFFp4zkxnU7wu4RDMPobtGEOCMeFCGhiLQif+O6HSyaPgowk8ycDnt3iiiE4HTYiVSlpDN+o6721gg9iPpaOLgd8o4z3w+bCmV7oWhnt4olCEJ8CWgajBNovEcIiDO91yEGipBQtLQin+lKalgxz051dpN0QlOyU53ce+EUQquUdNZv1JXeGqGHUfoFGPWm9wRg2MmAgs9e7FaxBEGIL1rriPNAbyz41NeREC8hoUhy2MjPcoUpoPwsF/lZLp69eppUiEowbDbFmAHpPHv1tKgrqnS0AovlrWk6NsSjJlD8mfmcEUyKd2VB9tHw5Xr43v90m1iCIERHtPOCzAN9B/GgCAmDYWiqa/0smzOh2Yr8oAwXuenJYpwkIDabIjc9mSFZ7jZ/IyuP5OwVG5i29C3OXrGBXYerokp270pvjdDDKN4FygYZQxq3DRgLB7aAv67l8wRB6HbaMy/IPABKqRFKqY/jcJ0pSqk74yFTZyAeFCFhKPX4uPD+zeSmJbN45lgyXUnU+AIM6CeGSW+hpTySZ6+eRm56cqvndsRbI/QRyr8Gdw7YQ25S8sbBzufgm23BkC9BEBKR9swLMg/ED631FiBhGwOJgSIkDFaOQWGZl4WPbG3YvuHGUyG1GwUT4kaseSSWt0YQwqgohNTc8G0DxpnPX78rBoogJDDtnRd61Dzgr5tKddFyDP8gbI6DpOVdhyM5Ho0aHUqpNcBk4BPgQuA44P8BaUAJcLHW+qBSaj3wHnAqkAlcprX+t1JqOnC91nqmUioXeAwYDGwETsfsjZIGvAz8BzgFOADM0lqH/2CdgIR4CQlDV1aEEroH+Y2FTqFiP6TmhG9LyYCMfCh8v3tkEgQhKnrtvOCvm0rRp8/z4I+mcufEETz4I/O9v25qHK4+BlihtT4OqASuAf4GzNFaFwD3A38KOd6htT4R+AXwuwjX+x3wL631OOBpILTL7Wjg7uC+cmB2HORvEzFQhIRBYkt7P/IbC3HHMKDyYHMPCkDWSDj0UdfLJAhC1PTaeaG6aDlPLsihfJ/5vnwfPLkgh+qi5XG4+n6t9Ybg60eBGcB44HWl1HbgN0B+yPHPBJ+3AiMiXO/bwOMAWutXgLKQfV9prbe3cX7c6fIQL6XU/cBMoEhrPT647QlMaxBM91O51nqiUmoE8CmwK7hvk9Z6UddKLHQVElva+5HfWIg7niKzxHAkA6X/SNj7b/CWgyuz62UTBKFNeu28YPgHNRgnFuX7zO2x07SCQBXwida6Je+MVS0kQPvv/UMrjQQAV0sHxpPuyEF5ELgLeNjaoLU+13qtlFoOVIQcv0drPbHLpBM6lbZKCfao2NI+SkfLBFvIbyzElYpC87lpiBdAVrAvyuFPYMS0rpNJEAQg+vmiV84LNsdBMoeNCDNSMoeZ22NnmFJqqtZ6I3ABsAm4wtqmlEoCjtFafxLl9TYA5wBLlVJnAFlxkDEmujzES2v9DnAk0j6llML8A63tUqGELiGWErNCYiC/oZBwNBgoLXhQAA7HXJFTEIR20ufni7S86zjnkRIyg+kcmcPgnEdKSMu7Lg5X3wVco5T6FNOY+BswB9PA+BDYjpnUHi1/AM4Ili+eCxzC9Mp0G4lWxes7wGGt9e6QbSOVUtswk4B+o7X+d/eIJsRKLCVmhcRAfkMh4WgwUPKa73P1N5PlJQ9FELqcPj9fOJI3knfcT7j4pbhW8dJa7wWOjbBrO/DdCMdPD3ldQjCHRGu9Hlgf3FUBzNBa+5VSU4ETtNZ1wF7M3Bbr/Ntjkb09JJqBcj7h3pODwDCtdalSqgD4h1JqnNa6sumJSqkrgSsBhg0b1nS30M0YhsZb74+pxGxvoyeO2VjLBLeHWEPJhPiTkGO28gA4UsAZoRa5UpAxFEp2N98n9AkScsz2QiLp666cLxIWR/JGMoe2x5PRXQwDnlRK2QAfcEU3y5M4VbyUUg7gp8AT1jatdZ3WujT4eiuwBzgm0vla69Va6yla6ym5uRFc/UK3Ybl59xR5emcpwQ7SE8dsV5WD7POhAQlKQo7Z6sPg7m8aI5FIHwRHvuxamYSEISHHbC+jJX2d5LDJnN9D0Frv1lpP0lofr7U+QWvd7fXZE8ZAAXv9Jh8AACAASURBVH4AfKa1LrQ2KKVylVL24OujMGsxy0zTw7DcvHe+uZulsyf0vlKCfYiuKgfZUmhAqccX188RegGeEkju1/L+foPMSl911V0nkyD0IVrS1w6b6p3lg4UuoTvKDK8FpgM5SqlC4Hda6/uA82ieHP9d4I9KqXrAABZprSMm2AuJS2iH+Ntf3cXimWPJdCWRn+ViUIZLwnZ6EF1VDlJCA4So8RRDSislhNMHm89lX8HAb3WNTILQh2hJX3t9gd5ZPljoErrcQNFan9/C9osjbFsHrOtsmYT40jQW1eU0w4IKy7xs21/Owke2kp/l4tmrp4mi6oFEWw4ylhwSK5QsdNKT0AAhIp4Ss2N8S6QHWw4c+VIMFEGIA63N8RaWvu6V5YOFLiHRkuSFHo4Vi3rFw1vITUvm2tNGMzInlbVXnMySFz7htZ1F4ubtA4SOg8Iyb8NvPjo3jTJvfZtGixVK1vR8GTNCGIYB3iNteFBCDBRBEGIikm5/+NITWXvFyRyurKXU42Pd1v388vQxoq+FmBADRYgrVixqbloy188Yw43rdjQosVULClgyazw2m03cvL2cSDHJd7y+i5//4BgWPrI1zOgYMyC92VjotZ2FhfhSWw6G3ywl3BJOt2nAiIEiCDHTVLfnpiVzuLKWG54On+tH56aJvhZiIpGS5IVegBWLumj6qAbjBMwb1IWPbMVms5GbniyKq5cTKSZ5dsHQBuME2k58t0IDhmS5ZcwIkakpNZ9TWkmSB0gfCGV7O10cQejtNNXti6aPajBOoHGuL/PWd5eIQi9BDBQhrli5A5muJEly7sNEKkecneqUMSHEF0+x+dxaiBeAOwcqv+l8eQShl9NUt8tcL3QWYqAIccXKHajxBaT+eR8mUjnivPRkGRNCfPGUmM+thXgBpGabBoqWPjqCEAtNdbvM9UJnITkoQlyxcgcG9Etm1fwCFj66VZKc+yCRckiyXEmS+C7El5ooDRR3DtTXQG0FuNrwtgiC0CJNdbvLaRe9LnQKYqAIMROpnGz/1GQyXU5Jcu7DRCovGY/E91jKFwu9DMuD0lqjRoDUHPO58hsxUAShA7Smd2WuFzoDMVCEmGipnKxVmUnqnwuhxDom2hpvQh/DUwLONLAntX6cO8RAGTC28+UShF5EW3pX5nqhM+hQDopSyq6Uuj3ewgg9j0jlZFurzCQIsSDjTQjDU9x2eBeAO9t8rjzQufIIQi9E9K7QHXTIg6K1Diilvh1vYYSeh1VycNLQTBZNH0WmK4lybz2GYXS3aEIPpbVQgkjli6ViTB/GU9J2eBeAuz+gpJKXIHSApnrXmu9rfH6Kq5CQLqFTiCXEa5tS6nngKcBjbdRaPxOzVEKPwemwc8bYPC46ZWSzpoy56SmitIR20VYogVXiMnSylIoxfZiaKD0oNodppIgHRRDaTajenTQ0s1kTZgmzFTqDWAyUFKAU+H7INg2IgdKHyHIl8duzxnHe6k3NGjU9e/U0iUsV2kVLoQTWWLJKXN7x+i5mFwwlO9VJXnoyWa42chCE3omnBDKGRXesO1sMFEHoAFmuJB67/CSKqurIcCVxyYPvt6ijBSFedNhA0VpfEk9BhJ5BaPiNy2mnvKYepWD53OMp99azcv0etu0vl7CbPoxhaMq9Pry+AAGtSUmyk5Pa2Ak+lhAum00xOjeNn//gmIau9LKC10cxDKg50naTRouUTKg63LkyCUIPpSW9bBia3cXVDYtC/VOdLJ45tmGuBwmzFTqHDhsoSqljgHuAAVrr8UqpCcBPtNa3RHHu/cBMoEhrPT647ffAFUCwNTA3a61fCu77X+AyIABcq7V+taNyCx2nafjNUwun4jcMbni60dW7dPYEbn91F8XVdRJ20wcxDM3eUg+HK2vDxoVlQAAxh3CVeesbjBOQFbw+S2056ACkRJGDAmZ54bK9nSqSIPREWgutLfX4uOP1Xc3CuK25ftv+cgmzFTqFWDrJ3wv8L1APoLXeAZwX5bkPAmdG2H6H1npi8GEZJ2OD1x0XPGeFUkr+E7qBpuE3OWnOhptQMG8Ub1y3g2tPG82qBQXSqKkPUurx8XVpTbNxYVV8aasaTKQO9E2bfkmivABE30XeIiULakpNz4sgCA20ppd9/gCzC4Y2GCfW/hvX7WDR9FHSmFHoNGLJQXFrrTcrFRZS4Y/mRK31O0qpEVF+zizgca11HfCVUuoL4ERgYztkFeJA0xvDgNYRbxRH5qTiSrJJuE0fxOcP4HbaWzUg2grhaquZoyTKC0BIF/koQ7xcmabHxXuksXGjIAitLvpYOjjS/mMHmrpaqngJnUEsHpQSpdQozMR4lFJzgIMxyvMzpdQOpdT9Sqms4LYhwP6QYwqD24QuxroxtDhUURv2HswbxUOVtdhssQwtoafidNip8QUijgunw95sDIXus7Cafg3JcpObntxs4ovGyyL0ATzBaOD2hHgBVBd1jjyC0ENpTS9npzrJTU+OuD/JbouoowUhHsTiQbkGWA0cq5Q6AHwFzIvhevcASzANniXAcuDSaE9WSl0JXAkwbFiUVV2EqLCSnn3+AGsuP4mDFbUsffkzHt64l1XzC1j4aGOy8h3nHE92WrLcLEZBbxyz2alOhme7WTZnQngOyoIp2G3g9QV4auHJfH3EiwJqfAGGZ7vbNV6i8bIInUNCjVlPOz0o1nGeIkC6yfcVEmrMJihZriTWXnEydX4Du4KSah/Zac4GvTq4Xwor5xewKGSuXzm/gLw0yfkTOo9YDBSttf6BUioVsGmtq5RSI2O4WEN5FaXUvcALwbcHgKEhh+YHtzU9fzWmwcSUKVN0R+UQwmkp6XnVggIGZaTQLzmJZ64+hdp6U7G5nHYyXXKzGA29cczabIoR2alkupN44sqTCWhIcdiorvPzk7s2kJuWzP+cOaZZAn1HPkcS4ruehBqzDQZKez0oxa0fJ/QqEmrMJiBWla7QBPllcyaEHZOUZOfYAek8uXAq/oCBw24jLy0Zh0MiJYTOI5bRtQ5Aa+3RWlcFtz3d0YsppQaFvD0b+Dj4+nngPKVUctAAGg1s7ujnCO2j1OOjtNpHbb3B8rnHmw0Y05JZ+MhWAgY4HDby0lMY1t/NkCw3/VPF3dvXsdkU/VPNEK1h/d0opbjw/s0UlnlZNH0UNzy9g9y0ZFYtKGD53OM5VFFLudfX3WILPY2aEkhON5swRkNKMGrYIyFegmAYmuKqOgrLazhUUUtu0BtSWOblhqd38HVpTUPxEjDn+sGZLoZlpzI40yXGidDptNuDopQ6FrOiVoZS6qchu/phNm+M5hprgelAjlKqEPgdMF0pNREzxGsvsBBAa/2JUupJYCdmEv41Wmsp1xNnmvY38Ruaer+BAjLdDn755PZm5QWlapIQDaEJmJmuJHLTkpt1Il41v4BMlxnmFVqLP8uVRJm3XkK5hOZ4SiA5ygpeAM5UsCVJDorQZ2ja2yRUnwYMzS0v7uS1nUXNygYXlnlxO+0yxwvdSkdCvMZg9jDJBM4K2V6F2cekTbTW50fYfF8rx/8J+FM7ZBTaQWgN9KYhOGeMzeOmHx7H386fRFFVHSvX7+HGdTtYMmu8VE0SoiK06la5t55rTxvNjetML8rimWPJdCVRVFXHwMw6iioby12eMTaPa087Jizu+d4LpzA6N02MFsFMko82vAtAKTPMyyMhXkLvp2lvk4XfGcFZE/PD9OnS2RMorjK9JD6/wbK5E9hT7GHd1v3U+AIyxwvdSrsNFK31c8BzSqmpWmsp9dsLCK2Bvnjm2AbjZNLQTC46ZWRDeE7oKsvInFRJhO+ltNbpPZr9TbGqbl3x8BZWrt/D7eccH9GLsnJ+AXe++XmDt2V2wdCGyRQaa/M/dvlJXPD396SLfF/HU9KYVxItKRniQRH6BKHz+qShmZx74vCGuRwae5ksmzMBQxOmi++ZX0BemhONxjC06FahW4gliLBUKfWmUupjAKXUBKXUb+Ikl9CFhIbgDM5IYfHMsTxx5cncNmcCD737VcRGjG6nXZRWL8RadTt7xQamLX2Ls1dsYNfhKgxDR7U/EqFVt+66YBLuJHuDFyV0bC16dCuzCxrrYWS6kiLW3i+qqmux0WPT71JcVceBshqKq+palVHogXiKo2/SaJGSKQaK0Cew5vVJQzO5LZj0vnjmWCYNbTTqC8u8DOyX0kwXX/XoVrYXVvDTFe+2qd8FobPork7yQgJhheBMGpqJoWHJCzs5d/UmLnnwfS46ZWQzhTY8243DLsZJb6StTu9t7W+J0N4mA/qlMDInNaLxcXRuGk9ceTKrFhRQHzAi1t5v+lmRush3xJASehCGAd6yjhkoEuIl9AGcDjtnjM3j+hljuOTB9/n+8rdZ8sJOrp8xpmFOz89ytdhw2Vogika/C0JnEIuB4tZaN62mFVUneSGxsEJwbvzhsVzz2AfNPCaLpo9qODY/y8U35V68Pkme64201lE4mv1NMQzNEY/pxdh/xMM35V6Kq2pxOmwRjY99R2o4d/Umlrywk/QUByvnF4Q1ZFy1oIB1W/c3O69prHRHDSmhh1BbbnaFb6+B4so0q39pMVSF3k12qpPf/HhsM++INadbYbWupMhNGsu99Q3nSLK80B0kWid5oRuw2RSjc9MY0C854s2nlWuSn+Xi7gsm8/DGvZI810tpq9N7NJ3gLaweOrsOVXHu6k1857b1nLNqI18Ue3how5fNjI9lcyZw55u7AXPcXfPYNgZlJvPs1dPYcOOpPHv1NMbkpfPL08e02UW+vYaU0MOwvCDtqeIFpgcl4DMNHEHoxdhsCrtNRdSDo/PSWHP5Sbz16WH+8M9PWDFvcphOXTp7AivX72l4L/O90B3Eu5P8/LhIJXQ5R7w+9pbUNFRbssjPcpHXL4WnF01lQL8UHt34Fb88fYwkyPdSQhPaQ5PQrd+7rf2hlHp8fF1aw+LnPm62grd45ljufPNznlw4FZ/fQKP51RMfsm1/441jYZmXWp/BkCx32HWj6SIfWjnMwppo25vkLyQgVh5Je5PkQ5s1urLiK5MgJBgt6cHdRdUseWEnD1x8AsvfMBeFnrjyZDTmwtItL+5k2/7yVvW7IHQ2HTZQtNZfAmGd5OMnltCV+P0Gnjo/d765m6WzJ4RX85g3mYoaH3NWbuSdG6Zz+XePlhu6Xop1497fncSTC6eitW52A295255cOJX6gEFSsKNwpPHg8wdwO+0txje/trOI3/zY4KsSD0l2RXF1HZOGZrJo+igyXUnU+MyePE1p2kXeSoYPNThaMqSyXElhpTelClgPxWq2mNJBA8VTBLnHxFcmQUgwMlMc3DO/gKualBa+/dVdFJZ5sQd13ms7i7jyu6NwOe2MyUvnT2dP4HdnyQKO0L102EBRSmUCFwIjAIdS5gDWWl8bF8mETscwNOVeH1W1fmrqAhRX13H7q7saelPU+AJo4GBFLflZLuxNbgyF3kPTmvmNN+6uZiWGdxdXt3qDbxk6YMZ/RlrBsxLg/YZm8XMfc9cFk7hn3mSq6/wNZa6ta2e6mk+QoV6Qpg3HHr70RNJSHBENrZZyU569epqM7Z5EgwelnV4Qy6CRSl5CL8aa2/0Bg5y0JB6/8mQOVdRS6vE1NGPMz3IRCBYNsYqPLHlip+hCIWGIJQflJUzj5CNga8hDSHCsxOVPD1by4f4K6gMGWalJLJszgeLqOhY+spXrnvqQlCQbWe4k1m3dz7I5EyKuZgu9g2iTyls6rsRTB4RXz/rZY9sYkpnCsjkTmuWaJDvsLJszgYBhkJuWzM8e20b/VGeDcRJ67YMV3rAywU0rdF3w9/e45tTRnFOQT25aMocra/npinc56f/+xTmrNlJZ629YBZTclF5CdRHY7JCc1r7zLINGKnkJvQzLk3y4wsunhyo5WF7L4Sofnx2s5o///IQ6v8GSFxpDt1bOL+Ded74MyzkRXSgkErHkoKRorX8VN0mELsG6uTtUUcvi5z5m+dzjSU12UFsfICXJzgMXn0BtfYBvKmq57ZVd/L9zj+e3Z43D5zfIdEkcam8l2hv3lo7z+gIUVdUCNBgwhWVeSqp93PZKo1eu3FvPba/sYtncCdzw1A6Kq+tYPHMsCx/ZSp3fiHjtwjIv1z31YYOnJpKRdM1jH7D2ipOoD2iOeHwsnjmWlev3sG1/eZiHpLXcFKEH4SmClCxQ7VxjS04HZRcPitCrCPWAL545liUv7GTtFSdz/r2buGfeZGYXDMXtDJ/fM1wOZhfkc9X0UfzqyQ8bDBfRhUKiEIsH5RGl1BVKqUFKqf7WI26SCZ2CdXOXk55MbloyQ/u7OFLtY8F9mzl7xbtc8uD7VNb6Wbl+D8XVdewtqcEGjMhOlTjUXky01blaOu6zQ1X8dMW71PgaDZhJQzNJS3Y0eOXOXb2JhY9spbi6jj3FHrbtL2/IR7HCDSJdu3+qk+Vzj+dQRS3lXl9EIyk3LZnymnouvH8zc1ZuDKv3H2poWbkpbVUBExKc6qL2J8iDadCkZDTmsAhCLyB00SYvOLfbbbBszgSUUix5YWfY/L5u6352Hqziuqc+ZG9pjSTECwlJLAaKD1gGbKQxvGtLPIQSOg/DMFg8cyyD+iVzx3nHUx/QLAwm0EF4t/gV8yYztL8Lp8MmxkkvJ9ob9+xUJ6ualAdeOnsCb+48zOKZY7Er1dD08/oZY1j26mcsnT2h2fGhJSwzXA4evewkHDbFmstP4oyxeQ37VsybzLJXP+Pc1ZtY/NzHHKyoxRb8jFCuPW00V62J3MMn1NAK7WrfULpYEuR7HtVF7e+BYuHKNKt4CUIvwecPkJuWzNorTmJolos7z59Imaee2nqDRRHm95t+eBzrtu5n5fwCxg5KF10oJCSxhHhdBxyttS6JlzBC52IYmhKPj4NlHoyhGTiUjUOVtRHDao7KTcXltFNe40MjCqu3E3rj3lr5XZtNMSgzhSWzxjM8283uomqe23aAC04eRpmnHr9h8PClJ1JUVcf1T31IYZmX4ioz5Co71cmAfikseeGThhW7uy+YREDD/Pvea0iMXzFvMr89axwBQ/PYpr3MLhjKZd8+inJvPX9943Ou+M4o7r5gckNT0fwsF8Oy3S328GlqaDWtAgZI6eGeRvVhGDCuY+eKB0XoZaQm21k651vU+w3KvfXYlGLho1tZPvf4iHrRblP874+Oo6TKh81mY0iGJMULiUcsBsoXQE28BBE6nxJPHX9943P+OGs8xVU+vPUBSj2+iDH5AP6AgcNmE5dvHyHaG/dMl5OBGSl8XVrDuq37+fWPx/JNubeh30l+lotHLjuxYUxt21/OwkfM+hnPXn1KmMFRVevnpmc+Clvhu3rNByyZNZ5jB6Xz3TEDwspeL509gZQkG49s/JqHLz2RIx4fpR4fxVV1Ecfx4EwXA/ultGpstFzBTFYTExKtwVPS/hLDFimZUPpFfGUShG7CMDSVXj9HPPW4nXbKPD6UMhs0lnvrI+pFu01RXu0jO80p87uQsMQS4uUBtiulViml7rQebZ2klLpfKVWklPo4ZNsypdRnSqkdSqlngyWMUUqNUEp5lVLbg4+VMcjb56mtD3Dh1BH4DTOsq9TjY93W/c1CcFbMm0x9wMDttEvuSR+mabWss1dsYNdhs93RmAHpTBqWwX+fdgyG1s2qb1lNP0PJz3JRXlMflo+SkhS5T4rbaccf0A3GibX9xnU7SE12cNrYAVR468lwJTF2UDpH5aRGDFFryziB6CuYCQmCtwyM+o43WnRlmSFiWsdXLkHoBko9Poqq6hie7SLL7eSXT37YsPC4cv2eZvP7PfMLcDttDO3vlvldSGhi8aD8I/hoLw8CdwEPh2x7HfhfrbVfKbUU+F/gxuC+PVrriTHI2WcJXf1Octhw2m0My3YTMDSFZV5Wrt/D9TPG8NC7XzWE4OSmJ9MvxU6GK3LzPaHv0FbPkPoAXPXoVh685IRmRsadb+5m5fyChvjn/CwXq+YXkOF28MDFJ3Dnm7sprq4jNz054gpfjS+Ar4WqXgBLXtjZcN27L5jMwH62qELUIiGlh3sYHe0ib5GSCYE6qKvseB6LIHQTTb3ahmFQHzDw+TX1gca53Wq6fPuru1gyazwjclJJtisCWpMp87vQA4ilk/xDHTzvHaXUiCbbXgt5uwmY01G5BJNIYSsPXnIC7iQ7AcybwG37y7n91V0smj7KDN1xO0lNtpOVmtLd4gsJQGs37oah8db7KSzzcijYyDP02OLqOuoDARbPHMtxA9MBGpopnjE2j7+cZ645VHjruWfe5IYEd6tPSm56MsXVkcO2vi6taVZi+MmFUyOGqEWDlB7uYVQdNJ9dHSwaaRk21cVioAg9ikjz+qr5BYzMceOtN3DYVNjcbi08ZrqdOGzgsNsYmCbGidAzaHeIl1LqyeDzR8GQrLBHHGS6FHg55P1IpdQ2pdTbSqnvxOH6fYKmq9+5ackUV9VxzupNXLt2G3ecc3yDIlvywk7qAwag6e+WZLmejtWw60BZTViDw/bSUknhJIeNXYer2FPkIT/LxfLXPmf53OPDwgiWzz2eP/7zU5a8sBO7TXHB39/jtZ1FTBqayUWnjGTe39/je8vWc/WaD6jxBVgxbzJv3zCdJ648mTED0xme5WZYfxf3zJscdt2V8wu4883dYTIVlnnRMYTrSOnhHkZFofmcmtux863cFUmUF3oYkbzaf33zc0qr61lw32Z++cT2Fud2gBwxToQeREc8KD8PPs+MpyAASqlfA35gTXDTQWCY1rpUKVUA/EMpNU5rXRnh3CuBKwGGDRsWb9F6HKGr3+cU5POz00bjDxg8cPEJ3PvOl9z60mfcPvd4BmWkYGhNssNGbqoor66kM8ZsPBO+rRv3ptdCg6E1w7PdPHLZiewtqWHd1sJgGIGbw5V1rNtayLWnjebovFQMDcvnHk+5t55+KY5m+SrXPfUhTy6c2pAvEvodctOSG65bWu2jqrae4uq6MDlj9XZEW8FMSBA9W3kAUODO7tj5Vu6KNGvsEyTEmI0TTb3arc3tAzNSsCuFw65wJdnolyI6TehZtNuDorU+qJSyAw9qrb9u+uioIEqpizGNnnk6uByqta7TWpcGX28F9gDHtCDXaq31FK31lNzcDq6s9SJUsFfEOQX5zJ86nAvu3cT3l7/NJQ++z/ypwxmdl8Z5qzcBYFOKHLeTpCQJaelKOmPMxjPh27pxf+bqU3jnf07lyStPJsmu+M0/Pmpoinjq7W+z+LmPWTB1OClJNu55aw9aaxZMHc7azV+z74iX8+/dxLmrN7HkhZ2kJTvITQv30lkeEGvyDP0O2/aXc8mD77Pgvs2Uenw8sOErVi0oiLu3wwoPG5LlJjddDPWWSAg9W7Ef3P3BntSx860QL4/0QukLJMSYjROhXu225nYFuJLMhcdMt+g0oefRoRwUrXVAKWUopTK01hWxCqGUOhP4H+B7WuuakO25wJHg5x0FjAa+jPXzejtWSM8jl52Iw2bj/Hs3NSvj+sDFJ/Dul6XYbQqHTeF0xlIvQUgUOiPhu7TaNBgWzxzLkhd2snjm2GbVta5a8wFrrziZK793FEoplr78KbMLhkY8bsms8Vzy4PsN18/PcpHitFFcVdcgZ25actj3sHqa/PL0MYzOTRNvR1+morDj4V0Ayf3MjvLiQRF6GNmpTh685AT2H/FydF5aq3O7oTW+gMbhiKVYqyB0H7HclVYDHymlXscsOQyA1vra1k5SSq0FpgM5SqlC4HeYVbuSgdeVUgCbtNaLgO8Cf1RK1QMGsEhrfSQGmXs9TcNj7jh3YouNmpbNmYDdpkhyyM1dbyHeCd+h3ozBGSksnjmW0XlpEcfU4cpa5qzc2NCvxO2MXEJ4RI67QUYzyXMyhyrqWPhIY8WvZXMmcNsru9i2v7zhO4T2NOlIMrzQS6gohLSBHT/fZpdmjUKPxDA0dX6DtZu/5uYfjW1xbr/7gskk2W3kpYmeFHousZjWzwCLgXeArSGPVtFan6+1HqS1TtJa52ut79NaH621Hqq1nhh8LAoeu05rPS64bbLW+p8xyNsnsG4oc9OSuX7GGPyGjpzobLfhdpo3rVkuUWK9hXgnfFsemUlDMzG0Wd53d1F1xDFlhZFZ/Uqy3M6Ix1XV+lkyazxvXvc9bp97PKnJSQ3GiXX+DU/v4NrTRod9h9Z6msSrMICQ4Ggd9KDkxHadlEyzipcg9BAMQ3Ooqpa/vvE5F50yssW53emw8dKOAzjtNvGeCD2amMoMK6VcmEnsu+Iok9BBQku/WmE4pxyVzYp5k7k6pIzrPfMLqPTWk5rskMT4Xka8E74tj8yi6aO45rEPmtXYD+3wfvurjWqgsMxLcpKNZXMmNCTFh5YQPlhRy/VPfsi2/eU8vWhqxJXAUbmpvH3DdBw2RV4r1WekE3wfouYI+GtjC/EC8aAIPQrD0ByqrCVg6IbQ2Zbm9jc+OchPJuWTI94ToYfTYQNFKXUWcDvgxCwFPBH4o9b6J/ESTogew9DsLfWQZLeRn+UiPyOZv84cTJ5bYQTKWHv5idQbkOywYbeBoVVUXbaFnkfUIVCGATXF4PeBwwnuXLDZwhqBuZx27l0wBY/P32BEhNbYP3ZgOvUBzW2vfNoQjgXmSp7Pb3DbK+Zxma4kyr313PbKLpafczwr1+9h0fRRZLqSyHAlRQxL21Ps4ZIH328oLzwoM5lan9HM6GqroaTQi6iMscSwhSsLSr+IXR5B6GSsuT0txY7WimFZKRHn9iS7jZQkG98/biCDM1wytws9nlj8f78HTgTKAbTW24Gj4iCT0AEqa30kO2zU+QM8eHEBo9V+Cl6fy9CHTmT4sz/BXf45S1/eCUBNnSHGSV/HMKBoJ/z9B/CX8eZz0U6MQIBdh6s4e8UGpi19i5/ctYEkh6J/ani4llVj35Vkx6bg66/3+AAAIABJREFUkmkjw8LK7r5gMlqbDRsXPrKVc1dvYuEjWxtKBF8/YwxLXtjJuas3sezVz7hnfnhlrmVzJjT0Oyks87Lo0a3s2F/JtKVvcfaKDew6XNUQxpXoneAl/CyOHAnWSEmPIQcFzBLFVYfM/wNBSGAqa31kuB0UV/pw2uEYIs/t9QEDrSE/yy2hXUKvIJZRXB+hgpdo+27AMDQHymoprqrjtlc+I99Zg/OpeVC+zzygfB/Z/7yI/zczH3eyjRE5qWKc9HVqiuHx88PGCI+fT6C6uJk34uIH3udwZW2zZozL5kygus7PsCw3xw1KZ83lJ/H0oqksnjmWu9/aTa3f36zR4j3zJpOWbA+r7vXaziL+9ubnPLlwKhtuPJXHrzw5LEF+0tBMFs8cy/BsN6sWFJCblhxWOrmlhpKJ0AneCj+zDL6mxpXQToo/BxT0GxzbddzZYNRDTUlcxBKEzsAwNJW1furqDYqq6uhnVGB/8oJmc/sdM/NJsitpxCj0KmKp4vWJUuoCwK6UGg1cC7wbH7GE9lDiqWPho1t5/men8Osfj0UFShoVmEX5PpS/jpo6gyy3KLA+j98XeYwEfBG9EUl2G/UBgyWzxuN22hvCtYqr63j26mkAzPv7e2Hn7jxYxd0XTOLxK04moDWOYMU4v1+zbM4EbEpR7q1n5fo9vLaziN/82FzfsNtUg6dl0tBMrp8xJmK+i2EYDaWJH7v8JG55cSev7SxKqE7wEn4WZ0p2QdoAcKTEdh0ryb7yAKTlxS6XIHQCJR5TDx7x1LN289dM/WFuRL2drPwMyXCLcSL0KmIxUP4b+DVQB6wFXgWWxEMooWWa5gf4DY3XF2DFvMkc8dRz8QPv89eZgynIHBauyDKHEbA5cTm7f1VZSAAcTogwRrTdGTEfJC89maKqOs6/971ml7JCqaxqX1ZuSbm3nv6pTgw0Fd76sGTOZXMm8OeXP6O4uo6lsyfw0Ltf8emhKpa8sJMHLp7CPfMmc9WaD1g0fVSzXio3rtvBHedMpMTjCytNvGpBAUtmjcdmsyVMb5REDz/rcZR8Dhn5sV/HbRko38DgSbFfTxDigDW/G4ZBQIM/YGC3Kf65vZCLThnJ7iNlfCuS3rY5sSeAvhOEeNLhEC+tdY3W+tfAacCpWutfa61r4yea0JTQcJGfPbaNr0o8/HTFu3xv2XquXvMBxVV15KYlc8v6YkrPesi8AQXIHEb93DVU2jLJdHX/qrKQALhz4by1YWOE89ZiT8uNWKZ4cIbZh6SlUKoUp42nFk7lj7PGNeSWLHlhJ2U19RwINhBrWkZ40fRRDQbH4pljSXXaWT73eArLahmQkcyDl5zImIHpEW/wB2WkNCtNvPCRrdhstoTqBJ/I4Wc9DsOAkt3xMVBSQwwUQUgArPn97+98waHKOs5ZtZHvLlvPuas38aMJQ3jo3a/47RuHms3tgXMfw5YWY9EIQUhAYqnidQJwP5AefF8BXKq1brMXitAxQsNF/nLuRH7++HZy05K5fc63GJfhw6GrefjcEVz4xJdc/oqH35z+FOMHuMDhpCYpi0GuxLlxE7oZmw3yxsLlb4RV8bLZbC2WKR7YL4V7L5zSrJxvZoqDXUXVVNf5+fu/9zZUmCmq0dz9r8+56YeRG4qNzktj1YIC3tx5mPKaem565qOG666cX0BVbT0DM1IienQMdNg2y3NT4/NTXEXCeFCsvjRN/2aJEH7W46jYZ5YYjoeBkpJhNmwUA0VIEEo9Pu54fRc3/fA4Lrx/c5O5vZzlPxocNrfnuRW5mf1w9svDZpcFD6H3EUuI133A1VrrfwMopb4NPABMiIdgQnN8/gC5acksnjmW3PRk/j97dx4fV1U3fvxzZs1ksjZN0iXdKLS1LBYaAakKshUsUBChLF2ARxYRFRdAeRD9ofIgyCMioqAPS2mBtrJKWctSZBEstJa1dAHadEnSLE0yWWa55/fHmduZJJM2ycxkJun3/XrlNcnNzL0nmTv33u895/s9pXlebj7zQCa46nG11kCgFtY8xAOnXM28p5r5wVPbWPztI8h3uxjml/HuoguHw4zn77Y4cZnirnOsuF0OvE6wAjWUhJuZWJbHYV/Pp/CRs6BxM2OKxvLHsxZT51bce8GXuP3F9Z1mhl9f08KvnvqQhRcdzvx73u7UG3LZonf41eyD0JqEF/g5bufuwCVRnkq2zIOS6nlp9mk1H5vHwjHJr0s5zDCvHgKUmqZ2/vbap6z4qJrqpnaKcz1MGO5n+rhijtyvhGljishxy0WhSB3Lsrhq5hSUgtI8L7d+6yDGOus6ndvvm3UVFyxv4cwHNlJR7GPppWMZJcGJGKKSCVAidnACoLV+TSkVTkGbRA98HidXnzSZq/6+lqUXH8EDc8bjV3Wo+o2w8rfQUgOn3UHeGzfzu2/8ijbPcHwehwzrEv1ij4fWVoRimnDpEA6Xh1J/KRYePq9rwdOyHu8TCxjZuBmKxuKbfadJOm7cDI2b8Sw7n20nLOPnT23jlm8dsjux3k50r2pooz6QODF/YlkeFUVmeFTXC3yIBS6J8lSyKRG91/PSiD3b9q4JLIalqJp9bolJku/ilXU1/ODhNTS3hzikoohJ5fnsaguxub6V19bv5DbW43E5OGxsEUfuV8IXRhZQ6HOTn+Miz+uiIMdNsfSQiT6wLM3OQJDbV6zjppNGseScCtyqHvXcdbBuuRnSddodFL55C9cdcz0/eCrEXXOnUyaTMYohLJkAZaVS6i5MgrwG5gCvKKUOA9Bav5uC9ok44Yjmqr+vpTTPTVl4K672OjNhWdF4+OZf4Z+3wpNXwMwbGVfoJuDzku+VYV2iD6KTN+pwkKDyEmwKMMIXwdmwKRYEn/MQu/L3x2qpoeiJBZ1LFT9xOcy8EZbM3b2sLFftzjt56OIj+XB7E797LlZGuC4QTDiMy+d27t53E13g2z0TrXGTSNokEX0I2voOFI0Dt2/vz+2N3BLYVdVp0auf1PLt+1cxutjH9adMZVRR520FOsJ8vKOZD7c38fGOJv6wYj2JCkaPLvJx7uFj+PZX95OeFrFXddHg5MYZLopfvBqOvAwKRsPMX8OM78Pz/7373H5IeS7LLv0ypXleme9EDGnJBChfjD7+osvyQzEBy7FJrFt0EQ5bhCIW9134JcZ423C1VEEkDA+cYS4Mi8bC2Quh9mPwlxJxuCn0JVmKU+xb7MkbHz4XlVdGznG/ZPQTl8f2r9PugJdugIfPxX/B8+QVuhKWvMRXHPu5aCw1reYSrqqhjY5whF899WGngOKRd7bwl7nTuWzRO5Tmefn+cQcwfngu4YhFOGz1eBK2eyZqm0kY4HRNRI+vgLenoVa9fZ4YQFqbAKXiS6lbZ145bP4XWBFwONna2Mb3HlrN6CIfP581Fb+3++nR73UxfVwx08eZfTzQEaamuYNAR5i2YIS2UISm9hDvb93F757/hOXvbeeB/zqC4XKnW/QgHLYIhiPcdmoFvmd/Al/9MYRa4YHTY8fe0/8CK64353blplwmWhb7gH4HKFrrr6eyISIxy9I0tHWwo9HMdfKV/Yr4nxOGmyTPZV3uXi+dD7NuJewvo9VdjJwSRXyPSFi5aaAA5ehy0W1Z6EAtRDpQ9uSNM280vSHx+1f0Dh5L5rJzVzPbW6yE5awJte7+vu7U+/n1s7WACRpcDgcLLzqcm575iNrm4O5gpK4lyJ/OOxSHUnwnrhzxX+ZOZ3JZHo3t4R4Dht4kotsVcro+p2ueSm+fJwZY/SZoa4Dhk1K3zvwRZrLGpq3owjH87JG1BMMRrjz+wITBSSJ+r4sJCZ57yiGjePfzBm5/aT0L7nmbRy8/Cq9UbhNdhEIRPqlt4dIH3uHJBRPxHXc9ON3w9ws6H3sfvwxm3YrOKzdJ8XIsEvuAjPQPKqXuUUrVKKXej1s2TCn1glJqffSxOLpcKaVuV0ptUEqttYeQ7Qvsi6W1W5p2Byc3HuVA3TcLmrcnvHuth02kRpVR6JPwZJ9n94j87XjUbQfhvvcEnDs/5rrH/hObzdyy0DUfov7veFQ0bwQwvSA99Y4UjWV7i5WwnHXzGQv5LGcKHd9bS/PcZ7n29TCrtzTtnvvkh0vWMP+et/nJzMnccPpB/PyJ9/n671Zy5ZI15Oe4dwcnEEuW39bUvseZ2OMT0V+/5us8dvmMbgFFTxMm2rPR9/V5YoBtjRaHHD45devMH2Ee6z/l+Q+reXX9Ts6qHMOIwtT0PB82rpgrjt2fD7Y18fsX1qdknWLosCzNtqZ2Ln3AnNuLdSM8eNYez+26YIxU7BL7jEwNYLwPOKnLsp8CL2qtDwBejP4McDJwQPTrEuDPA9TGjLMvlnI9Tqoa2rju6GE4ls0zB6+2htiFoa1oLGGXnxFFfrnDIqC1FuweEYDGzZT8YwGXTC+IXXS31sZ6TeL3qR72L0KtplfklVpWb2ni288GeOeEZbRfsZaNsx+nrWgyTv9wjvvbRuYv+YxvTh/Lih99jV/NPoibnzV5J1UNbWxtaOc7izrPY9JTsnxtc8deAwZ7uNfo4tyE86D0dsJEmVgxS219x8we33WfTEb+SACs+s+45bl1jC7yceLUEalbP1A5bhjHTCrlb//cxOd1gZSuWwxuOwMdu49t1x09DLVk7h7P7RGXH4crmVH5QgwuGQlQtNavAvVdFs8G7o9+fz9wetzyhdr4F1CklBo5MC3NLPtiye9WfPDjg/C7LDPEpqISXr/N5ATE3b225jyIO1+6f0VUOJjwTpydtB4MRzo/J36fev02mH1np/1Lz1mMLj+YgsJi/mfmSA4dU8DqLU384KltvLnTRyS3lM/r26huaqeqoY3VWxq59IF3qGsJcuF9/96dFA/sDrrj2cny8SqKfd2Ckf4EDL2dMFEmVsxSVaugZH8zd0mq5A4Hh4tNn7zHhpoWzjxsdFpm457zpTE4HIrbVkgvijAsSxMJR5hUEOSTq/Z+bpfJGMW+KKlwXCl1FDA+fj1a64X9XF251np79PsdgD1Bw2hgS9zzqqLLtjNURfMGyh0Wn/zsMNytW1CL53ZPVn7pBjMuteQAGkIuVF4pxQ6p6iGiXB6zv3TJEalp1cycWkq5KwARDd//jxmL394IHS3wzbsJ+spx5OTjuvBZiATRDheqrR7uORF342amFI3lvtMXcvWrXq44dhJOBeO8Aerbm8nz+1lx2UF4dTs1rZqQtrolsbsU/OPCSRR5LGpaNb9+pZZH3tnCn847jO8+2DkH5fYXP9k9EWORz01rMILP07cL1d5OmCgTK2ahcBB2rIUpp6R2vQ4n5JVT/flHjCg4iSMmlKR2/VFFuR6On1LGP/6zjWtOmpKyIWRi8Gpp76Bc1+Job4Sl8/Z4bt+0y6LQP4rhMrRL7GOSmUn+AWAisAawb2dqoL8Bym5aa62USlS9cU/tuQQzBIyxY1M4DGCg2TkBL9+I6+irwZsHoTZzd+X128ydRDtZ+blrsfxl1DnL2NkRYbLknQwqad9nc0vhnIdiw7yiSesrPg7zp5OKce38GP71ZzjiUrNPRZ9jnbcM5fbhDDZBdI4ddfQ1sPzHnYaLFT4+n9sveJ6A00tRywYcz/wPI6edC6qU/Lzh8PrtjPn0VZrPWMh9F0zngvvMkK6ZU0v5oncbrifOg7wyxhx9DX+fsx/tysd9q6v5+SlTmTIiH5dDUer38NOTv0B1UztX/b3zRIz2/D57qrgVX5GrvMDLo5cfRShs9VidSyZW3LOMHGer34dIMLUJ8lFN3nIKGquYOX1kWt/jEw8cwTPv7+DBtzfzoxNS/3eInmXdtYFlkR/aiXI6IVADp//ZDOt6/bYu5/ZybnkzwKxpYxiXK+d2se9JpgelEpiqte5TILEH1UqpkVrr7dEhXDXR5VuB+KmDK6LLOtFa3w3cDVBZWZmqNg04HYjmBJzzMFjBzmWE7bsrVaug/ED0Bcu55fUm5s9wMLk8Vy6iBpm077MOB5RNhW+v2F3FK0IBP/pKE66atSbgmHljLDgByCvDEajF8eBZnfc7T27C4WIeq51wqAbH2qXdAh3O/zscfQ35bY3kept45LIj6Qhryp1NuO470UzoeOz18OQVOBs34y8ayzmn3s+1r+/im9PH8qunPuSBi75EhbsFn6uZP5wyanfuy8ULV/Ho5UdR1xLsseJWfytyycSKPcvIcXbbavOYhgDlo45SvqA+5msHpKf3xFZekMMhYwpZtmoLVx53gByrB1C2XRvoQC0KIFAfu+kTX0q4/EA4/xGCniLmHpVHeX6OzHci9knJ7PXvA6nMKHwSWBD9fgHwRNzy+dFqXkcCu+KGgg05VrgDJnzNXBAund+9zOvXrzMHM20RCYc57bCxUhNdJBYdKkg4iHJ5cOeXUVaYi1uHwB0NOAorTJBywXKYswhOvgXCbeau3pxFJoh48grIKUqYuKmq38e/+BQ46Ax46y7z/DmLzJw8wQDcNwvu+irO+75BaetG3E5wRKJ5LzOu7BwcRZP47zx1JNMKmvnbtyYwvHUDnmd+zMjWdUzPb2TpnNGcM30kVQ1ttIesPVbckopcQ8SO98CbbyalTaGOiObVXaUUqFYKaEnpuhP56v6lbN/VztufdU2/FPsSy7LMWJOlc7uXEj7pt+BwQaAGR7iNkYU+CU7EPiuZPX848KFS6jml1JP2V29eqJR6CHgTmKyUqlJK/RdwE3CCUmo9cHz0Z4CngU3ABuCvwOVJtDlrWZamtrmDiHLD164yXb6JyrwWjTN3pgO1OFwemZ9BJBZXYpjbDjKPNR+a5S6Pmatk8iwzAd5z15pA4rlrQTlg1X2xn4+93gQdbr8JOuISNzlrIXzyrNkvly0wsx8fe7153a6qbvP0OJacR2vDDtbuaDOv76GUsbN5K+X/V8kUaz35b9xiemaeuxbumYl70Wx+dZSTmVNLcSr2WHFLKnINETvWQvEEUKk9zr28OcwnoTIAvM2b9/Ls5E0fV0yO28ETa7alfVsi+1iWpj7QgUMp0OHE5/fcEnDmQKgVt0duPIp9WzJDvH7Z3xdqrc/t4VfHJXiuBr7b320NBvZQlN+/sI4bzziIknA1yp2bMMEZhxOcbrQGR/4IkAOYSCRBiWEePhe+vcLkphTvByfeEBtCaD9n6TzTo7JueazXbtatEGyBlbeY3/mKTQD96i0w7VxYvcg8t2B0bPbjHoKP0flOfvL0Nu47fSGFodrE+3jATOyIO9esv0svi3vZXH4//zlCXsfuRPtAxAnKSaErxHBHE1g5uyty7W2GeZHFrAhUfwCTZqZ81Y9+EqLRbWqx5DRvJjD8iynfRrwct5MvVhSx4qNqbtQHoVIccInsZZ/jW9rDVJY6wHL0fH6PtBMpmoAzxT2GQgw2/e5B0VqvTPSVysbtK+oCQX7/wjou+dpEQqGQuYvtdMPcR81dbojlAgDaCtNWMA6cUhNd9KCHEsOEgyY3pXi82c96mowx/udhE6Fjlwlalsw1vStL5pqf7ecWjUXHr6+HWv4ehxkC/lGkgg3uyUTOXtS5V+a0O0yyqL0Of2nCNvocEQpaqzjY8SljVC1T9Cam5DQw8oXv4L73BKj5kJJcF3+dX7m7bLBU5BqE6jZCuB2G7ZfS1Ta2a17aHGbCSNODktOS/h4UgEPHFlPb3MEH25oGZHsiO9QFgjz+7hbGDfOYoa9WJPH5XSm0FUEXTTDHaSH2YclU8ToS+CPwBcADOIGA1rogRW3bZwTDES752kQ+qqpn+sRW1JLzY4lzZ90PR19lhsy8dRcc/0vw+MnxSBKv2IMeSgzj8kRzU3aCcsFFz5keC7tCXNFYExjEv8bpAd+wxOuzA5HT7kA1bY095/XbzL67bIEZInb0NVA8AUfTVpacN5GgFWLew2Zd152wjLJcRbHfS96L15p2gFnH6Xcl3K5yuqHhs85JprPvhON+AfefCg+fi+PbK5hcXiYVuQazHWvNY/GElK72pc0hwhZ8eYyXUEMxOc2fp3T9PZk2pggFvPRxDQeNLhyQbYrMsyyLC78yntLWjV3O7ws7n99PvAHtzsPllpuPQiQTot8BnAusB3zAt4E/paJR+5ocj4Op5TnMneqMHbwgNrZ/V5UZg3/0VWinB+0vl4sssWd2ieH43olzHgJfCdRvgp3r4f5ZcM/MWK7J5FnmhLnmodhr5iwyQUj7rgQ5KPfDyGnwzb/Ce0tNcue8x+G7/4Zv/M70fpx5j/l++Y/hjkp4/Dt42qrJe/4n3PsNPwBnPrCRr961gR8/W0Pwaz+NbaOlhpawg/BZi7r/HZEwPHF558/KE5dDwajYz+HgXmeYF1lu53rT01c4Zu/P7YPnPwszPAf2L4Sgr2zAApRCn5uJZXm8+FH1gGxPZAelFGWqKcH5fT60NUbP71ejlTvlxSCEGKySCtO11huUUk6tdQS4Vym1GvhZapo29FmWprEtiIsIvo56VEt14iE35QeZKkuuHHC4cLjk7orYC4cDSqfAhc9AJGSGDOaNgLY6aNjUbU4TnrwCzlsGqxfDzN/ASTeaBPp/32MqdPmKzIn0vGVmToqcAsBhel/yyuGwBfDIf3UuTbzyFjjhBljUNc9lPsy8kaInFnDDyY9y6r1NVBT7uOLr+7M9vIsx858krB1sarT42SNVANxw8qN8odSL0+1F+Uth1+bEnxUrmgBv9xaJwa1uvdm/nO6UrbI9rFm5JcyxFSaFL5hbjr/h45Stf28OHVPE39+pora5Q8pZ7wPCYQuvM4LqaE98zCoeb87vzhxwOHDIhIxCAMkFKK1KKQ+wRil1M2Zmdxk02UuWpdne2Eq5px2nskxddLuUa9dhNFYEOprQbj8qxWOxxRBlWVD7cadJGjnnIcgpjJUYjte4GVxe+NJFgIbmHfD6H+GrPza/Xzi7c71+Kxyr0nX+MlP5Kz6B/q27TIJ7687E2xo+CU7/MwcVedn039MhEkZFGlD1G+HR3+JpqWHE7PsBqG0J0eIq5oOAk/wcN+P9CofLl/iz0rQt9rfmyp3IQW/n+livWIq8VhWmLQxfjhbJD/rKKd72Ko5wO5Yr/bO8Hzq2mGXvVPHKuhrOqkxtz5DILpalaQ+2U0AbyuHqOTE+1I4OtaNK9s9cY4XIMskEFPOir78CCGAmUzwzFY3aFwQ6OhgR2YYr0oYKd5g70S/eYO48xw9nmbPI3K3OG2GCE0mcE73RUxUvpUyJ4QQJ7NSth9unmWDElWPKBrfVdSsXzOOXdQ48/KWxUsB2eeIjLjVzrARqE2+r8XO4bxbqmZ/iaNqO476TUX88zPTsREsbFz2xgAfOmcjPT5nKzc+u47sPrubzulYzj4k/wRC2OYtNr9GCfyQeJmFZ0FINjVvMo2Wl7v8tUk9rqN+Y8gDl+c/C5LrgoOjcjEGfqeTlHaBE+fEluRT53Ly2YeeAbE9kTluwA78OoMLtphLi7Ds7H7POuBscbvDkovzD5fwuRJx+96BorT9XSvmAkVrr/5fCNg15lqXxhgM43T5QTjMEx74IDFSbO9H+UnNidvvNxHn5o+TgJXqvpypeTg8URhPK7RwOO8H8xV/Gnrd0vsknad6eeD3uXPN9RaXpNUk0qeh5y0w+y5xFpuqXva2zHzAB0JxF4Mkz47C7vnbmjbBkLg1NzVz6wIbdm871OM08Jg4vlE01ZZPDQTMEyAqbwCdQa7b79WvNcxyO2LwwXXuU7N+L7NO8w1Q8KhidslVaWrPi8zCVZeCOvu2h3Gglr+bNtBWlfrb6rpRSfGFUAW9srENrLeWGhyjL0vgiAVSo3RybHjzbFAyxe5pDrVA0xlwD6IgZQSGE2C2ZKl6nAr/DVPCaoJSaBtygtT4tVY0biixLs6u1naKWrfDKTaaCR7gjdoFWtcpczAH84D9ghUzugFxEib7oqYpXJGh66o68DBY8ZQIQ/3B47NJY9Swwr4vvbem6nlCrCU6OvR5a6xMHMaFWOO7npkzsvMfMUMWmbbDyZjP867lrTanNnkodF40lYHl4ZN5EynIVjUEHQUVsHhOHw+QnWBZUvw/x1XFOuwNevhFO/b15zp7mhckrT93/XaROXTQwLahI2Srf32lR3675Utxbbveg5DR/lrLt7M2BIwt4c2Mdm3YGmFiaN2DbFQOntaMDf9NW84MnOqy2cXPs/A7mHO9wyTleiASS+UT8EjgcaATQWq8BUlsLcoixLM3ndS0URepQa5eZRORwh7lzYtdDtxWNRTtcJjlZ5jsRfdVTFS+tzfwl959qckXumQk1H0FLTefXF401lbvKpnav3jVnkVl+4q9Nb4ddXrjr63MKzP69dD78cTos/papyBSoNjkoeWXQ8Gni14ZaCZ/3KGNyO5ju2cwYVcvBjk+Z5q+nJLfL56G1NhacgFlvpAOO/4XpXbGsPc8LI7LT7gAldUO8Xt0SBuCwuBGAEXceEVcuOQMwm7ztwFGmxPAbG+sGbJti4FiWxh+sR71yk5kd3rJMSfc5i8yNHZBzvBB7kcynIqS13tWle1on2Z4hrbGtgzF6O0rlw8FnmovE+GEvYC4eo+PplbfADIURoq8cjs5DoFweU2K4ZYc5UQL4S8y+9vptpsfBnrG9aKwJSty5cO/JcOg8U2UmEjI5Act/ZAKaOYtMMJDo9fbY6qXzug/fmnWrGYp17PWmPHGXIWD67EWEfKXgUPjrN3Sa68Q5+07wF2P5SqgLBAmGI4ygA6e9DbtXJ74tcxabdk6eZT5fNqn0ld3qNoDTa3r4UmTlljATC6EovniWUgR95QM2WSNAeYGXEr+Hf22sY96R4wZsu2JgtHZ04NcROPY6k68XP8T1tDtMEZFjrgFXrpzjhehBMgHKB0qp8wCnUuoA4PvAG6lp1tBUEG7AFWkDK7f7hdvSeTD3MZjxfSgYjXbnojzS9S+SYA+BgsQ5GGfcDd+6D/5+AbyFI99NAAAgAElEQVR0gwkciieYeXc6mk3PR14Z7H+8qQgWX5oYzEl31q2w+Czz+vjcqb9fCCfd1ENZzQkm0b6lxrw+dxic/wja5SViRXA9fx2edcvhe+8mnOtEX/A065qauXjhKqoa2vjHhZM42B6GNuPKWHBiv2bJ+WY7R19tltk3AaTSV3ar22D2JZWaoS/NQc3q6ghnTOz+u2BuGd6mz1Kynd5QSjF1ZAFvbqrDsrTMzzPE+EMNKOU0OX8Pzel+k+b8R9DefCLevOTmehBiCEvmyP894ECgA3gIaAKuTEWjhhrL0jQE2nE6FPjLTF5Jogs3LIiE0E43ylcsY1JF6rTWmpyMmTeaXpXzloHHb4ZhzX8SZv3eJM+3N5pqM5682AX/sgU9lyYunmAu9qtWmZyScIcJbgA8+QmHNbCryjy/cTMM2x92boBADVopXM9fF+vlsMI9znViBycA16/YQePs+826fcU9J/UvnQffuBmufN/8DyRBPrvt/CSlw7ve2BomrGF6gpg06CvHG9hm9rkBcuDoAuoDQT6paR6wbYr0s8Ih840O91xmXSlQCpf04ArRo2SqeLUC/x39EgnYEzE2BjoYm9OKinSYJGVnDwnMrhzIK0f5hsmFk0gtyzKlf+OHPn3rPuhoMgmaOQXw7LUmOJg8C06+qfMFf1tD4n12V5XpnSgaZy4o37oLZnzPDLN68KzuwxqOuNT0toDZjtMFbh8EanG8eaf5faDaBDA9bDPk8O4OTgBWb2niwqfh4Quex0s4cTvbGswyrU3lHJHdItHgdPT0lK3y1aowPhdMGdb9dyFfGQ4dxtNaTTAvdVXD9mTqSJOH8ubGOqaMKBiQbYr0sc/3xarVlBVGx8qsdz0eOT3mPC+E6FG/r4KVUpVKqUeVUu8qpdbaX6ls3GBmWZp11c18Ut1CmTeEK1AD958Cd1RC87bu9dBn34l2uKFYxsWLNNCRzkOf8spMla3HvwN3TDf5UEdcCofONY/P/NQEFXYVLzvPJH6fPe0OePnXZohXoNb0oHz1R2ZYWddhVk9eYYKe9StMr8yl/zRjsO+bZRL17blT3rrL/L6i0uSwnHF3l0T/Bwm4i6ko9nX682pbQnR4ik254TmLurfz9dvM90rJ/CeDwa7NpjcjPzU9KFqb2eMPKYmVF44X9JlulZyWLSnZXm+U5nspy/fypiTKD3r2+b61vcMU6GhrMEMU1zyUYG6zxSYfUM7zQuxRMsMfFwNXAe8BKTnjK6UmA0viFu0HXA8UARcDtdHl12qtn07FNtOlLhDk4oWrePLyI/A72kzvyNxHTanVj5404/pn3WqGnoRa0Xnl6NxilCTMiVSxLDO0Kxw0AUr8XbwZV3bP73jyCpMH1bAJvny5SYrPHx1LYo/PU2mLDgU7/v/FgphZt5o5K3qaO8WKwMHfgqVzzVCz565NPP+JvxSO+4WZJNJXbIajhdtMEJQ/kkKfl7/Or9w9zKui2MfSiw8nv/FjM4wrrwxO/zPkjzRJ/S/dYPJdTrsDnr668/woIjvVbzKPBSNTsrrPmiyqmjWnjk/8+1A0QPG2bAGOTMk2e+PAUQX8a1MdEUubIcBiULLP9//8wXRUe9gMkfUPN9MIrLwldlzLK0fnFKI8uZlushBZL5kApVZr/WTKWgJordcB0wCUUk5gK/AYcCHwe63171K5vXQKhiN8Zb8iil0hVEerSfS0wrB6MRz0TdjwgnkM7ITSL0BOAQ4JTkSqdE2KP39Z56EGiXI18srM8ENvQWyyw6OvBpc3FkyDqdsfbod/fD82hOv0v8DK35oeD3+pGb417VyznbYGs65IyAQn9jwniYKYonEmaT7cYYKLZ38aCy5W/ha+dR8Ov2JyeT6PXT6DYDiCx+VkuFWLsgtPNG42PTOTZ8HJv4UTf2P+npduMEPHqt+T+U+yXf2n5jFFPSgrt0SAxPknAKGc4WgU3kBVSrbXW1NHFfLyulo+3NbEwRWFA7ptkTrBcIRfnXIAtDaYYdxKQajdTCEw89fQWge+YeDKQWlLbo4I0QvJBCi/UEr9DXgRkygPgNb60aRbZRwHbIzOWJ+iVQ4cn8fJ/8waj2r4LFaxq2gsnLUQ3n8UDptn7hLmlYM7B+XJz3STxVASqOk8MeHK33aePb7rBIwVlXDcL+H+WZ3zRlbeDJUXmGFctguWd+99efwyE8TUb4Tc6J1De3Z5u2yxtmKvSZRfMnkWoE1p4/g2vHRDrDxxdFiEw6EozY+rFVufoPDEuuVwwg1mCFk8mf8k+9V/anqdfcUpWd2rW0KM8sNIf+Lfa4eLUE4JOS0DG6AcOMrknry+cacEKIOYz+PkmHE5qKaq7sc9l88c73KHm5w7yT0RoleSCeMvxPR2nAScGv06JRWNijoHUx3MdkU0z+UepVRqzlppYlkavzMMwUD3csLL5sOh55sDVfnBpnKSt1DuqIjUiUT3vfgL9qpV8OIvTXDxvXfNvnf2A7Gx0Udfk3jI17RzYz0n8RL1fpTsb/JGfMNiJ2n7d0vnm+/t7SXKaZn5m9h8AfFtmHFltOLXxJ7LAjvdiSd8tMKJl8v47+xWv8kM0UvBzamOiObNbZFOkzMmEvKV4m0euBwUgOJcDxXFPl7fsHNAtytSx7I0ygqbYaiJjnueXHPs8g0Db5Gc64XopWQ+KV/SWldqrRdorS+Mfl2UikYppTzAacCy6KI/AxMxAdF24NYEr7lEKbVKKbWqtra2668HVHN7B55IhzloJbqQc7jMhVy4Azw+uVjaR6Vtn23ZYXoyul6Yt9QADnPRd8+J8PSPzdjoC5ab4CLRvuovNb0ttopKUyo70UV/9Qdm2FfXfBd7XeF2OOv+WFni9SvMtr+/Bi58BhzOxK/zFUcr33g7n9wtC1qqoXGLCaLiAy679+XNP3YPhGT+k34bsONs/UYToKTAOzsitIXZa4ASzCmN5qAMrINGFfLvz+rpCEcGfNv7gnTvs3WBIEXOIMrq4bhnRcyxSwEud8q3L8RQlcwQrzeUUlO11h+mrDUxJwPvaq2rAexHAKXUX4Gnur5Aa303cDdAZWVlZma0tyx0eyMFkSCqvcksu+g5M/799dvMRVn07q325KE8PplFdh+Wtn02EjJDurrO7j5nEbhzoL09lquxZK55zffeSVwOM6/cjKMuGmtyVI77Jaz4pSlR3Fa3u8gDvhJ49mrz+ubtideVUwDPXWeCoqJxgDa5Inb75j+Z+HWhVjM8zR33WUk08eSC5SbQiYTMTYC37oZJJ5k7mOctM2PDc4eZvIa93cWMLzDg8piARu58Dsxx1opAw2cw4uCUrG7lljAuBYfsZUL6kG84nu3/REU60E7vnp+cQgeOLuDZD3bw7ueNfHliyYBtd1+R7n3W74qYnlqn2+T6uXPNkK7XbzM3hRwudE6BTLwsRB8lE6AcCaxRSn2KyUFRgNZaH5KCdp1L3PAupdRIrfX26I9nAO+nYBupZVlQ8wEqf6SZa6GlOjZkJn4eiKOvRucUo11uqdglUif+gtrhMidGe3Z3X7G5yLeDja75J2CC6PgclaKxprejowmW/8jkfwyfYnJU8spMb4g9s7ydJG/TVvd1zb7TfCbWLTfznJz2p9g8KWAen/tvU4JzyfmdgyoAt7/z2O3W2s45No2bTdu+vQIKK8wwt4PP7Jz/dfYDZs6X3gQnXYOfcx6Syl8DpWmbCSZT1IOyckuYqSXg28vZLugrQ6HxBrbRXjAhJdvujakjC3AoeGPjTglQBhvLwhdqMonvjVs6HxNn32mOld4CU3rYIXPdCNEXyZxtTwIOAE4kln9yarINUkr5gROA+GT7m5VS70XnWfk68MNkt5NygRr4z1Jo2mrGoiYaz3/yTVA4GrCkYpdIHfuC+m/Hw20HwTPXmIvxlhrTQ/L4d8ywLF8J1H4cm+MkftiTw2OGc8261Qy7mhUdRbn8R6bnb/FZZv4ee3b5xy/rniQ/40rzs9Nj8l3s4WMzbzQ/u3Jg3uMmmFGOxEnt/lLTk3LRc+Z1r/wWUKb3pbU2NodJONjDMLJo8ntbXff8r6XzzPK9SRT8PHyuWS7Szy4xnIIKXjsCFh/XW1T2YkRfrNTwwCbK53pcTCzL47X1kocy6ARqUGiwQvDYJZ2PGU9cDu5cdLgD5ZPAU4i+SmYm+c9T2ZC49QaAki7L5qVjWykVbjfVjp7/uSkr2NMYfOWUg5VIra4X1OuWm8cLno4NPcgbYS7O7ecFqmO1+QtGmaDl6KtMXpSvGArHwCMXmeDEZs+K3FOJYDtXJLckFhzZJs8yPYt2aeKuZY8hltS+8LTOy6vfi82bYvdkuDyJX2/nc+0tgNmTZF4rkpfCOVBe3RIGYHrZ3p8b9JknZSoP5ck122hqD1GQI3kKg0ao3ZRh7ynvzgqj8sql51WIfpBPTSpYFuREZ7E+/hdmiEuiJGKn10zeJAcrkUqJLqjtIGXYBDPkyeky+6ndqzHjSjNG+p6ZJnA4+qrY3CfBVti5LppUH1VRaQKQeY+b3pjJszpvr2isGZIz91FY/2L32dxPvAGWLehe9rhrUntbQ8/BT3xPRm6pCVaKxpq2nb/MtE1j/k47gOnaxt4UpEjmtSJ59ZtML1xu8jdyXtkSZngOjOtFFfewtxhLuQZ0NnnbQaMKiGjNW5vqB3zbop8syxyXrJD5OeE53yPHDSH6KZkcFAHmIBVsgbZ6M8Y+UAub3zb1z+Proc9ZDPnl5m6LEKm0t94EMPtpoDY2e3t8XtSOtWafPeq7ptTvwtPM2Gk7yT6vDE74dWwIQ9FYODuaG7JueWxdj11igpoFy01G2vwnTbDetBXamxKXPZ73mJmsNKcInvyuCZwS/S1tDeZ7uyfD4TA9KRe/bHIW4vNWznkISqeYx655JL2p3mUHP/15rUje7hLDyd3ICVuaf1aF+fKIXlYrVg5TaniAh3gBHFCej9fl4PUNOzlhqkwgOigEm01BkF3Raodn3R+7CWOf8/1yzBCivyRASUY4BK07zYVf/AXSaXfAe4+aykGtO01ScsFIqdgl0qM3F9StcfsomKAj0mGGIzZtg+kLzHK7B6NxcyzJvvxAWDi7Sz7HXDOE7MuXm9fYs7RXVEbzP7pMVmZXAosPPFpqonfLvSY4qVoVmx8lvvqYPVkjdA68HA4ztCL+77J7Wb69wgQw317R90pcdvDTn9eK5NVvgvwRSa9mTU2E5iC9yj+xhXKGZ6QHxe10MHlEPq/JfCiDQyRsepoDtbHE+MmzYO5jgIaGT80+LL0nQvSbBCj9FQlD7YfQ0WySkLsmxM+80QQnj3/H3EmRvBORLr25oI4fBlZRCcde3zkIOOchcPtMb0fXQAIS/xwJdd73wUz4mGiysgVPmcT9+KpacxaZ3JinfhjLdalaZXp1LnzGDD2zwqa6l12iu2vgtad8EYfDVC7r7/+0v68V/aejF3cHzEx6Va9sDuNU8MU+BChB33Dyd/4n6W33x8GjC1n81ma21LcyZliCyVFFdrCLkviKOhfDWbfc5MvNe9wc12TGeCGSIrcE+8OyTNfu2r+bBOOeJrgrGmuqEUl5UpFu9gV10Rjz2HV/c8blVXz9ulhwArFeB22ZHJTT7jB3A4+93gwJ6ymnSikzH8pFz5p5VL77byg5IPHnQVuw8ubOlb1e+W20Pdd2zkX5+rWmglPRGDNfyqm/hyvfj/WKxP9tPeWLKBWr+CUGj11VEGqDgtFJr+qVLWGmFENeH3LOQ74y3B31OEKBpLffV5XjzAXtCx9W7+WZIqMCtfDKTWa+nkTHOuWA0kkm708I0W9y1dxXkTDUbTDfH/It832iC6S8MjN3Q94IOVCJzLIs09M3+04TeBRWdD6xVlSagMEKmyFf61fAcXE9LE3buie0z77T7Ncur/lMPHAGPPEdc9JO9HlAmzuMS+aayRmXzDU/R4Kx3p9EQcjeAq/4ZHl7W6fdAU9fbe5ySpAyuNR+bB677kN9XU2rxfs7rV5V74oXzFCpYYARhTmMKfZJgJLNLMscs2b+xtwE6TExXoZzC5EsuXLui3DIlGcNtpiDkCcPinNNou/z18cShs9eZMbV+4ql50RkXmstLDojmvj+JzOExh7GlWi411n3m8DDDmJe/H9w0s1mbhR75nh3rklujx+DPfNGeOH67jkkZy8yd8bjtznjStPLaGcvJzMUq2wq/NcLEAyYIKtpm/mc2rkoMlRr8LADlMIxSa3mlWh54co+BiihaKnhnJYq2oonJ9WG/pg+bhj/+M82GgJBiv2Sv5B12urNMS9/JIQCZjhX/UZTlbClxgznluBEiJSQAKW3ImFoqjKzdAPs2tJl1u2FplSrb1i0RKaUExZZws7TaNxs8qJW/jYWRMy4svtwr2ULTCUuO6CoWgXPXm3yS/ylsPMT8/PsO02gYr/WVxybKd6ewb6twfS0/OsvJgdl5c1wxKVd8l8ehLIDk/u8tNR0L1Tx0g0yd8lgU/ux2W9ykpt1+7lPw5T5YL8+riaYY/egDHyiPEDl+GIeX7OVlz6u4czpFRlpg9iDUCu8/xgcfGb3fLpw0JTGzinMdCuFGBLkCrq3WnaYYTJam4u8rjPFL5tv7qxoyxykXDLZlsgS8XkabQ3mYt6u0FU6JfE4aocDzrg79rqWGvDkm/1+yVwTtDhc5oQdv+6iseZ39lCu5641ifdHXGouPI//ZYL8l/OSm6W9a4Uyu1DF0ddIFZ3BpubjpId3tYZMeeEje1teOE7EU4Dl9JLTsnnvT06D/Yb7Geb38NwHOzKyfbEXVgQOPT8WnIB5XDLXBCZay5BuIVJEApReU2b4SKSj811jW+NmGDYRcsvkokhkl/g8jddvMz0f9kzv9nCveEVjzYnWN8wM67pguXn0FYO/PPYcl9f0FNr5Kfa64/NB5iyGEYeYylyRIARqUj9Le0+VvIZNlLlLBhOtTQ9KksO7Vm4J0xGBL/enUrFSdOSOwrdrY1Jt6C+lFIdPGMbL62poCEjvX9Zxus2NmUTHG6dLek+ESCEJ9XvDssyByZNnhm/Zd427Tibn9oHHl7l2CpGIw2F6Si58xgQJOEzg4PGDcnbPoTrtDjOc8dFLOu/jk2fByTfBib8yJYY3vgwVXzJB+7zHzAWmJw8W/MN87/aZWefBVObSuufPTjJBfU8TVXryZJjlYNL4ucnvKxqX1Gqe+zRMgQcO7GeV14680fgaP0mqDck4ZlIpz76/gyfWbOWCGRMy1g7RhWWZm5N2VcOuxxuH2xxThRApIWfvvbEsaG80ibeLz4RHLup81xhiicCSdyKykWWZO9P3ngy3HxorHfzA6XD7F00FrmOugUv/aYZ9vXRD9xKaFZVmmNZ9s+CPh8GDZ5n9fuMrUFBh7iq6vGa9Lp+5yMwfEZ1M0QJvvjl5F47t/tk558HkejoSVfI65yGZxXmw2bbGPJbs3+9VhCKaFzeHOKIcnP08FLfnjcHbVo0z2NTvdiRjXImf/Ur9LHtn4CuJiT0INptj4lt/NTmnnc7/D0CeTOYqRCpJD8qe2BMyefNjY04bN8MzV8GJvzZDX6yIuXOSVy55JyI7tdbGZpkHmHZu4jHUM280jxDLJ7GfkyiZ/skr4MJnzXO7zmJvBwf2Z8jOEfny9+HI75gkfB2J9bIkc2KXmd+Hhm2rTaBbPL7fq3i1KkxzEI4a2f9mdPhNcnpu4yc0l1X2f0VJOHpSKfe+/hnvb93FQaNl2FDGWRa0N8WOm+31cN6y6I0Zj8nPk+pdQqSUnMH3xL6w013uJletgntOis1y7XJLcCKyV9ccDV9xz5OL2rrmk/hLE7/GCnUOfuxJH+2k964J7G/eDvfOhB3/MSd3u5clWXubL0Vkv+1rTHDi7P+x9NFPQhR64LAkOs868kyAkslhXkdNHI7bqXj435lJ1hddtNaaY519HFu9CO48Au6Ybq4DdCSz7RNiCMq6s7hS6jOl1HtKqTVKqVXRZcOUUi8opdZHH4sHpDH2hZ3DlTiR2OEyY+slEVdks66zrdu9I/GKxpohip2qdvnhgqfhoucgp6iHGdude0567ymB3V8qxSREjNZmiNewif1eRVOH5oXPw3x1FLiSOLOFcoYTcebgb1zX/5UkKc/r4iv7l7J0VRU1Te0Za4eICgf3cB3gNgVFhBAplXUBStTXtdbTtNZ2//pPgRe11gcAL0Z/Tj97ptj2Zji765jThWZ4Svys10Jko645GmseMmOmu+ZsOD2dq3Z5C8yEZMMmmGGOZ92fYDZ5d+KTth18dA2O7N/nlUtgL2LqNphcv+EH9HsVz3waIhiBY5OdPkQp2gr2I6/23SRXlJzZ00YRjljc9eqmjLZDYK4Fgq2JrwM8frkGECINBksOymzgmOj39wOvANekbWuRsJnrRFsw/0kz/0lOEZz/iDlQaW0Sgn0lcmAS2S9RjoavpHvOBoDb2z2PI68cGreY3Kv4CRhf/KUpDnHOQ91zUOz12cFR/O/nLDalZPf02bEsM6wiHIwN+Qm1me/zRshcA0PNp6+axxEH93sVj34SosIPk4qSb05r8RRKP30cZ7CJiCe5SSP7q7wgh6/sP5zFb33Od46ZyPA8b0baIQC3P1oAxGNu4DhzTEDt9JhrAyFEymXjWV4DzyulNHCX1vpuoFxrvT36+x1AedcXKaUuAS4BGDs2iYm+ImFzNy9Q23mm+DmLzB3ltgbIK4P8URKciKSkbJ/tDTvQiNf1556WgTkx23On2IrG7j1BvT8J7HZifXxQM/tOExC11Jjen/KDJEjJgLTts5++aoYY5o/q18s3NER4a3uEeZP7PjljIq3FX0BtepT82ndpHH1M8ivsp9Onjea1DTv5/Quf8Jsz+h+87cuS3mfDIVMCO37m+LMfMHOeeKWUuRDpko2frK9orQ8DTga+q5T6WvwvtdYaE8TQZfndWutKrXVlaWkSQ0dadsCuzd1nil8yF+rWmzsmEpyIFEjZPjsQeirlG9/L0lOCel8T2LtWHWuMfh5nXGm+XzrPfE7FgEvLPmtZ8Nk/Te9JP6OL+98P4nbASclNobJba+H+aOWkoPrt1Kywn0YW+Zh54AgWv7WZVZ/VZ7Qtg1XS+2zLju5VD5fOA4dTck+ESKOsu8rWWm+NPtYAjwGHA9VKqZEA0ceatDUgEup5pvjhkyTnROyb4ntCrnzfPKbrs9BTYr2vOPZ9JJT67YrM2L4GWutg5LR+vXxXh+aRT0IcPRqKUjQKSju9tBZNonjL82ZIbwadXTmG0nwv1zyylo6wVIsacFa4hwqGEbkWECKNsurTpZTyK6Xy7e+BE4H3gSeBBdGnLQCeSPnGLQtaqk2lDnu263j2TPFyQBJDhb3PN24xj5a15+cPVCnfnhLr2xpi3ydRilZkmQ8eNXejxxzer5cv/ThIaxhOS/Gk67tGHEVu0yb89R+kdsV9lON2ctGM8WysDXD94x+gMxww7XP2VMVTCJE22Xa1XQ68ppT6D/A2sFxr/SxwE3CCUmo9cHz059Sxx7z/7XhYeVN6ZrsWIpvE7/O3HWQeaz7ce5AyEBINJ5t9p5mbZfeszSMy20aRGlrDB4/BqMNMpbg+ag9r/rY2yMElMDHF8xk2lR+BpVwM3/RYalfcD9PGFHP6tNEsWbWF/3vt00w3Z99hWaBc3aseyjFIiLTLqlsAWutNwBcTLK8DjkvbhuPHvK9eZJZ99arYTPGpmO1aiGySKM/j4XPN0K2eEuUHStfEeru35Jt/kypeQ82nK2FXFRw8p18vv+/9INWtmh92O2skL+LOo7mskrKNy6j64g8yVs3LdlZlBVsbW/nN8o/weZycf0SKEm5Ez1pr4d4T4dB5sOAfppKXcpoEeZmcWYi0kitu6D7mffUiuP2LgDJzQKRqtmshskVPeR72BIuZFj+cLH+E+Ro2AQorJDgZSl6/3eQWjf9Kn1+6q0Nz5+oOvlQGBw9PQ9uAnRNOwxVqYcS6B9KzgT5wKMV3v74/08YW8d+Pvc/vX/iEiCXDvdLKPk6+/Bv4wxfh9kPhD4eYqQeEEGklV93Q85h3melaDFWyz4tM2/oObHwRppzar5yiP7zTQXMQFnwhDW2Las8fT/Pwwxj1wV242namb0O95HU5+dEJk/jaAcP5w4vrOefuN/m8LpDpZg1dcpwUImMkQIE9l1AVYiiSfV5kkmXB09eY3pMpp/T55f/aFube94KcPA4mpHnk1Y5J5+IMtzF2zf+md0O95HI4uOzoiVx29EQ+2NbEcbeu5GePrmVDTUummzb0yHFSiIyRsRLQv8nkhBjMZJ8XmfT23bD13zDjh+DJ7dNLm4OaH7/cxkg/XDQ1Te2LE/SPpm7MiZRtWELNAXNoGZ6GhJc+Ukpx9KRSDh5dyBNrtrJsVRUPvb2Fg0cXctwXyjhyvxKmjSkix+3MdFMHNzlOCpExEqDYEs20LcRQJvu8yIQd78ELP4eKw2HisX16acTS/PClNra3aG75CvgG6AxWu983Kah5m/1f+xFrZ/0Dy923oCpdhvk9XDhjAmccOpo3Ntbxxsad/GHFem5jPW6nYr/SPA4oy2NSeT77l+UxpjiXscNyKcyVBO9ek+OkEBkhAYoQQoiBEdhpZuH25sOMH/Rp5viwpbn6lXZWfB7mOwfDlOI0trMLy+1n24GXMe6dGzngtR+w7ug/Z9U8GEW5Hr5x8Ei+cfBIAh1hPt7RzCfVzWxpaOXtT+t5au32Ts/Pz3ExZpgJVsYNy2X8cD9fGj+MiaV+VB/eEyGESJfsOcIKIYQYujqaYfFZsGsrnPgbU6q1l7a3WPzklTZe3xph3mQ4ZXz6mtmTwLAD2T7lAkZ9fC9TV8xn41E305FXMfAN2Qu/18X0ccVMHxeL4NpDEXY0tVPb1EF1czs1zR3UNnewdksjL35UTShiqoGV5nv5yv7DOXZKGV+bVEqhT3pahIidfzIAACAASURBVBCZIQGKEEKI9KrfBEvnQ/WHcMy1UNa70lt1bRYPfxTiT6s7CFvw/S/CzLF7f126NIw5AcvpZdRH9zDtiWOp3e9MqiedR2DYQX3qDRpoOW4n40v8jC/xd/udpTXVTe18uL2JD7Y1seKjah5bvRWnQ/Gl8cUc/4Vyjp1Sxn6leRlouRBiXyUBihBCiNSJhExvSfsuqF0HG16A1Q+YGbmPux5GTwdAa017GNojmrawmRW+KWh6S9bVW7xbHeaNbRHCFhxRDpccBCOyIPVj16ivERh2IKWfPk7ppsco37CEQPEU6seeRFPpdDryxhL2FmI5vWiHJ6sDFzDzq4ws9DGy0MdxU8qxLM2G2hbe3dzA6s2N/Hr5R/x6+UeMHZbLIRWFTCrPZ79SP8PzvAzze8jPceF2OnA7HLhdCq/LidOR3X+zECL7SYAihBAiNVrr4eYJnZc5PTDxODj8EvDHZlRctzPISYurE65GAeMLnZwxycuJ+3nZryjLqlHlF9NQ+hN2BS8jr+oV8j9fQcV//oCi88SJG858lo6SASg1lmKV/mIqx5shYjVN7bz9WQPvft7AO583dMtn6eoXp07lwhkT9vgcIYTYG6X10JuJVilVC3w+wJsdDmR+Jq/kDPa/ob/t36m1PinVjemLDO2zA2Gw71Ppkuz/Zajts9m0n2RTWyC72pNMWwbDPptN/+veGEztHYxtzfg+uy8bkgFKJiilVmmtKzPdjmQM9r9hsLd/KJL3JDH5v3SWTf+PbGoLZFd7sqkt6TDY/r7B1F5pq+grmW1ICCGEEEIIkTUkQBFCCCGEEEJkDQlQUufuTDcgBQb73zDY2z8UyXuSmPxfOsum/0c2tQWyqz3Z1JZ0GGx/32Bqr7RV9InkoAghhBBCCCGyhvSgCCGEEEIIIbKGBChCCCGEEEKIrCEBihBCCCGEECJrSIAihBBCCCGEyBoSoAghhBBCCCGyhgQoQgghhBBCiKwhAYoQQgghhBAia0iAIoQQQgghhMgaEqAIIYQQQgghsoYEKEIIIYQQQoisIQGKEEIIIYQQImtIgCKEEEIIIYTIGhKgCCGEEEIIIbKGBChCCCGEEEKIrCEBihBCCCGEECJrDMkA5aSTTtKAfMlXb78yTvZZ+erjV8bJPitfffzKONln5auPXyKDhmSAsnPnzkw3QYg+kX1WDDayz4rBRvZZIQaPIRmgCCGEEEIIIQYnCVCEEEIIIYQQWUMCFCGEEEIIIUTWkABFCCGEECKVLCvTLRBiUEtbgKKUGqOUelkp9aFS6gOl1A+iy3+plNqqlFoT/fpG3Gt+ppTaoJRap5SaGbf8pOiyDUqpn6arzSI7WJamtrmDrQ2t1DS3Ux8w39c2d2BZUlgj28S/X+l+jwZyW0LsCyxLU9Pczub6AFsbWqkPyOcqac9fB/8zCqo/zHRLhBi0XGlcdxj4sdb6XaVUPvCOUuqF6O9+r7X+XfyTlVJTgXOAA4FRwAql1KTor/8EnABUAf9WSj2ptZZP/hBkWZp11c1cvHAVVQ1tVBT7uOVbh3Dzs+uobengr/MrmVyej8OhMt1UQeL3K13v0UBuS4h9QU/H2/KCHMaX+OVz1R8dLfDGH833b/0ZTvtjZtsjxCCVth4UrfV2rfW70e+bgY+A0Xt4yWzgYa11h9b6U2ADcHj0a4PWepPWOgg8HH2uGILqAsHdJ0uAqoY2rvr7Wi47ZiJVDW1cvHAVdYFghlspbIner3S9RwO5LSH2BT0dbz+va5XPVX9Vv28enR7Y8GJm2yLEIDYgOShKqfHAocBb0UVXKKXWKqXuUUoVR5eNBrbEvawquqyn5V23cYlSapVSalVtbW2K/wIxUILhyO6Tpa2qoY0in3v398FwJBNNS7mhsM/29H6l4z0ayG2JxIbCPitievpM5XqcQ+ZzNeD7rB2gTD4ZmrZCi3xOhOiPtAcoSqk84BHgSq11E/BnYCIwDdgO3JqK7Wit79ZaV2qtK0tLS1OxSpEBHpeTimJfp2UVxT4a20K7v/e4nJloWsoNhX22p/crHe/RQG5LJDYU9lkR09NnqjUYGTKfqwHfZ+s/Nb0nFYebn3f8J/3bFGIISmuAopRyY4KTxVrrRwG01tVa64jW2gL+ihnCBbAVGBP38orosp6WiyGoxO/hr/Mrd5807THRf3ll4+6cgxK/J8OtFLZE71e63qOB3JYQ+4KejrfjSnLlc9Vfu7ZAXhkMm2h+3r42s+0RYpBKW5K8UkoB/wd8pLX+37jlI7XW26M/ngFE+0N5EnhQKfW/mCT5A4C3AQUcoJSagAlMzgHOS1e7RWY5HIrJ5fk8dvkMguEIbpcDl0Nxx3mH4nE5KfF7JHEzi3R9v9L5Hg3ktoTYF9ifqUcvP4r2kIVTgc/jpMgnn6t+a6wCfyl48yCvPDbkSwjRJ+ms4jUDmAe8p5RaE112LXCuUmoaoIHPgEsBtNYfKKWWAh9iKoB9V2sdAVBKXQE8BziBe7TWH6Sx3SLDHA5Fab6380J/Ztoi9i7h+zUEtiXEvsDhUJTl52S6GUNHoAZK9jffF4yGug2ZbY8Qg1TaAhSt9WuY3o+unt7Da34D/CbB8qf39DohhBBCiIxr32V6TwAKRsGnK0FrUNIjJURfyEzyQgghhBDJsiLQ0QSeaICSPxI6miGwM7PtEmIQkgBFCCGEECJZ7bvMoyeuBwWgflNm2iPEICYBihBCCCFEstobzaM9xCvfDlA2ZqY9QgxiEqAIIYQQQiSrLRqg2D0oeWXmcVdVZtojxCAmAYoQQgghRLLauwQoTjfkFEmAIkQ/pLPMsBB9ZlmaukBQ5rkYwuQ9FiJz5POXRl17UAD8JdC0LTPtEWIQkwBFZA3L0qyrbubihauoamjbPVP45PJ8OYEOEfIeC5E58vlLs645KAC5pdKDIkQ/yBAvkTXqAsHdJ06AqoY2Ll64irpAMMMtE6ki77EQmSOfvzRL2IMyXHpQhOgHCVBE1giGI7tPnLaqhjaC4UiGWiRSTd5jITJHPn9p1t5o8k5c3tiy3OHQsQs6WjLXLiEGIQlQRNbwuJxUFPs6Laso9uFxOTPUIpFq8h4LkTny+UuztsbOvSdgelAAmrYOfHuEGMQkQBFZo8Tv4a/zK3efQO3x0SV+T4ZbJlJF3mMhMkc+f2nW1tA9QMmVAEWI/pAkeZE1HA7F5PJ8Hrt8hlSYGaLkPRYic+Tzl2aJAhS7B2WXBChC9IUEKCKrOByK0nzv3p8oBi15j4XIHPn8pVF7Y+cKXgC5JeZRelCE6BMZ4iWEEEIIkaxEOShON+QUQkt1ZtokxCAlAYoQQgghRLLad3UPUMAEKIHagW+PEIOYBChCCCGEEMmwLOho7j7ECyCnCFokQBGiLyRAEUIIIYRIRscuQEsPihApIgGKEEIIIUQyEs0ib8spkgBFiD5KW4CilBqjlHpZKfWhUuoDpdQPosuHKaVeUEqtjz4WR5crpdTtSqkNSqm1SqnD4ta1IPr89UqpBelqsxBCCCFEn7XvKUAphI4mCHcMbJuEGMTS2YMSBn6stZ4KHAl8Vyk1Ffgp8KLW+gDgxejPACcDB0S/LgH+DCagAX4BHAEcDvzCDmrE4GNZmtrmDrY2tFLb3IFl6Uw3SfSRvIdCDA7yWR1Adg+K19/9d74i8xjYOXDtEWKQS9s8KFrr7cD26PfNSqmPgNHAbOCY6NPuB14BrokuX6i11sC/lFJFSqmR0ee+oLWuB1BKvQCcBDyUrraL9LAszbrqZi5euIqqhrbdsxhPLs+XicIGCXkPhRgc5LM6wPbYg2IHKLVQOHrg2iTEIDYgOShKqfHAocBbQHk0eAHYAZRHvx8NbIl7WVV0WU/LxSBTFwjuPlkCVDW0cfHCVdQFghlumegteQ+FGBzkszrA9piDUmgeJQ9FiF5Le4CilMoDHgGu1Fo3xf8u2luSkj5npdQlSqlVSqlVtbVyEMhGwXBk98nSVtXQRjAcyVCLMmsw7rPyHu7bBuM+u6+Sz6oxYPus3YOSqMywLzoqXQIUIXotrQGKUsqNCU4Wa60fjS6ujg7dIvpYE12+FRgT9/KK6LKelneitb5ba12pta4sLS1N7R8iUsLjclJR7Ou0rKLYh8flzFCLMmsw7rPyHu7bBuM+u6+Sz6oxYPtsWyM43OD0dv+d9KAI0WfprOKlgP8DPtJa/2/cr54E7EpcC4An4pbPj1bzOhLYFR0K9hxwolKqOJocf2J0mRhkSvwe/jq/cvdJ0x4TXeL3ZLhlorfkPRRicJDP6gBrbzS9JypBfo8rxwQuEqAI0WtpS5IHZgDzgPeUUmuiy64FbgKWKqX+C/gcODv6u6eBbwAbgFbgQgCtdb1S6lfAv6PPu8FOmBeDi8OhmFyez2OXzyAYjuBxOSnxeyRhcxCR91CIwUE+qwOsrTFx/gmYoMUns8kL0RfprOL1GtDTkfC4BM/XwHd7WNc9wD2pa50YaJalqQsEd58oRxb65EQ5SDkcitL8BMMYUqzrPiMXV0L0jhxvM6C9ETwJSgzbcgqgtW7g2iPEIJfOHhQhACl3KfpO9hkh+kc+OxnS1tBzDwqAVwIUIfpiQMoMi32TPUlYVWMrO3a1U5pn7rpLuct9V28njpMSqUL0XvznakdTO79/YZ18dgZa2156UCRAEaJPpAdFpEWiu3i/PfMQfvfcOlZvadwny13u6/pyZ1dKpArROz0da2ubg6zeYkrfymdnALTvIQcFwJsPbZI+K0RvSQ+KSItEd8CveWQtlx0zEdg3y13u6/rSKyIlUoXonb0da0E+O2lnWdDelHgOFJu3ADqa+f/svXl8VNX9//88s2Umk2WSkLAYUFREo4IYRJCPVYuKFhQt4gaCuABS66d+XPBjS2ul7a8ufNuPtYhLlU0RccWlLNpiW1yhKLVURGUJsiSErLPfuef3x5mZzCSTZAgJhOQ8H4953Jk79945gXPPPa/z3jC0JUujSQctUDQdQnMr4B6XXae77KYcjFVEp0jVaNKjufsqdq/oe+cwEKwFZMsWFGeO2morikaTFtrFS9PumKZECEFxnivpwVmc56I4z8VrM0fqjEzdgMaZhOw2S8o+kWplV6dI1WjSI2ZtbHxf9fG4WDfrAn3vHA5iVeRbC5IHFYeS3avj26TRHOVoC4qmXUgM0txV5WPhum95aPygJivgvXNdFGZn6IdlFyfmF3/lvHWMfOivXDlvHfUBIy2rSKwv7alREy7dZzTdnZaSS+S57MyfVJp0X82fVEpRVgbH5GXqe+dw4I8KlJZcvGIWFJ+2oGg06aAtKJpDprkgzTc2fsfssSUUuB308bjolePUD8puQiq/+MnPfsKK20e2aBXRKVI1mmRauyeq/GEee+8rZo8tweOyUx39/OsrBx2WekUaDt6CotFoWqVVC4oQYrgQ4lMhRL0QIiSEiAghag9H4zRHB80FaY4q6cn0xRu4av6HSCn1BLMb0ZxfvD8UoTC7+ZVdnV5Yo0mmtXsiZERYvbmc6Ys3cM1THzF98QZWby7XWbsOJ34tUDSa9iYdF6/HgeuArYALuAX4Y0c2SnN00VJAPOgMMt2Rtmbh0umFNZpkWrsndMa7TkAgDRevjGy11S5eGk1apOXiJaX8WghhlVJGgOeEEBuB/+3YpmmOBgzDBODlGSOo9IaYv/YbNpZVU5znotof1hlkuil5Ljsv3HI25XVBKr0hXtlQxp0XDaTA7WgSPJ/o5tVcwG/iZMs0JdX+EP5QhIiUOO1Weri1n72m69HS+ApQURckz2Xn6clDm7iA6TH3MJKOBcVqB7tbW1A0mjRJR6D4hBAO4DMhxMPAHnRwvQb18PxyXx0zlmxIij1Z+ME2fnLhSRS4HTpjVzfENCVbK+qTJkxP3lDKgEL18G7Jnz6WXri5yZZpSrZXetlXG+CelzfpOBVNl6Wl8fXWc4/n9hc2UlEf5OnJQxlQmKUz3h1JfJVgdYDN2fJxzhydZlijSZN0hMYN0eNuB7xAX2B8RzZKc3RQXh+MPzyhIfZk9thT6ZGVQU+dfalbkspnfvriDVT5w6360yemF1436wJemzkySXhUekPsqPTFxUmqa2g0XYHmxtd7LzkFw5RsLKuO9/0qf7jF2C5NB+M/oGJMRCv/7hnZ2oKi0aRJqxYUKeUOIYQL6C2l/OVhaJPmKCEcMVP6RocjJlLKZs7SdHVa85lvLcbEYhHNZh8KGREyHVYdp6Lp8jQ3vlbWB7EkTIR13+8E+A40xJi0REaOFigaTZqkk8XrMuAzYGX08xlCiBUd3TBN58dutaQMzjQl2KzaC7C70lLQ7qEG9DpsVnyhiA4K1nR5mhtffaEI1f5w0j7d948wvgMNdU5aQltQNJq0SWcW+QAwDKgGkFJ+BvTvwDZpjhKKsjKaFAibN/FMHDZBUZbOv99dicWRpCrI2NJ36V772IJMHrmqaRFQHRSs6UqkGl8fuWoQffNdvLKhLL5P9/1OgK8SHGlYUJzagqLRpEs6QfJhKWWNSPat1P47Gmw2Cyf3zOal6SMIR0xsFoHTbiHX6cBm0xaU7kpiHEmqoN2Wvkvn2scVuPFk2lk2bTgRCU67RWfx0nQ5YuPr8uj4arEIXA4LuRkOfn3lIH5xmQ6I7zT4K6HgxNaPy8iBkBeMINj0Ip5G0xLpCJR/CyGuB6xCiAHAHcAHHdssTWeipbSwNpuFPh5XK1fQdCVa6g8xWoojaem7dLBYBPnuDHC3+RIaTaegtXvJZrPQO8X4qivEdyJMU6UZTsvFK1as8QDk9O7Ydmk0RznpCJQfAz8FgsALwCrgVx3ZKE3nwTRli2lhNd0L3R80mvZB30tdhEA1SLNBfLSEM6GavBYoGk2LtOiHI4SwAg9KKX8qpTwr+vqZlDLQ2oWFEM8KIcqFEF8k7HtACPGdEOKz6OsHCd/9rxDiayHEFiHE6IT9l0T3fS2EuK+Nf6emjTSXFnZPjZ+KuiCmqb39uhOtpQkGNfGqqAvyXZVP9xGNphnSuZdA30+dHn+V2qYjUDISBIpGo2mRFi0oUsqIEOK/2njtBcDjwKJG+38npXw0cYcQogS4FjgV6AO8K4Q4Kfr1H4GLgF3Ap0KIFVLKzW1sk+YgaS5l7K4qP3ct/1yv+HUzWksh3Nyq8IDCLKr8YV1ITqOJ0tK9VFEXJGREsNss1AcMJj/7ibaydFa8FWp7UC5eWqBoNK2RjovXxmha4eWoQo0ASClfbekkKeXfhBDHpdmOccCLUsogsE0I8TUqcxjA11LKbwGEEC9Gj9UC5TDhdFh47saz8GTaycqwEQhH2F8fwmET8RW/12aO1D7R3YRYmuDEiVVimtNUq8K/W7OF/77wJKYvVkXnLi4p4mdjSsiwWYhIkFJq0aLpdsTupcKsDGacfwIelx1fKILdKvjiuxoyHSqldr7bTmFWRnxhSI+5nYy6PWqbWdD6sTERo6vJazStkk6qJSdQCXwfuCz6GnsIv3m7EGJT1AUsL7rvGKAs4Zhd0X3N7dccBkxTsrcmyOw3vuDKeR8wdcGn1AYMln6yA6vFwpC+Hl0krJvRWprgVKvC40v7xsXJkL4eppzTn1+9vZmvK7xc/eSHjHzor1w5bx1b9tVp9xVNt6HA7WDRTcO495KBzHlrM9c89RGz3/iCfbVBln6yI/7ZF4pw7yUD4+fpMbeTUbdXbV35rR8bK+bo0wJFo2mNVgWKlHJqitdNbfy9J4ATgDOAPcDcNl6nCUKIaUKI9UKI9RUVFe112W5NpTcUn1iCejDOemUT40v7MvP5fzLj/BN0kbBD4Gjss4kphNfNuoDXZo5McjdJVYixwO2I96EZ558Q70OzXtnUqv+9pnNxNPbZzorFIshy2rjn5eT74Lbn/8n40r7xz/e8vIleOc74eXrMPTg6vM/W7QGLPb1K8hYbOLK0i5dGkwbpVJJ3CiF+JISYF7V6PCuEeLYtPyal3CeljEgpTeBpGty4vgP6JhxaHN3X3P5U135KSjlUSjm0sLCwLc3TkByQ6Q8bKX2kPS47u6r88dV0XSSsbRytfTaWJviYvEwKs5Prj6SysBRlZ8Q/x/pObJvI4VoZ1kHHbedo7bOdicT+FwinjkPxuOxJn2NdVBdmPHg6vM/W7VXuXSJN91RdrFGjSYt0YlAWA18Co4EHgYnAf9ryY0KI3lLKqMMmVwKxDF8rgBeEEP8PFSQ/APgEEMAAIUR/lDC5Fri+Lb+taZ3GAc7P3XhWyniDcMSkOM9FH4+LXjlOHTegiZOqSGOey87Tk4dy66L1VPvDSX2ocd8SQmCassP6lE7tqjmSpDvG5rsdDOnrYWNZNcV5LjIdVtbNukDHanVGandDZhruXTEycrSLl0aTBunEoJwopZwNeKWUC4ExwNmtnSSEWAp8CAwUQuwSQtwMPCyE+JcQYhNwAXAngJTy38BLqOD3lcCPopYWA7gdVXvlP8BL0WM1HUBigPOQvh48mTbmTypNWg1/5KpBZDttLLppmBYnXZRDtTAkWlgK3A6q/GFynDZemj6C0n4eFkw9i2ynjUeuGpTUt+ZOGMwDK77o0FiUdFO7ajQdQaz/FWZl8OQNpfTOzWgyxj40fhCPrPqSu0cP5OKSIp6ePJSeOc6UFktNJ6Bub3rxJzEysrUFRaNJg3QsKOHotloIcRqwFyhq7SQp5XUpdv+pheN/Dfw6xf53gHfSaKfmEIkFOA/p6+G+S0/mx0s/ozArgznjTqNfQSZ7qv08vHILFfVBXp15jn5QdkHa08LQ3LWKchzc+NynFGZlMHtsSTx7kcMmGF/aF2/QYG9toEMEcGtpkjWajiRkRCjMyuDu0QPjMVgXlxSx5OazqfGH2Vsb4NFVW9hYVs3mPXW8NH2EXgjq7NTtgaJT0j8+Ixv2b+249mg0XYR0BMpT0WxbP0O5YmUBszu0VZojQizA+d5LBuILRZg7YTDV/jCPvbeVivogs8eWsLGsGoCwYR7h1mo6guYsDG1Ja9rctZZNGx5PmTp98Yb48e/ddR4/XvpZh9ZPaS1NskbTkThsVu4YNSApQcTqzeUA3DP6ZDwuOzPOP4H5a79hY1k1Unacu6OmHQjWQai+DS5e2oKi0bRGqwJFSvlM9O3fgOM7tjmaI0mB28ELt55Nrd/gnpc3xCeKD40fxKOrtsQDN/WEruuSjoXBNCX7vUEC4QhWIXA5rHhcTYVDc9eKSFKKhJ2Vvhbrp7RHvEgsiL+xVUcHHWs6GtOU2K1wfKE7qe/HUm9PXfBp0pi78INtepzt7NTtU1tXGjVQYjhzIOyDcADsztaP12i6KelYUDTdBMOIYBGCGUuaphaeM+60eICzntB1XVqzMKRy23rkqkH0zHFyXIE7STg0LkRXlJ1BttOO3Qov3HI2v3p7M6s3l1Oc5+KJiWfy8zeSw8sS66dAgwXm1ZnnIBBJQfjpWllSBfHroGNNR2Oaku2VXirrg9QHI0w/9ziuGtoPq0Vgs1q4/umPmoy5L9xyth5nOzt1u9X2YC0ooIo12vu0f5s0mi6CFigaAMLhCFv3e/GHUq96H9cjE4fVwqszz6GHWwdqdkVMU2K1wJOTSpm+ZENKC0Mqt617XlYCNttpT3IDixWiq6wPcudLn8evN3fCYP70j2/571En8cvLT0UisFkEFfXBpPYk1k+JsavKjy8YYdKfPo77798x6qS4qE7HyhIL4tdoDhfV/hBWi+Dpv3/Lf194EpedURy3mLw8Y0TKfm61CD3OdnZiRRrTqSIfIyZQfJWQowWKRtMc6WTx0nRxTFNS6VNFGSu9oSaF9orzXFiirjxF2TpgsysSs4xc/vg6fvb6F8wZdxpr7z6fV2eekzTZb85tK9NhjbuBxbKA7anx43JY4+Ikduxdyz9n8ojjmL5kA4GwyQMrvqA+aLRYPyVGcZ6Lbfu98WQO94w+mUA4wuyxJQzp69FZuTSdDsMwlTukBaac05/y2mCSlbq5MVe7dx0F1EWrJrTFgqLjUDSaFknLgiKEOAc4LvF4KeWiDmqT5jBimpIafxDDlMydMBiHTbDk5rPZXx+k0hvilQ1l3DHqJH7y4mc8fv0QcB/pFms6gkTLyK4qP1MXfEpxnovXZo5M6bbV2AVMZeGyNnEBe/d/zkspaHrlOtlV5afSG2J8aV8mP/sJr8wYwUvTRyClbFI/JWYdeXJSKT97/QuG9PVw9+iBSX77j18/hPqAgd1qIWREMAyzXQPsNZqDxTBMth/wQtRKGDJMemQ1WAaH9PXgdlhZeNMwdlb64glJnpxUqt27jgbq9oLdBfbM9M9xaoGi0aRDqwJFCLEYOAH4DIhFykpAC5SjnJhftD8UYfqSDRRmZXDvJQPj7jOxCWFhtoOK+qBe0evCpJt+N1WQeSwGpcDtoNIb4ndrtsTTB2fYLCkFjVUIivNcVHpD8ary2yt9/Okf3/LTMSVEQgbhiMmJPdxJ8SJWC/GMcomZkAqzMvCHItz36r/a7Pql0bQnpimpqA9QURfknpc3xfvhH68/k4tLiqioCyWlG47FYgXCJr082o32qKBuz8G5d0GCBUUXa9RoWiIdC8pQoERK2TGV0zRHBNOU7K0NYLOKeLzB7LEl8QcpqAnq9CUbWHzzMB0Y38VJN/1uLMj81ZnnEAibWAVJWbxM02TKOf3jk67l00fwyFWDkiZoj1w1iP31oXimovGlfSnOc2FKyZRz+jPxmQaBPH9SKSf3zMZmUzEjpil5evJQTCnjIqjaH8btsCb13fGlfZske2gpXbJpSiq9IW1t0bQb1f4QhkmTMfVHL/yTRTcNY0elL0lk76ryc9vz6rsan0GeS6cY7vTU7gFX3sGdk5GttlqgaDQtkk4MyhdAr45uiObwEbOclNcGQBJ/QMZWshPZVeXHZrHolecuTswykhgD0pwotVgERdlO+uVn0jvXRcSEiroAu6v9eNGmdwAAIABJREFUGKaMT7qG9PVgmMqlZc6401g2bThzxp1GjywHhmmy8INt3DHqJP65vZKHxg8iknAuqL43Y8kGyhOC5y0WwYDCLJAw563NXPPUR8x5azO9Pcniqrm+nKogY8wt7cp56xj50F+5ct66Dq1mr+n6mKZkT02AcMRM2Q9r/GH65mc2+93kZz/RcVRHA3W7D66KPIDFCo4s7eKl0bRCOgKlB7BZCLFKCLEi9urohmk6jmp/iCpfiM92HkBCfFIaSyOcSHGeC6fdqsVJFycx/e66WRfw2syRrYrS2MT+p69t4usKL1c/+SHldcG4OLl79EDueXkT9778LwAKszMoyHLgsFnpmePk3ktOQQATzjqWR1dtwW61NJmwFWZlYErJjkqvEkDRuJLpjawjOyt9SX23ub6cyk2xuYKSeoKoaSuV3hArNu6KuzgmUpznIi/TgZQy5XexeyiVmNZ0IqRUdVAO1sULwJkL3or2b5NG04VIR6A8AFwB/AaYm/DSHKX4QxH+VVbFhaf25tdvb+ah8YOUO83ab3jkqkFpraJruh6x9LvH5GVSmN26D3xsYj++tC8LP9jG7LElZDttPHfjWdx18Ulxa8jGsmqmLviUyc9+QlaGjYnPfMz3577Pjc99Qn3QwJ1h4Y5RAyjIcvDcjWcxpK8HUAHE914ykGuf+ojzHlnL1U9+yJf76jDNpqvSj723lScnlcb77isbypg38cy0+nK68TcaTboIJBPO6kdEyiZj6iNXDWJPjZ9HVn3J/IQ+GyvQOH/tNzqL19GAvwoiwYO3oIDK+lW7u/3bpNF0IdKpJP/+4WiI5vBhscDQ/j3YWxNg9eZyKupCcX9+U0qev+VsIqbEZVcr3dp60n1pKTYjZEQozMrglN7ZSXEnxXkuFt40LOWk/4A3lGSpiNVDmf3GF0kxKg+v3MIdowY08d+fsWQDy6YNbxIvU1EfpLfHybJpw9lV5afaH2bJhzvi/bo4z0XvXFfKvpxu/I1Gkw6GYRKKmFTUBbFbLTy8cktSvNTDK7fw0zGnMHVkf3rmZPDqbefgC0XYtt/Lo6u2UFEf1AtDRwPe/Wrryj34c92FULGlfduj0XQx0sniNRz4A3AK4ACsgFdKmdPBbdN0AIGAgWnCjCUbmD22hOI8FxvLqpm+eAOgJmYLpg7DItDipJuTqmp8YiYsl8PKvZcMJGQ0jR2JuVw1nvQ3dpsaX9q3ibvWPS9vYumtwzGlTClyIlIyf1JpkwxdHpeDiBniruUNdVde2rArZbrkRFJlJtMTRE1bCIcjVNSrtO33vLyJ2WNLqKgPxsdXUPeBJ9OBKU3sVoEnMwPTlLgzbDx+/RCdpOFowRcVKBltECiZhVD3PpgRFZOi0WiakI6L1+PAdcBWwAXcAvyxIxul6RhCIYMD/hCGqSZ+89d+E3fvAuJphXOcNo4rcOsHZDentdiM2CSsLhBO6XL1RCMXqycnlfLKhjKG9PXw5A2lLJs2nJN6ZlGYlZxVa1eVn321AWwWkdJHX0p47L2vmD22hJdnjGDxzcPonatc0g4m2D9GW+JvNJrGGIbJ9iofJioPf3Nj7PxJpVR5Q9gsFnKcql8erHulphMQt6B4Dv7crEKQkYZK9BqNpglpFWqUUn4thLBKKSPAc0KIjcD/dmzTNO2JYZhs3e9l+uJky8mjq5T7QYHbQc8cJxl2Cz3c+gGpaT02I2yoWJDyumBKl6scl53FNw/DKgR2q4VVX+zmrosHcsAbapJ2+OGVW9hYVg00WFr6eJw8MamU2xIsJfMnlfLrtzezenM5qzeXx49fNm04nsxksXEwKYNjE0SNpi2YpqTaHyIYNpn63Kf89oenpxxj890O3A4roYikt7ZQH93Egtwz2uBMktlDbWt2Qe4x7dcmjaYLkY4FxSeEcACfCSEeFkLcmeZ5mk5CzO1g+uINTVb1NpZVM+etzThsFiSSfJd2LdAoYrEZiRTnuRBC8F2VL/451SrxvIln8uu3N3PBo+9z/TMfUx8Mc0ofD1W+cJO4knte3sQdowbEz31o/CBe2VBGxASX3cILtw7nr3efx4vThmOzEBcmMZTbV8NnvRqtOZyYpmT7/npqA0bc7XDu6q+YO2Fw0hgrpSRimkSk5LqnP6LKHz7STdccCrE0wc62xKBEBUrtrvZrj0bTxUjHgnIDSpDcDtwJ9AXGd2SjNO2HYZjsqPLhtFmZO2Ew1f4w89d+E1/VO7lXNqaUZNgsFLozsNm09tQoUsVmzJ9UygMrvqCiLsSsS09m8c3D2L7fxxsbv2POuNM4tiATh83Cg2/+Oy4kdlX52VUVYPYbXzB3wuCUVpnjC928PGMEld4QCz/Yxo9HncSct9Q1YqLFYbVQHzRSxrY47brfao4M++uDVHrD9Mh2JI2xv/3zl8weW8LAXtnI6BjrCxnsqPTpLHFdAe9+cLjBaj/4c91FalujBYpG0xytPtWllDsAAfSWUv5SSvk/UsqvO75pmvbggC9EMBxha3k9AA6rhV9cXgKoQncWIbAIgTvDit2ug/U0DTSOzXhp+ggee+8rKupC3D16IHcv/5wLHn2f2W98weRzjmNATzf7agNU1ofi4iQWb3JsQSazx5ZgNlP7wW610K8gk9P65PDA5afxh/e+ShI4s17ZRNCIUJznbJoK+4ah9HBr9yzN4cc0JYZp4sm0sX2/sio2HmOllFiEoD5oICU8vHKLzhLXFfBWtC1AHsCRCXY31HzXvm3SaLoQ6WTxugx4FJXBq78Q4gzgQSnl5R3dOM2hYRgmSEltwGiSxvX+H5yCYZrYrAIpITtDZyzSNCUxNuO7Kh+rN5fz5A2lKSu+P3fjWditFgrcDi4uKYoLmcT0w3MnDObx64dw+wsbk/ZZBRRlO5N+J5FdVX6cdiv760Mc38PNsmnDiUhw6pgpzRHCNCXfVfuwWy3srw+lHGMzHVbVb+uC7K8P4bRbdBrhroK3om3uXTHcPaBWCxSNpjnSLdQ4DKgGkFJ+BvRv7SQhxLNCiHIhxBcJ+/KFEGuEEFuj27zofiGEeEwI8bUQYpMQ4syEc6ZEj98qhJhykH9ft8U0Jbtr/YSimZYa+/z3ynXSIyuDBf/4FpvFoid4XQjTlFTUBfmuykdFXRDTlK2flMb1IJoi1WVP6aZV4w9z1fwPufbpj/jxqJOYdenJTYTMXcs/pz5gMGfcabx313k8OmEwTrsFe4JrYXOxL/luB8+t24bFYuGYvEz65WdSlK0DjTVHhtpACH84QjiSeozt7VGCe9OuGm57/p8U5WRwfA+3zhLXVWgPgVK9s/3ao9F0MdKJQQlLKWuESBpM05nxLEClKF6UsO8+4D0p5W+FEPdFP88CLgUGRF9nA08AZwsh8oFfAEOjv7lBCLFCSlmVxu93W0xTUu0LUkANDjPM/43tw6/WVrCxrBZQD1ApJeu2lnPFmX3pkaXdY7oKrdUuaXQw+CrACIHNoXLzWyzNXu+c4wtYdNMwhKDFGie7qvzctmQDL9xydkoh0zc/k93Vfu5+6XM2llVTnOfi1ZnnxI8pcDtYeuvZBA2JRYApwWqBFz/ewZ0XDdQrz5ojjhmJkBk6wLHWIKbIoDArWbTvqvITMSXZLhtnZOXy6sxztKWvq+GrBE+/tp/vLoKyj9qvPRpNFyMdC8q/hRDXA1YhxAAhxB+AD1o7SUr5N+BAo93jgIXR9wuBKxL2L5KKjwCPEKI3MBpYI6U8EBUla4BL0mhzt8U0JXuqfeTWbcW9aDT2PwyidM0EnrnEzZC+Kh1icZ4Lm0UwZnCxXsnrYrRWuySOaUL5ZnjmQvj9aWpbvlntT3G9wqwMxg05hsnPfsL/LPu8SRzIQ+MHMX/tN/HzdlX5sTZTxyRiShKNOruq/MgEq8/++iBBw+TG5z7h+3Pf58bnPqE+GOG2C05str+2t9VIo2kOwzAw923GseBiMh4fjGvhxTz3g4bxFVQ/Lzvg4/qnP+aAL6zFSVfDNMF3AJxtqIESI6sQ/FUQ8rZfuzSaLkQ6AuXHwKlAEFgK1AI/aePv9ZRS7om+3wv0jL4/BihLOG5XdF9z+zXNUBcMkUcN1mXXN5iPq3dS8OYUfnZ+YbxgXlG2U6dg7YK0Vrskjq8CXrwuqY/w4nXgq0ia7PvDhooxOf+EuLvWxrJqHl65hTnjTuNv957Pi9OGs/CDbfE6JhBLR0yT9MMPjR+EzSqY9comHr5qEMumDWf59BHs94a4ct46Rj70V374xAdU1AXjBRxjFhlvMMKeGn8TARKz8sTOv3LeOrbsq9MiRdPumKYkUl+B7aXk8dXzxhQevLAXoPr57685g7mrv2p+gUBzdBOoVoUWD8nFS2fy0mhaIp0sXj4p5U+llGdJKYdG3wcO9YellJL0XMXSQggxTQixXgixvqKior0ue1QRDBrUByLU1NU39W2t3slpPVVBuwGFbp1OuBPQEX22ufiNJhmDjFDKPiKNUNJk/5tyb8q4k41l1Uxd8ClWIeiZlcEdo05qIkRChmThB9uYPbaEZdOGM3tsCQs/2EYkIuMWlmp/mIiU8Ro90ODDP+P8EwCVCWz22BKChskXu2v56Wub2LKvDsMwqagLsqvax96aQJKg0ZPCjqE7j7PhcIS9tQGsZup754R8Oy/PGMGCqcMIR8y4YNcphY8sHdJnY1XknW0o0hjDXai21WUtH6fRdFNanaUKIYYKIV4VQvwzGsC+SQixqY2/ty/qukV0G0vV8x2qvkqM4ui+5vY3QUr5VFRADS0sLGxj845ewuEIW/d72VMTYHe92dQ31tMPw2LHZhE4HOmEHmk6mo7os7HaJUlpeFNlDLI5UvcRYU9yEXvsva08ctUgfKFI6vTANgt2u5WTe2azbNpwXp4xgtljS3h01Rb21PiZOrI/c97azDVPfcSctzYzdWR/9tYGKM5zsbW8njlvbaYwOyOl1cfjsjOkr4e7Rw9kzlub+f7c95nz1mamnNOf1/9ZxpZyJaS+9/BaZr/xBXePHsiQvp74+Z1lUtiV3M+66zgbDkfYUl7P1U9+yOd7/CnvnW8OhAkaJvcs/xxLQsymTil8ZOmQPuuLCZRDcfGKWVC0QNFoUpHOMvrzqID38cBlCa+2sAKIZeKaAryRsH9yNJvXcKAm6gq2CrhYCJEXzfh1cXSfJgHTlFR4Q8xYsoFKb4inNtRSednChoeopx/G1S/gs3lwZWjLSVemce2SZjMGZRbCtUuT+gjXLqWKnCaWkodXbuGU3tk8Mak0Sfg8ctUg6gMGpimx2SzYLIJKbwiPy86M809g/bYD5LsdzBl3GsumDWfOuNNwOaw8t24b8yaeyXub97Grys/OSl9K8eMLRZJcy6ChJspVQ/s1sbrMeqXB6tJZJoXa/axrUF4fjFeJ/9Xaiibja3jC83htHh5dtYWK+iC+kBLHzS4QaI5uvFFLzKG4eLnyQVi0QNFomiGdpfQKKeWKg72wEGIpcD7QQwixC5WN67fAS0KIm4EdwNXRw98BfgB8DfiAqQBSygNCiDnAp9HjHpRSNg6879aYpqTaHyBfVrP06mPwRuqZecGJ3P/Xr5l20XJ6Z1nIdruptuSSY7OQ5dAPyq5OYu2SFg6CohK45d2kLF7CG26SoauiPsh/9tThtFuYM+40Mh1Wqv1hHl6pJmOvzRxJnsvO/voQr27YybTSHAZlWfivM7N4eXM55wzoiS8UId/tIBCOML60L4//ZSvjS/vy0oZdPPbeVp6YeCa3Pf/PpIr12U4bElJaV6wW0azVpTNNCptLWvDazJGt/x9pOgWGYVBADUuvPoZyn+RXayu4ZaWXn120nMG9M/GbVu5+5ztWbf6E4jwXT91QSq9cJ+tmXYDDZqXA7dCxfl2NuIvXIQgUi1W5eekYFI0mJekIlF8IIZ4B3kMFygMgpXy1pZOklNc189WoFMdK4EfNXOdZ4Nk02tkt8QaD5NZ+jXXZ9fSt3gmeftRfuZipI/sjrVYi2U58VoGMmLgd+kGpScBigayeSbtiLmKJaYqfnFTKz17/gvsuPZmpCz5tcpmQEWF3jcFj723hNyNtFLw5Qfnoe/ox8eoX2Ge38O1+L/PXfpMUSH/zfx0PKAHkybTz2x+ejt1qwReK4A9FGPuHf/DkDaUpUxo7bJak/UP6erhj1AB65Tp5afoIirI6RwKItJMWaDolZiSCqPgPzuj42tfTjwVXLOLGt+v577d28+iEwRxXkMnsy/K471ITm9VC7xynivFzH+nWazoMX6XaZhxCDAoogZIQz2SaslOMWxpNZyAdgTIVOBmwA7EcpBJoUaBoOh7DMLGHarHW74ErnlApC9f9nqzXbqBw3OuUR3IJGhFcdjvF2Zl64NO0SqKLWMiI4LBZsVqUiKj2N7WujC4ppFDUIGWQuZcUkfXe/UnZjWwvXc/ei5Yz563dPDR+EI+u2hKvfRK73vxJpby3eS+989wUuK0cV5BJMKJiXuav/YZHrhoUL4RXnOfiyRtK6ZmVERdShVkZ3HvJwKRjmq39cpiJJS1oKrCOvPuZpmVMUxKurySj0fia+/pkHh33OuVmLi67BcM0AcH2Sh+D++bqBCTdAW8FONxgtR/addyFsH8rb2/aw7y1X/PlnjqynDa+d1Iht57bn0HFhxDjotEc5aQjUM6SUg7s8JZoDgrTlOyv91Po2wNv3xVfsebyx+EvD3Jsrh2XJRMhwOPSlhNN+jR2ETNNydOTh/K7NVt4aPygeEzI6JJC/nihC9uCi6F6Jxmx/ufdB7vWq5Ord1KUKeIxIrPHljDnrc3Mn1RKrsvGgqnDKMqxI0SPeExJcZ6LuRMG88hVg7BbLfTLz+TVmecQNswkl5lEIXXNUx91SjeqVBapzuJ+pmmZGn8Qj7f58dXrs+G0W/AGDWr9EXrlOvG49P9rt8C7/9Dcu6JIVx6Run386IUN9M3PZMyg3tT4w/z1y3Le/Hw3lw3uw6xLBlKcl9kOjdZoji7SESgfCCFKpJSbO7w1mhYxTUmlN0TIiCAQ5Fu8WL37klb3WHE7jJmLIezYLBbyMx16RU/TNqKV5i1GiJOzHDw0/nRChuSl6SOQUlIoarEtuCi5lsqK22H0b2DZJLXP049ynwoI31Xl5+Re2bw4bTiLP9jGk3/fTnGei+dvObtJwPtdyz9nzrjT8GQ66NGMu1ZMSH1X5eu0blSpLFI6JqFzY5qS/d4guWYtopnxNSht5Lsd1PjCZNgtnFDk1AtB3QlvOWQcukD5Z20OpTLMZSe5ufbc0+P9xxcyePPzPbzzxR5W/3sv0793PLedfyIuh7a8aroP6QiU4cBnQohtqBgUgQobGdShLdMkEcsGFFuJnXft6Vzasyrl6p7MPwGvzUNRtvNIN1tztBEVJZgm0luBWDYRqnciPP3wXPM8IqcPuPIxTDBrKlLWg4jn9/f0o/Kyhfxqpcp4U5zn4su9dcx5azMPjR/EJ9ur2VhWrWqZpBAYJxRlUexxtTrp6+xuVGklLdB0CkxTsmVvHSs27uTeMyPJ4+uEhbD+T0TyTqBa5GIRUJiToavEd0e8+w8txTCwo8bkhW2ZlNpg6mA3wYQ+lOmwcc1ZfbnwlCJe+GQnj/3la5Zv2MWt5x7P+DOLyc08RNcyjeYoIJ2l9UuAAagUv5cBY2l7mmFNG0nMBnRtaW8u7RdBLJvUdPX6vFmErJnkuvSESHOQmCaUb4ZnLoTv1sfFCaBEyrKJsPufUL6ZKl+QLftDKetB1Dt7se/m9dROWsn96ww2ltXGizfOX/tNk5TAld5QyjTDtjQnfWnXftFoWmF/fZDfv/sl/3N2ZtPxdfkUGHknYXs2v3xrM1JCUbZTi5PuiK/ykF28fvNRgCqhruEIVqY8piArgx9/fwC/GFtCttPGg29tZthv3uWGP33MH97bytot5ZTXHXLdbI2mU9KqBUVKueNwNETTMrFsQEP65vDrC3sg6nanrgSefwIBRx652q1Lky4xq4kRghevU/3KlZfaOmLPhBevI3vKaqa9u5dnLltIwZtT4qvMlZct5JblZWwsq2VIXw8zzj+B+8ecypd76+IB8tCQEhjglQ1lzJt4JjMT0gzPnTCYB1Z8wZ0XDWw12F27UWnaA8Mw8Ycj3HlOPnZfeer+79vPPmll9eZyZo899cg0VHNkMU3wHTgkgfJZeYRV2w3u7p8Le8Dhb7nC/cm9c/jl5aexvdLL+19VsHl3LX/fuj/+fUGWg1N753BhSU9+eGYxWRm6GLPm6Ef34qMEu9XCjHOP466z7FiR4O6hVq8TH6KefhjWTLKd2nqiiRIVH9IIYQg7VeQgLAkT+JjV5MXrlK99rD/5q1L2L/xVKjuXDFNRH+aWlV6evf7PREIBstxu7n9TiRNQRR7nvLWZpbcOZ85bm5u4YMWyeE0d2Z8lH+5g9tgS+uQ6cTlsCAHjS/vyuzVb+O34QURMWhQf7elGlRjrpcVO98A0JVvK6wgEDc4ocEDYk7r/eyuwOPMoznNh1X2iexKoBhk5JIEy/7MgWXYYebwH9oA9sL/1k4DjCtwcN0Llr/YGDXYc8LGj0suOSh9fl9fztzf+ze/WfMUvx53G5YP7tLl9Gk1nQAuUo4Bg0MBth1nn5iO85fDCeMgqgnHz4I2Z8dVrec3zkNlDT6Y0igTxIap3Yvf0w3rZQu5fZ3DfpadwXGYQEfaB1QGXPQaRcMOkbN3vVUzTitubxDjh6UfEYmfJzWezbb+X/+/9PcpS8uZ/mHLO8fx7T33cEvLQ+EEs+XBbEwvJvIlnYrUIlt46nDuWbmRjWTVby+u5e/TApIKNcycM5oA3xMMrv2R8aV8K3A78IYM+ua60kj8kig2Xw4phyibZwBofnxjr1ZlSFms6BsMwqQsEOTmzDosbhK8K1j7UtP9PWAh/e5TqM3/J3AmDseru0D05xCKNZbUmq7YZTDgRHK4spLBg96cnUBJxZ9go6Z1DSe+GWixb99Wx+KMd3LF0I1v21nL3xQMRQndUzdGJFiidnFDIwG8EyQ2WI6SpsiNV71Sv9x6AMXMh7zikxY6R2Qe7QwfPaaL4KhpctgCqd1Lw5hTuHPsaPUK7EPX7kwQuVy+GK+bD6zNUmuCPn8S44Q0sgOXA10qc1JcTnvA8d6woY9XmingdE6tFUFEXQkrJkpvPRiLZWxPg4ZXKrWtbpY+ltw7HlBKbRRAMGxzj8GIxQzx4YRE/f9dkxvknxFMYQ0M2r8U3D+PH3x+QJFyevKGUU3rltCgaEsVGurVSdOX37kU4HOGAL0BRsAxRsxPyj28YY737VEY6dyFk94ZPniH8vfvokdmHX7y5mV9fqfPEdEu8UXesNgqUl78KAXDpsYCwYDhy0ragtMaAntn84rJTeXbdNv7412/IyrBzWzTWT6M52tCBCp0Y05QI0yA3XIUAVRRq/LNQPFQdsGs9PD8BabETyToGu3bt0iRihFL60ffPtZDtK2sQJ9H9vHQD2F1ww2vIOz5DXvpbrA43piMLs7CEunELMKa8AxnZPPaDnqy8tYTCLDszlmzAaYMl15/IsHwffSinNwco7RHh+WuK+fhHJzG9NJv6QAi71cKDb35Bge8bMhZcjP2xQZz+5x+y7Ipcjst3pczmZZrExUls3/TFG6j0hlr88xPFxozzT4iLk9g1bl20vsk1dOX37oMZiYCvnCKzAuFwQ9FpIKzKmlg8VI2vyybBs6OR0iQwbCa3vxfgy31e7rxooE7C0F3xtd2CYkrJK1vCDO4BRdHSJoYjt90ECoDVIrj5v/oz/Ph8Hl75JR98037X1mgOJ9qC0kkxDAOL4ccWqEIYITAN2Pg8nHQx/GAuvHOXeoB6+iFtLmwO/bDUNMLmSO1HL6wq2L16p5qIjfyJCoqPhMHuhsVXImJWlXHzsLkLIRIkCxDPT4pbXE6+ZgmvXH8cZbVhekT2Y63ZCR89AWdcB55jISxxLJtEZvVOenr6EbnmBWqsJzKtNIfc1yckiSPH8okcO2U1o0sKmVaaQ1GmoNwneWpDLS6b4P/G9onv+9XaCjaW1RIyIi3GiySKDY/Lnpbw6OwpizXtRMRAHPgWu79SWUjMEFht8Pe5sO1vypL47s/jY+yWAxHsOdncMWogvT265km35hBcvD7eE2FXveS6AQ37DEduq0HyB4tFCKZ/7wR2VPqY9comVv/kPF1DRXPUoS0onRDDCGMN1WGp242o+BLq90LNThgyEb5arYpEjfxJNO7kBSxZRUe6yZrOiKsArnm+IRVwNMtWWT0Q9sHAMfD9n8Oq+2HBGHjzDtW3Yv2peqeystTshIzspmlXl03CcuBrjn1tHFZ/peqbZ09X16ve0eAqEz3euux6siLVDOrlathfPBSuWQJXPIFNhvnjJXmUrplA34XDKF0zgT+O7Unv4DdJ+565xM3okkJcDguB6j0YB3awd/dOfvba52zZV4dpqsKQMbEBxAPyE0klPHTK4m5ArMZPqB4ksPrnsH8L1JfDuXdD/+8pN8foGFtzxSJ+98EBXA4rp/TOIV/XPenexARKRk7Lx6Xg5S1hMm0wvFfDvogjF3s7CxQAp93KLeceT9kBP0+s/brdr6/RdDTagtLJMCMRrKE6FbxcX55cKGzcPDjrJqj9DlkwAPOmNVizisCidaamEaYJFV/C2t/G/ehlVk+EvScFFgsyJBCjfwWLxiWLjjdmJleCj6UWRqROu5rTp6FGxJQ3YeFlLaYprq33sqPGoNTTTwmh7/88Hohsi/XxrCJ1blYRNjMMjeqxFLw5hf+bsgpH7VYsy64ns3onvT39eHzCEmo4QKTOhyW7KC42bl20nvlrv+GRqwY1iUFpLDx0yuIujmmCtwJhhpVI/3CeEtVJwfCL1L3T8zTqr3+Te1eWc8eogRRlaWGiQbl4ObKUy/VB4A1L/vxtmHP7gDNh5qVcvCpBSmjngPaS3jlqsaA+AAAgAElEQVSc3T+fp/++jUnDj6UoRxdv1hw9aIHSyTDDAWwhH0SCTWME3pgJU95SD1arA5mpxYmmGRID5Le8DYDw9CP/lnfBVQiGBQyZWnS48ho+e/qp/maxpnYXs1gbzosYraYp9mQ5iYgw5jXPY6nf2zAxjF0jUSCN/ImaDKRoYwYhxLLrk861L59Ej9G/gWX3w7VLsRSVJIkNl8PKqzPPaTGLF+jK710W01T9MhJULrPuQuWO2LgPLp+sko8ICzYLXDvsOHrlZqSVNU7TDfBWtMm9629lBj4Dzj8meb/hyMVihrCG64k4stupkQ1ce1Y/Pt3+Gc/8Yxv3/+CUdr++RtNR6BG3E2GEA1hDtYCpVlJG/6YhIB7Uw1Oa4DkO7OmlWdV0U5oJkI+nHn7nXhVzctMq5WIV62cxQRJ7P24e5PYFYVPvE9zFGDcPanc3fDYN5TZ2zRLILVYr0Y2Ot3nL6fnnW7Cs/S2y4MTUbSw8WV0ju5eaDKSoVi+EVVlarlkCN76ttllFDZabF68DX0VcbByTl0m+O4OibCfH5GVSmK1Xw7sdIT9EQmCxg8Wm0msXlTS4NMao3gl5x0PYx9bKEEU5GeRlasGqieLd3yaBsnq7QbYdTs1P3m84lKuYPZC6mvyh0ivXydn9C3jh453UB40O+Q2NpiPQFpROghEIYA1VqTonyxoCkeO1J6LBmlgdSLsb4fQc6SZrOjPNBcjLCPz1N8qt5YUJyf3s4ydhxI/AkQl3fAZIFVD/5Z9h4MWQ0xsmvgJhrxIO7kIw/DBxOeQUgxmBi+fAgW/gnbvB3RMmvaZWrIUVQnXKIjLqF7DwMsTQG1O3sWaXimOZ9Br8/XdN61Fc8zw43DDqgeQ0yePmKQEPap/RcpYvTTfCCCCD0WyIvsrkMXbcPJWyfdd6daynH9TvReYcQ2HPInpkubSY1TTQBoESjkj+siPMsJ5gbbSuGBMotkAl5BzXTo1M5gen9+bDbytZ9mkZN/9X/w75DY2mvdFL8Eca08QMeLEe2ILY968mgcWsuD0pIN50ZCHcPbRrl6ZlMgvh2qXJFoxrlyo/51RuLStuh1E/hw//CAgwArDv37Dyf6HfMBWr8odSeH68OqfHyfDeg/DU+bB+AQgL1H0HdXvU95c8rOpIrPk5GEElhp65UMVUZWSreBV3oSou2tgqk5GjVrXX/Bzje7OUcBr9G7hpFXLyCszCEmX9SeUCmd1HWVQmLlcpkxMxTajfB9VlamuaHfgfoOkUmCYyUI+s+Arx3KWwe2PTMfaNmXDeLPXZ0w+ufAoyC4hk5FKUk6kt1Zpk2uDi9eneCDWh5OD4GBGHulZHWVAATizKYmCvbJ79x7cYET3uaY4OtAXlSGKayJAfS6ASXpoEVzyR2uWl56nIqX8m4i7CZtPZhDRpYnMqX3p7pnLbsjlVYKe7MHU/s2XA6F+rFWZvbYOP/vIpjeqlRH30o7EtDJ+hHtqNEzqM+oWyqqQ6/4bXoXwzIre4oY3+KrWSXV8ej0OpHfUwjlG/wWkxEVY7W31ZFNgjFEaacWEDdZ3PlsIFfcCVr8R8zLUtFpcTE2xFJVrsd1VMExnyIYI1DYkWmkneQMGJUauhgPo9YLFjc3lAW040icTimA7Sg2H1dgOHBc4sbPqdEY076UiBAjDm9N78vzVfsWbzPi49vXeH/pZG0x5ogXKkMIJI3wGVrQvUinEzgcUSQcTdC5tN/3dp0sRXAUuubOo+dfMayOyR2rWqcis8P0G9v2K+ytAFqSd09syGzzl9YPGVTVelJ7+uHuipzpemEhEX/VJZYRrjyoOBY8jBi61mR1xknZh7HAfM4uZd2Cr+03DdYB3U7VZWlcSkAbE2vHgd3PIuZPU8qH9azVGAaULIj/BXKrfG0b+Bdb9vcYw1bG4sRgBr/vHKAqmFq6Yx/irVn5zppxiWUrJ6e5gzCpOzd8WIxGJQggfaq5UpKe2XR4HbwbJPy7RA0RwVHJERWAixXQjxLyHEZ0KI9dF9+UKINUKIrdFtXnS/EEI8JoT4WgixSQhx5pFoc3tiGgHlcvDsxfCHM2HxFXDRr+CrlcrfPsHlRV7zPJGsY7Q40RwcqYLks4pUkHAkCFcvaepa9f5D6nP1TlUHwlepxEeKIPV4ID0oK0lKa4aAqm2pz5dSxcHU72v2+vIHD2PzRlNtLxgDb9+FzbuPfIs3tQvb5Y+re+js6bDkh/D0BfDcpbDvCzVhTRUMreNUuh6miQx5oeobWDgWHhuiYpq+//Nmx9jd9ODRv1dwwN5TCVYtTjSp8JarbWKmw1b4d6XJ7nrJiBTuXQDSYidic3e4BcViEZw7oJC/ba1gb02gQ39Lo2kPjuQofIGU8gwpZSxN1X3Ae1LKAcB70c8AlwIDoq9pwBOHvaXtiBEOILz7m6RI5bVpMPg6FRA/Zi7cvh455S0iBQN1lXjNwROzMMQoHqqCymMFGQ0fTHkbbl8Pk99IDhIG1ScdWfDn+5pM6LhmCfQ8Hab/HW5+V8V6pBIZFpsSPY3Pv3qRCrRfcbuypFy9qOn3NiciEkoZZ2KNBDARVGWfSPDG1cg7PlMr5H95EE66pGl8zbKJsO9f6u9PzIrn6af+nTRdh4iB9FchfPub1M9hxe2qf3z8JEx6FW79K3LyCmpzTmTN5nKuOLOvLsipaZn6fWrrSt/Fa832MAIY1oKh1nDkYA/sP7S2pcF5JxViSnjln7s6/Lc0mkOlMy3LjwPOj75fCKwFZkX3L5JSSuAjIYRHCNFbSrnniLTyEDCMANaqHQibo/mid1c8ATYn0uYk4szBZtPpLTVtIGZhiLk1nTcLPnoCLntMpQCu2garp6p4jwkLVcatRDz91DFb3obMPBUzAipD19v/o867Yr4SJ3++T71/fUZDfMfVi6D2O3XcXx5UAsKVpywvRkBlwskqgpxjlEiY9Kq6ftU2lQFs13r40SfNuIdF2LKvjlsXrWdXlZ83p57E6avubznGwJ4Jr9+mspC9cZtq17VL1b+TpmtghKB2N0KIaDKEFP2g6BS48AGwu5B2N8GMPMJhwZjBxbogp6Z16qMV3w/CgrJ6m0FJPnhaeJQrgdKxFhRQKYdP6Z3NS+vLmHn+Cepe0Wg6KUdKoEhgtRBCAk9KKZ8CeiaIjr1AbMZ0DFCWcO6u6L4kgSKEmIaysNCvX6PV3M6AEcDq3Y+wWlXGo5QpYCWE/WDPQlgd2Gy66mtXpkP7rMWi6olM/bPKeGW1N62YfdUCCNYqt69Lf6vO2/J2g8B4525lcRh8fVSY3JXcZ1+f0RAs790Xr1hPdm/YtAxOv0oJj6ptypJSX65cySwO5Woz6gHl3hhPH7wENr/RYMlpJl7AsDjj4gTg5+/u5blxC/G8MaXZc/BXqX3+AzDmd2oxIBZAr0mbTjvOGiEleg2/Smkdq5/TuB8c+Fa5cFntSGceTqcLpx5muzTt2mdjFhRnegKlrM7kPwdMbipp+bjIYRIoAOedVMT897/h0+1VDOuf3/oJGs0R4kg9nf9LSnkmyn3rR0KI7yV+GbWWyIO5oJTyKSnlUCnl0MLCzrUqaoQDyNo9iJpdUP4fNUlKVfTOnqnqSYR9KtWqpkvToX3WNKHiSxWD8dgZEPIluz5lFal+9uYd8Oxo5fr1vbvhzn8rK4MjS1ldRv1CCRF7ZvJkr3ioEiR5/ZWwAJW+9dnRqmBj37NVauLHhyphM+b/wU2rIb8/ZObDuXc1dd9aNglG3qmuXTxUFdS78qlGqZJfoIqcuDgB2FhWywvfuJCTV4DnWGURahybsu736r23Qrn+yIgWJ22gU46zRhDpq1TugpbomttnS1O7JhaeDJkFSIcb0+E+cm3WHDbatc96y9W4lGbfWbM9DMDwVvJwGPZsVQflMHB2/3ycdgvL15e1frBGcwQ5IhYUKeV30W25EOI1YBiwL+a6JYToDUSj0fgO6JtwenF031GBEQ5g9e1HSFOZhU0DrBngtCSngHW4kaBMrvnH68mT5tDwNspaFaxJFhgjf9JUICyfApNXqFonia5ajTPMFQ9VAceJ1phYQdH6cmWtidUuceWpc9c+BCNmqoD6Fbc3n1Lbt18JI2ce+CvV+dcvVyvj3grI7o0wrRTnuZJEypTBmYhFYxval+jKFmtXrI06OL7rYASRYT8iWKf6i7tQTSDPn6X6XMyql1kAGxbB8eciCwYQySzEZrMf6dZrjjbqK9SYlKZr1JrtBsdmwzFZLR8XceRgD1YpkS069tnvtFsZcXwBb/1rDw9cfirujM7k6a/RNHDYZ8FCCLcQIjv2HrgY+AJYAUyJHjYFeCP6fgUwOZrNazhQc1TEnxhhCHiVODEjyn1LStiyCgI1aqKWf7xa8fMch7RnIjLz1eqyFieaQ8E0IVSfLADq9iYHsqeK1cgqUg/IK55Qq81ZRapmyXmzlAUitiI98iepCz2eN0udZ7WrivSr7leWmVX3q885xzScFxM8iXj6QSSELDwFmRl1PVh5nyryGKhVbmJhPwVuB09PHsrokkJeueEEPrxtAJnWhHTGu9Yr17HXZ6iCkhf/uiGIftf69ILjdVHHzo8RAN8BNb7ancp1yzTh749ExcmvILuXcjlc8ws46WKk5zj8mb2x2XVsn6YN1O9LO0C+KmDyyZ4IZ6eRxdxw5CKkiS1YfYgNTI/zBxbhD0V4e1Pnn0ppui9HYibcE/iHEOJz4BPgbSnlSuC3wEVCiK3AhdHPAO8A3wJfA08DMw9/kw8O0wggw14IVCGMIOzfAq9PVxOt034IX76jKtEe+FYFa2ZkI/KPV4XyNJpDxVuhYkYSBcC63ye7FYZ9qbN8Lb6iQVR8/+dKpOT1bwh2HzMXep6a2vrhOVZNDI1QQ8B87LvXZygxHtu37vcqBmbi8obK79ctA4sdseAHiD+cqVzDYm2ICSCbCmQeWOTmiYtclK6ZQO/nzkJEwk0Fj7uniq/JLGi4t2IFGlsKjo8VdXzmQvj9aWpbvlmLlM6EEUDW7lWWtUC1cmd89VZl/Rt6i0rssGicEsJCwKW/xcw/kT3WPjgdepzVtBFvedpV5P+y0yAiYUQaJUeMWC2Uw+TmNaAoiz4eJy9pNy9NJ+aw2/aklN8Cg1PsrwRGpdgvgR8dhqa1C6YRQAS9iJoyeOmGpi4wyycrlxVhQRaVgD0T4fRoq4nm0DBNVYzQCCk3wlh635jFor5cTfSnrgIzBBariu94bVpDlq/GLl8rbleCpGZXg7uWNJX1b+AYVWU+5sL12VLY/5UKmL/wgdQCRgglROyZ0euEk6vP3/BaQ3sS23DZY0o45Z8QFxYW/35ITNX9wf8pd7SXJqt9A8fA9+5pKFYZd1fr1XqdC13UsXNjBJCBWkSguuUxduMSyD8BGfYTyexBHW56ZWfoTF2atlO3D/qckdaha7YbFDjhxDT0TKJA8TPgUFqYFkIIzhtQyNJPy/i2op7jC1vxQdNojgDa+bA9MQKIqp0IW0bDgxMaJlqjf6MCgS02pNWBcBeAztSlOVRiK/6xSfXE5anT+2bkqhXA+r1KGGQVNXyf0ye1qMg/AVbPVsIjFnuyYZGa/C+f3DA5nLAI1j+jzrOkyFI3cIwq/BgTJBOXwxs/Sr5HvPtTtyG3WJ2faGFsXIhyYzRQ/8Z3lECz2GDBD5Kv/9JkldWstcWAVEUuddxK58AIQPmXCHtmK2OsVRVhtDmJuIuw2eyknxhWo0mBGVFjWBoZvAKG5P0ygwuOgXT0cMShVMzhsqAAnHtSIcvWl7F8wy5mXXLyYftdjSZd9LJ9exAxIOCF+nIEqAxBqSY4rrzoQ1OLE0070njF//2HlDtXfbmarL1+G9hcYLUqcZLXX03kQH2/YAxU7UgdExKsVemJR9wB4/+k4kiG3QxfvNoowH6yKoIHysWscZa6ix5UvxU7p3FWsNh5qdpQtU3FE3z0ZIOrVWIhyuKhKvZlyA3KSuM5tqFdiVTvVHFgrdG4yGWsHbqo45HDNONjLEYArLYWx1ikRF7zPD5HgQ6G17QPvgPq2Z5GDMr7ZQZ+g2arxzfGcGQDh1eg5GU6OKOvh5c37MKIaPdVTedDC5RDwTTBX61ql/jKoXa3ijep3Z16ghP2Ia9ejMgq0uJE0340XvHftV5Vhr/hdRXfMWaumrTX7lEWjMeHNsSYxCqrf/K0muQnioor5qtaKGUfw+njYeFl8Hip2p72QxgyqeE3EyeHWb3UduIr8OMN8MOnlHBIbGOqIPnPljZtw+WPK8EVqIUPH1NCzFfRUIhy4Bj1d6y6X6U4fu5SJWKshyAyYtdOSm+sizoeMSKGEsqxMdZb0eoYKzPzefifFnyGdufStBPxKvKtW1De/jZMjgMG9Ujv0hF7NhILdn/FITTw4Dn/pCIq6oK8/9Xh/V2NJh20QGkrRhhCdRAOQNW3sOhyNUFadb9arW5cv+GaJcheg5BFp2hxomlfUq3415crlyjPsdB7sDpm2cSmLjEjf6LOPe8e2P4hTHlLiZrrl8OWler7Qdc0dadZPhlG/Ljh9zz91GvqKgjWwcKx8MezYPGVKq124yD2xkH7nn7KUmNGlKC68e2GzFv15VBT1vDbRki5aRWVwA8ebppR7MXrANl2kRG79i3vwk++UNuiEh0ndriJGEqYevdD9Y70xtiegwjmDeDcxzdx+Zn9KHBrq5emnaiNVjdwt6w6Aobk3R0GI3qBLd0hQ1gIO/Nw+PYeWhsPkiHHesjLtLPgg+2H9Xc1mnTQMSgHi2k2VN8OB0D+/+2deZhU1ZXAf6eqq/eG3th3CIqKEYWIoqNOIK4TUQeDo4kwY+LExEQn25jJRjJxJjFj4h4nGmOixBhc0JhMCCguUQRRFkGiIiCr7N3Q9FbLnT/ufXZ1U71Udy2vus/v+97Xb6t3z33vvNvv3HvuORHbs1I60H4c1WyFx+bCzLvtx140DHn5mGAh0aJy8tRNREk1Xo+/5+blfYyXDbUf1bGYdZNqm2jxjBttRK7PPGk/+I670M7fWHgdjDnLRkNacLXNBp/IncZLiuclGn3hxzbRYltDaMEc+PST9pynvmDflbP/HfoNsWUHQjai3XM/sL+ZPs/K0Hbys1eW9w4FAtZly8t9csaNLZP26/ZYN7Jrlth3NS/f3qeuGhmBgE6IzyaRJvsMY2GbGf75H7fWqfg21kQBMAgHpYxwc4Anv3AGVSX5OiFeSR1eJ0lJx50cL2yLUB+GM7sQvSuecGEVBUd2dlO47pEXCDDjuEEseH07G/cc5iMDyzJavqJ0hBooyRAJO1eD/dYo8aIexX9EbV9p9wXzQQSTV4gJFUNhf/KCwWzXQOmNxPf4RxJ8jNfHhR2u2Wpds6ZcYw0HL+LVuT8Ag52jMutBm/TOmwQfix496d0zFK5/3Y4gPjvPGgj17Ux0B3vOzLuhoKwl2lb5SJg937qYeTlKyoa0GBaxCCz6VsuxtqMgeflW/qn/2jpx5OW/hsXfhU/+DMpHoOQIsZgNG1yz9egIXUd2Wz2AVm0seUWYaJimwoGU52uULiVN1G63nTKduHgl697lES6sprBuew8E7B4zjhvEwtU7+NXLW7j50hMzXr6itIf6LHSVSKONgBRpgJotiUOynnGj3S4faXtfC6uQ0oEESioJqHGipBOvx798xNFhdCPNLWGHj73I6qlnnAyfYj/uH7rUzi9ZeJ01DCrGtOj3sjttlK54d5pPPWRHPuo+gPmX2w/Hoor2J7rn5dse8cbaFuME7N9Hr7KuWp47VeVYKHN1KR9ljYz2XK2KB8B5Nx/t5rVgjg2DrJG3codIGPZvtC6CiSJ0ee0rxLWx/SEWRUqHUFhYqMaJkj5qt1v3rg4yvTdGDEu2JOne5QgXVpFfv8uGYM8g/YpCTBtXzRNv7KCmXttLxT+ogdIVIs02ytGe9fbjLVEEovhJwrMfhuKBkF+kkX+U7BPMbwk7PP27dgSwdKDV05n3QLTJboPV4yevdfNXnKGx6mEbQviqx+HLq+z8lDcft8Z6fMJHLx+Kl3EeWkIQL3eT8EsGtB9dK5Fx1ZHh5R0PtBPRqWSAvn+5QjQCTTU2Old7URA91xqvjS0sh7xiG4ZaI3Up6aZ2e6dz2P68OUJ9BM4elvzlw4XVBGJhQg37uilg97lg4mAawlF+s+z9jJetKO2hBkpnRCM2vGDdHhsBaff6o7Nwg93uP8L6RA84FgpLdFKt4g8CwZaww/X7rJE9fZ6dbHz3qS0Z272IXp5bV/wk9s0vWrfGJz4H90y1EbUWfduOcHjnvXwbnHYdLP9fO8H9XxbZSGJbl8PkOVAyEIoqUx/Ct72wwKWDNPJWLuC5dR3aabO/R5oTP8+yIXD9Srj6KVj7mJtbpIaJkiFqtnY6Qf6RDc0MKYYTq5K/fLjQXrvgyI7uSNcjRlWVMHlUBfe/tInahnDGy1eUROgXdCKiETj8gW2QarfZifCv/txuv3wbFFUdHYFo9nzb61w2VKN0Kf4i3GDnf5z3X1A22LoodOaiaKItv5n7RzuZ/dl5LXMAwCZvzC+xyRznPAOX3QcVo23OkgHH2p7tgjIYPc1mdF/6Q+u+0PbdueK3RxsSsZg1iGq22Xfx8AdwYLPtxYxGWp+bKCzw7Pm2w0A7CfxJNAK1O+wzPeSeqTcRvj2XwmC+HS1b9G04aXank5UVJWWEG6wBXdZ+YpNNNVGW74ryiZFdS87YluZie+2i2o3dlbJHzJo8nEONEe5Zmp3yFaUtOkm+LdGI9YM+srf1JPjL7muZpPnnb8D077koXU02jGpBmXU50A8ixW948z8e/bQdJbnk3o5dFC++y+q/9xuwLjV1e1r/pnykjbb05LV2BCZ+kvrs+VD5ETtB34vq5WV7/7uvW6MnFoVQkR1ZiX9vYjGbyyQ+KtnMe6yBVLfHfqwOmmjdgaDzIAGKv4hGrLusl7jTe75nf8O2sZ6eXLnAPstIM6yaDxMvtUbJP/zM/tXnq2SKA5sAA/2Gt3vK798OExSY0c2YHM3Fg4gGCyg5uIFsZCUZXVXC2ccM4Jd/3cynPjaCcQNKsyCForSgLbyH12N7eBfUbj26h/mJz7X0MG9fCU990UaQCRXb7NrFlfoPU/En8SMM21daPU/kQuMlVyzsZ/d9Ki5p4upH4FNte7V/Y/Xfm9/ijbZcdCv0cyGO2yaRXPUw3HGS7Y3ML7Y9km3fm/q9LcYJ2L9PfcG+f150p7o2+QI6m6ui+INYzLaxnnECLc/3yJ6WNnbVw/Dby+2ISV4BTP2c1bl+w2wABX2+SibZ96792z+xgRKOGh57O8zHBkFVdx0oJEBT6QiKD27o5gV6zhUfG0FBKMDXF6zR7PJK1tFWHlp6bO+fYaN1tTcJvtUkzfk2Y3b/4S09uYriR9omHhwwwbpVxRsbM++xYYYL+0NxtY2kNegEG+73hjVwwY+sa9gn72hJovjCTyAYssaPN9qy8Drr5lhUaa/dnfkhbY0aaBnh8daj6iedc3jtbKQp8fMNFR89ET6/1M5zKh9p/f/VMFGywX5noPRLPPv9D++F2ddgOH9kwsNdprF0JCUHNmQ8kpdHeXE+/zxtDG9sreHWxe9kRQZF8dAva2jdYxuLtkyCb5v3oWyIjWIUCFnjRCdoKrlCosSDF91qPwobDra4T312Seu8IWXuN7GYzer9hy+3TgZZVGmX9tyrEiWR7Gx+iGfUtH3/Gg62rAf13cs5vHb2ygWJn2+4HqqPsbl18vJdG6tR2BQfsO9dazyHjh4eMcZw7+pmRpXB5IE9K6a+YgKVO56jdN8a6gac3LOLdZNp46rYsOsQP3/+PYaWF/GZ00ZlRQ5FUQMFWvfYLrsTTv9SS9br+I+qvALbu6wjJkquE26w+Uva0l7ekM7mebSXdb0780MSGTXeHBRvwnRp+5NVFZ/itbPL7rTPMD4R48x7nGteCPKKdI6J4i92rrYBQBLwf5sjvHMwxlcndW9yfDyHq08mJnlUvf+nrBkoIsLcM0ZTUx/mOwvXsfdQIzfMOIag5hhSMox+aUPrHtuuTuRVlFymvVGKjnqsE43CdIVkf9fWqPFGSy67366XDtZOglzE0zmvjf30ky15bELFdiRO21jFbzQdhn3vwEn/dNShcNTwkxWNjCyDs9ufP99lYqES6gZMYuDG37Nj4ueJFHYjXnEKyAsEuHHGeO7/62bueG4ji97azbcuPI6/G1+NiBoqSmbQ/wZwdJjSzS9Cc52N2FE5JvFEXkXJZRKF5r3iEf/kDYmf9F422C6VY3TOVy4Tr3OrHrahp5vrbTur80sUv7JrDWCgevxRhx54s5nNtYa5EyCYou/23eNmE4zUM+6Vf7cdpFkiLxjgX88ayw3Tx1NT38zVD6zggttf4v6XNrHnUGPW5FL6DvqfHjRMqdL3UJ1XMo3qnJKLbF1m/1Yf02r3ppoot73exGmDYWoKPU6bS4ex69irGfq3XzFq1S28P/mbqbt4kogIp42tYvKoCl54Zy8vvLOXH/5xAzf/cQNTx1Ry0UlDuWDiYKpLC7Imo9J7yRkDRUTOB24HgsD9xpgfpbSA7rqvKEquojqvZBrVOSXXeHcxVI23EQ4d9WHDF5c0EArAdRNTX+TBEZ+g4Mh2hr51Hw39xrJn/OzUF5IEoWCAGccNYsZxg9hR08Cy9/bz6qb9fGfhOr731DpOH1fFJz86lPMnDqa8WANbKKkhJwwUEQkCdwOfALYDr4nI08aYt7IrmaIoiqIovZIj+2DbCjjpig93NUUN1y+p5+39Mb53KlQXpafoD465moL6Dxiz4js09hvNoUFT01NQkgwrL2LW5OH84ynD2HawgVc3WWPlpife5NsL13HOsQO55OShTJ8wiKL8YLbFVXKYnDBQgFOBjcaYTQAi8jtgJqAGiqIoiqIoqeeN3wAGRp0BwL6GGF9a0sCynVGuPxGmpJV+YmoAAA8gSURBVHMwMBBk24lfZsxr8zjm+c+z7oInaew3Oo0FJoeIMLKymJGVxVw+eThb9tfz8sZ9LNu0nyUbdlOcH+T8Ewbz9xMGMm1cFVXqBqYkSa4YKMOAbXHb2wF/dCcoiqIoitK7qD8Ay38OQybRWDaKx99q5qevNXGo2fDVSfDxEZ1foqfEQiVsm/RVxqz4Lif85Qo2nfoD6ismEIg1I9EwDf3HYoLZ//AXEcZUlzCmuoQrTx3JW7sO8cp7+1i0/gOeWLUDgGMHl3HC0H58ZGAp4waUMrCsgIrifPoXhcgLSqvoYA3NUeqbIxxpcn+bo9Q32b8NzREawlHyAgEKQ0GK84MMLCtgSHkRQ/oXUhjSUZveQq4YKJ0iItcC1wKMHNnDdK6KkgFUZ5VcQ3VWyTW6qrPNkRgN4SiNDUdo3v02/Z77D0qPHOCWwq/wyEOHOdQMx1XA96fCmH6Zkh6aiwezZfK3GbH2Nia88PlWxyKhMvaPuoh9Y2dypOJ4oqFSyHIY4EBAmDisPxOH9eeaM8eyeV8d63YeYsOuQzz/9l6eeGNHWssfUFrAiMoiRlQWM6KimBGVRQyvKKa8OERZQYiSgiAlBXnkBwOIoGGTfYwYY7ItQ6eIyOnAPGPMeW77mwDGmP9OdP6UKVPMypUrMyihkuNkvYVSnVWSRHVWyTV8qbM7axo465alRGKG20N3MTP4CgANJp9vhK9leeEZnDwoj+mjQ0waGMzeB20sTPHetQSbajABmxuqZPdKSncuIxBtAMBIkG3n3E7tuIuzI2MXONIUYUdNA7UNYQ43RqhrChON2e9Q73O0IBSkKGRHSApDQYrc38JQgMK8IAWhAJGoIRyN0RiOceBIE3vrmthzuIk9h5rYfbiRPYca2Xu4mWgn37gBgVtmncSsyQkT2WRdZ/syuWKg5AHvANOBHcBrwJXGmPXtnL8XeD9zEgJQDezLcJmpJtfr0F359xljzk+1MMmQJZ3NBLmuU+mip/elt+msn/TET7KAv+TpiSy5oLN+utddIZfkzUVZs66zfZmcMFAARORC4DZsmOEHjDE3Z1mkVojISmPMlGzL0RNyvQ65Ln9vRJ9JYvS+tMZP98NPsoC/5PGTLOkg1+qXS/KqrEqy5MwcFGPMn4A/ZVsORVEURVEURVHSh6bwVRRFURRFURTFN6iBkjp+kW0BUkCu1yHX5e+N6DNJjN6X1vjpfvhJFvCXPH6SJR3kWv1ySV6VVUmKnJmDoiiKoiiKoihK70dHUBRFURRFURRF8Q1qoHQDEXlARPaIyLq4fZUislhE3nV/K7IpY0eIyAgRWSoib4nIehG5we3PpToUisgKEVnj6vB9t3+MiCwXkY0i8qiI5Gdb1r5Cb9CrdCEiQRFZJSLPuO0+q6ciskVE3hSR1SKy0u3LmI4k036L5Q73nNaKyCkZkGWeiOxw92e1i2DpHfumk+VtETkvxbIk9f6m+95kGhE5393XjSJyU5ZkSIluisgcd/67IjInTbKmTF/SLa8k+b0gIgVue6M7PjruWml7B5U2GGN0SXIBzgJOAdbF7bsFuMmt3wT8ONtydiD/EOAUt16GzTFzfI7VQYBStx4ClgOnAb8HrnD77wWuy7asfWXpDXqVxnvzFeC3wDNuu8/qKbAFqG6zL2M6kkz7DVwI/J9rb04DlmdAlnnA1xKcezywBigAxgDvAcEUypLU+5vue5NhnQy6+zkWyHf3+fgsyNFj3QQqgU3ub4Vbr0iDrCnRl0zIS5LfC8AXgHvd+hXAo249re+gLq0XHUHpBsaYF4EDbXbPBH7t1n8NXJJRoZLAGLPLGPOGWz8MbACGkVt1MMaYOrcZcosBPg485vb7ug69jd6gV+lARIYDFwH3u21B9bQtGdORJNvvmcBvXHvzKlAuIkPSLEt7zAR+Z4xpMsZsBjYCp6ZQlmTf37TemwxzKrDRGLPJGNMM/A5bv4ySIt08D1hsjDlgjDkILAZSnmwwhfqSdnm78b0QX4fHgOmu3U7rO6i0Rg2U1DHIGLPLrX8ADMqmMF3FDV2ejO1RyKk6iHWbWQ3swTZq7wE1xpiIO2U7tsFUMkwu61UauA34BhBz21X0bT01wF9E5HURudbty7aOtFf+MGBb3HmZelbXOzeYB6TF3S1jsnTx/c3WvUkHfq5Lsvc/43Xpob5kRN4kvxc+lMkdr8W2237Wk16HGihpwBhjsP+EfY2IlAKPAzcaYw7FH8uFOhhjosaYScBwbC/GhCyLpJD7epVKROQfgD3GmNezLYuPONMYcwpwAfBFETkr/mC2dSTb5QM/B8YBk4BdwK2ZLFzfX//ix/ufK/qi3wu5hxooqWO3N7zt/u7JsjwdIiIhbKMy3xjzhNudU3XwMMbUAEuB07HDxnnu0HBgR9YE64P0Jr1KEWcAF4vIFqzbyMeB2+nDemqM2eH+7gGexH4sZFtH2it/BzAi7ry0PytjzG73MRUD7qPFhSTtsiT5/mb83qQRP9cl2fufsbqkSF8yeu+7+L3woUzueH9gf6Zl7euogZI6nga86BNzgKeyKEuHOF/KXwIbjDE/jTuUS3UYICLlbr0I+ATWB3YpMMud5us69DZ6g16lGmPMN40xw40xo7GTLZ8zxlxFH9VTESkRkTJvHTgXWEf2daS98p8GrnYRiE4DauPcV9JCm3kcl2LvjyfLFS7C0BhgPLAiheUm+/5m/N6kkdeA8S6qUz72XX06yzJ5JHv/FwHnikiFcw881+1LKSnUl7TL243vhfg6zMK224Y0v4NKG7oyk16X1gvwCHboPYz1QbwG65/4LPAusASozLacHch/JnbYdS2w2i0X5lgdPgqscnVYB3zX7R+LbTA2AguAgmzL2leW3qBXab4/59ASxatP6qmr9xq3rAe+5fZnTEeSab+x0X/uxvqrvwlMyYAsD7my1mI/iIbEnf8tJ8vbwAUpliWp9zfd9yYLunkhNhLVe55eZkGGlOgm8C+ubdkI/HOaZE2ZvqRbXpL8XgAK3fZGd3xs3LXS9g7q0nrRTPKKoiiKoiiKovgGdfFSFEVRFEVRFMU3qIGiKIqiKIqiKIpvUANFURRFURRFURTfoAaKoiiKoiiKoii+QQ0URVEURVEURVF8gxooSpcQkS0iUu3WX8m2PErvRkTmisjQHvz+RhEpTqVMipIKRGSoiDyWomtdIiLHp+JaiqIofkINlD5MXAbVpDDGTEu1LIrShrlAtw0U4EYgKQOlu++DonQVEckzxuw0xszq/OwucQmQlIGieq6kCtUlJZ2ogZJhRGS0iPxNRB4UkXdEZL6IzBCRl0XkXRE51Z1XIiIPiMgKEVklIjPjfv+SiLzhlmlu/zki8ryIPOauP99lem1b/vMicpuIrARuEJFPishyV8YSERnkzqsSkb+IyHoRuR+bZMm7Rl1cmc/E7b9LROa69R+JyFsislZE/idtN1RJC9nUUxGZBUwB5ovIahEpEpHJIvKCiLwuIotEZIiI5InIayJyjvvdf4vIzSLyZaxxs1RElrpjdfHXF5EH3fqDInKviCwHbmmvPop/8EkbervTzXVdKG+uiDwtIs8Bz7ry18UdWygii8WOUl8vIl9xv39VRCrdeeNE5M9O/18SkQlO7ouBnzhZxiU6z/2+lZ6n+xkpPccHev45176uEZHHxY1Id7XNbK98Reky2c4U2dcWYDQQAU7EGoivAw9gDYCZwEJ33n8Bn3br5dgMtyXYXuFCt388sNKtnwPUAsPddZcBZyYo/3ngnrjtCvgwYedngVvd+h20ZFu9CJsxttpt18WV+Uzcte7C9nxXYbOsetctz/Z91yUn9XSKWw8BrwAD3PZs4AG3fgKwAZiBzRSc7/Zv8fQ1Xmfd+izgQbf+IPAMEOyoPtl+Hrr4Tjfvc+tnAes6KW8uNit4ZZz83m/mYrNVlwEDXPmfd8d+Btzo1p8Fxrv1qcBzcfo7K062js77UM918f/iAz2vilv/IfClRLqUbPm66NLVRYfnssNmY8ybACKyHnjWGGNE5E1sowRwLnCxiHzNbRcCI4GdwF0iMgmIAsfEXXeFMWa7u+5qd62/Jij/0bj14cCjIjIEyAc2u/1nAZcBGGP+KCIHk6hfLdAI/FLsCMsznZyv+JNs66nHscBEYLHr6AsCuwCMMetF5CGsjp1ujGnuRj0XGGOindRnQzeuq6SPbOvmIwDGmBdFpJ+IlHdQHsBiY8yBduqy1BhzGDgsIrXAH9z+N4GPikgpMA1YENfRXdD2Il04L17Pldwgm3o+UUR+iDU6SoFFcce60mZ2VL6idIoaKNmhKW49Frcdo+WZCPCPxpi3438oIvOA3cBJ2N6PxnauG6X953skbv1O4KfGmKfFusrM62olsL078W6ChQDGmIgbfp6O7a2+Hvh4EtdV/EG29fTDywHrjTGnt3P8RKAGGNjBNUzcemGbY/HvQ8L6KL4j27ppEmy3V95UWutYsnUJADXGmEkdXIMunNeRDIo/yaaePwhcYoxZI9Z1+5y4Y522mZ2UryidonNQ/Msi4Eueb6iInOz29wd2GWNiwGewvck9oT+ww63Pidv/InClK/sCrCtYW94HjheRAteDON2dXwr0N8b8Cfg3bAOl9E7SpaeHsW4vYN0FB4jI6a6MkIic4NYvAyqxI353Oj1s+3uA3SJynIgEgEu7UR8l90hnGzrbXfNMoNYYU9tBeT3CGHMI2Cwil7vrioh4beqHet7JeUrvJV16XgbsEpEQcFUWylf6OGqg+Jf/xPrer3VDu//p9t8DzBGRNcAEet4rNg/rEvA6sC9u//eBs1zZlwFb2/7QGLMN+D2wzv1d5Q6VAc+IyFrssPFXeiij4l/SpacPAvc694MgdiTux+56q4FpYsNe/wj4rDHmHewcqNvd738B/FncJHngJqwb2Cs497Ak66PkHulsQxtFZBVwL3BNJ+WlgquAa5zM67FzEAB+B3zdTU4e18F5Su8lXXr+HWA58DLwtyyUr/RxvEnMiqIoiqJ0gog8D3zNGLMy27IoiqL0VnQERVEURVEURVEU36AjKIqiKIqiKIqi+AYdQVEURVEURVEUxTeogaIoiqIoiqIoim9QA0VRFEVRFEVRFN+gBoqiKIqiKIqiKL5BDRRFURRFURRFUXyDGiiKoiiKoiiKoviG/wcQ8czZgtRLDAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 806.625x720 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.pairplot(vars=[\"mean radius\", \"mean texture\", \"mean perimeter\", \"mean area\"], \n",
    "             hue=\"class\", data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.values\n",
    "X = x[:, 0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(90, input_dim=4, activation='relu'))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = \"./model/model.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, \n",
    "                               monitor=\"val_loss\", \n",
    "                               verbose=1, \n",
    "                               save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 318 samples, validate on 80 samples\n",
      "Epoch 1/2000\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 5.8656 - acc: 0.6321 - val_loss: 5.7007 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.70066, saving model to ./model/model.hdf5\n",
      "Epoch 2/2000\n",
      "318/318 [==============================] - 0s 11us/step - loss: 5.1390 - acc: 0.6321 - val_loss: 1.0710 - val_acc: 0.4625\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.70066 to 1.07100, saving model to ./model/model.hdf5\n",
      "Epoch 3/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 1.0749 - acc: 0.4308 - val_loss: 1.0676 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.07100 to 1.06759, saving model to ./model/model.hdf5\n",
      "Epoch 4/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.9786 - acc: 0.6321 - val_loss: 0.6017 - val_acc: 0.6125\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.06759 to 0.60171, saving model to ./model/model.hdf5\n",
      "Epoch 5/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.6054 - acc: 0.5912 - val_loss: 0.4383 - val_acc: 0.8375\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.60171 to 0.43829, saving model to ./model/model.hdf5\n",
      "Epoch 6/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.4356 - acc: 0.8396 - val_loss: 0.4987 - val_acc: 0.7375\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.43829\n",
      "Epoch 7/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.4852 - acc: 0.7704 - val_loss: 0.6170 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.43829\n",
      "Epoch 8/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.6164 - acc: 0.6226 - val_loss: 0.4037 - val_acc: 0.8375\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.43829 to 0.40369, saving model to ./model/model.hdf5\n",
      "Epoch 9/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.4093 - acc: 0.8302 - val_loss: 0.6893 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.40369\n",
      "Epoch 10/2000\n",
      "318/318 [==============================] - 0s 10us/step - loss: 0.6562 - acc: 0.6572 - val_loss: 0.4410 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.40369\n",
      "Epoch 11/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.4441 - acc: 0.7767 - val_loss: 0.5569 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.40369\n",
      "Epoch 12/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.5493 - acc: 0.6855 - val_loss: 0.3516 - val_acc: 0.8250\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.40369 to 0.35156, saving model to ./model/model.hdf5\n",
      "Epoch 13/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3621 - acc: 0.8585 - val_loss: 0.5493 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.35156\n",
      "Epoch 14/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.5419 - acc: 0.7296 - val_loss: 0.3419 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.35156 to 0.34191, saving model to ./model/model.hdf5\n",
      "Epoch 15/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3537 - acc: 0.8585 - val_loss: 0.4622 - val_acc: 0.7625\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.34191\n",
      "Epoch 16/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.4502 - acc: 0.7830 - val_loss: 0.4166 - val_acc: 0.7875\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.34191\n",
      "Epoch 17/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.4094 - acc: 0.8019 - val_loss: 0.3435 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.34191\n",
      "Epoch 18/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.3579 - acc: 0.8522 - val_loss: 0.4250 - val_acc: 0.8375\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.34191\n",
      "Epoch 19/2000\n",
      "318/318 [==============================] - 0s 50us/step - loss: 0.4348 - acc: 0.8396 - val_loss: 0.3429 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.34191\n",
      "Epoch 20/2000\n",
      "318/318 [==============================] - 0s 35us/step - loss: 0.3629 - acc: 0.8491 - val_loss: 0.3669 - val_acc: 0.7875\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.34191\n",
      "Epoch 21/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3783 - acc: 0.8333 - val_loss: 0.4013 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.34191\n",
      "Epoch 22/2000\n",
      "318/318 [==============================] - 0s 33us/step - loss: 0.4072 - acc: 0.8176 - val_loss: 0.3384 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.34191 to 0.33841, saving model to ./model/model.hdf5\n",
      "Epoch 23/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3620 - acc: 0.8553 - val_loss: 0.3490 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.33841\n",
      "Epoch 24/2000\n",
      "318/318 [==============================] - 0s 41us/step - loss: 0.3758 - acc: 0.8522 - val_loss: 0.3701 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.33841\n",
      "Epoch 25/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3945 - acc: 0.8491 - val_loss: 0.3297 - val_acc: 0.8375\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.33841 to 0.32973, saving model to ./model/model.hdf5\n",
      "Epoch 26/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3602 - acc: 0.8522 - val_loss: 0.3494 - val_acc: 0.8250\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.32973\n",
      "Epoch 27/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3732 - acc: 0.8396 - val_loss: 0.3635 - val_acc: 0.7875\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.32973\n",
      "Epoch 28/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.3841 - acc: 0.8365 - val_loss: 0.3286 - val_acc: 0.8250\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.32973 to 0.32864, saving model to ./model/model.hdf5\n",
      "Epoch 29/2000\n",
      "318/318 [==============================] - 0s 37us/step - loss: 0.3586 - acc: 0.8522 - val_loss: 0.3391 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.32864\n",
      "Epoch 30/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3690 - acc: 0.8491 - val_loss: 0.3463 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.32864\n",
      "Epoch 31/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3748 - acc: 0.8522 - val_loss: 0.3245 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.32864 to 0.32446, saving model to ./model/model.hdf5\n",
      "Epoch 32/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.3553 - acc: 0.8553 - val_loss: 0.3400 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.32446\n",
      "Epoch 33/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.3655 - acc: 0.8491 - val_loss: 0.3409 - val_acc: 0.8250\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.32446\n",
      "Epoch 34/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3655 - acc: 0.8491 - val_loss: 0.3229 - val_acc: 0.8250\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.32446 to 0.32293, saving model to ./model/model.hdf5\n",
      "Epoch 35/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3517 - acc: 0.8522 - val_loss: 0.3351 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.32293\n",
      "Epoch 36/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3617 - acc: 0.8522 - val_loss: 0.3299 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.32293\n",
      "Epoch 37/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3563 - acc: 0.8522 - val_loss: 0.3242 - val_acc: 0.8250\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.32293\n",
      "Epoch 38/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3493 - acc: 0.8522 - val_loss: 0.3362 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.32293\n",
      "Epoch 39/2000\n",
      "318/318 [==============================] - 0s 45us/step - loss: 0.3571 - acc: 0.8553 - val_loss: 0.3254 - val_acc: 0.8250\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.32293\n",
      "Epoch 40/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3481 - acc: 0.8585 - val_loss: 0.3253 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.32293\n",
      "Epoch 41/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3485 - acc: 0.8553 - val_loss: 0.3286 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.32293\n",
      "Epoch 42/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.3508 - acc: 0.8553 - val_loss: 0.3207 - val_acc: 0.8250\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.32293 to 0.32066, saving model to ./model/model.hdf5\n",
      "Epoch 43/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3431 - acc: 0.8491 - val_loss: 0.3276 - val_acc: 0.8250\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.32066\n",
      "Epoch 44/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3482 - acc: 0.8553 - val_loss: 0.3216 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.32066\n",
      "Epoch 45/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.3437 - acc: 0.8616 - val_loss: 0.3200 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.32066 to 0.32002, saving model to ./model/model.hdf5\n",
      "Epoch 46/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3430 - acc: 0.8553 - val_loss: 0.3221 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.32002\n",
      "Epoch 47/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.3446 - acc: 0.8585 - val_loss: 0.3171 - val_acc: 0.8375\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.32002 to 0.31712, saving model to ./model/model.hdf5\n",
      "Epoch 48/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3398 - acc: 0.8585 - val_loss: 0.3218 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.31712\n",
      "Epoch 49/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3433 - acc: 0.8585 - val_loss: 0.3174 - val_acc: 0.8375\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.31712\n",
      "Epoch 50/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3394 - acc: 0.8585 - val_loss: 0.3199 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.31712\n",
      "Epoch 51/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3410 - acc: 0.8585 - val_loss: 0.3190 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.31712\n",
      "Epoch 52/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.3400 - acc: 0.8616 - val_loss: 0.3177 - val_acc: 0.8375\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.31712\n",
      "Epoch 53/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3390 - acc: 0.8553 - val_loss: 0.3190 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.31712\n",
      "Epoch 54/2000\n",
      "318/318 [==============================] - 0s 41us/step - loss: 0.3401 - acc: 0.8585 - val_loss: 0.3167 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.31712 to 0.31668, saving model to ./model/model.hdf5\n",
      "Epoch 55/2000\n",
      "318/318 [==============================] - 0s 35us/step - loss: 0.3378 - acc: 0.8616 - val_loss: 0.3193 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.31668\n",
      "Epoch 56/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3397 - acc: 0.8616 - val_loss: 0.3164 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.31668 to 0.31639, saving model to ./model/model.hdf5\n",
      "Epoch 57/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.3374 - acc: 0.8616 - val_loss: 0.3178 - val_acc: 0.8375\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.31639\n",
      "Epoch 58/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3389 - acc: 0.8585 - val_loss: 0.3163 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.31639 to 0.31630, saving model to ./model/model.hdf5\n",
      "Epoch 59/2000\n",
      "318/318 [==============================] - 0s 12us/step - loss: 0.3372 - acc: 0.8585 - val_loss: 0.3178 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.31630\n",
      "Epoch 60/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.3380 - acc: 0.8679 - val_loss: 0.3167 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.31630\n",
      "Epoch 61/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3371 - acc: 0.8679 - val_loss: 0.3162 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.31630 to 0.31620, saving model to ./model/model.hdf5\n",
      "Epoch 62/2000\n",
      "318/318 [==============================] - 0s 11us/step - loss: 0.3371 - acc: 0.8616 - val_loss: 0.3159 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.31620 to 0.31591, saving model to ./model/model.hdf5\n",
      "Epoch 63/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3369 - acc: 0.8616 - val_loss: 0.3159 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.31591 to 0.31585, saving model to ./model/model.hdf5\n",
      "Epoch 64/2000\n",
      "318/318 [==============================] - 0s 11us/step - loss: 0.3364 - acc: 0.8679 - val_loss: 0.3160 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.31585\n",
      "Epoch 65/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3365 - acc: 0.8711 - val_loss: 0.3147 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.31585 to 0.31470, saving model to ./model/model.hdf5\n",
      "Epoch 66/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3358 - acc: 0.8616 - val_loss: 0.3148 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.31470\n",
      "Epoch 67/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3361 - acc: 0.8585 - val_loss: 0.3142 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.31470 to 0.31416, saving model to ./model/model.hdf5\n",
      "Epoch 68/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3353 - acc: 0.8679 - val_loss: 0.3146 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.31416\n",
      "Epoch 69/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3356 - acc: 0.8711 - val_loss: 0.3133 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.31416 to 0.31326, saving model to ./model/model.hdf5\n",
      "Epoch 70/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3349 - acc: 0.8616 - val_loss: 0.3132 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.31326 to 0.31324, saving model to ./model/model.hdf5\n",
      "Epoch 71/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3351 - acc: 0.8585 - val_loss: 0.3126 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.31324 to 0.31255, saving model to ./model/model.hdf5\n",
      "Epoch 72/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3344 - acc: 0.8616 - val_loss: 0.3128 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.31255\n",
      "Epoch 73/2000\n",
      "318/318 [==============================] - 0s 11us/step - loss: 0.3346 - acc: 0.8711 - val_loss: 0.3117 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.31255 to 0.31175, saving model to ./model/model.hdf5\n",
      "Epoch 74/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3340 - acc: 0.8616 - val_loss: 0.3116 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.31175 to 0.31155, saving model to ./model/model.hdf5\n",
      "Epoch 75/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3341 - acc: 0.8585 - val_loss: 0.3109 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.31155 to 0.31093, saving model to ./model/model.hdf5\n",
      "Epoch 76/2000\n",
      "318/318 [==============================] - 0s 45us/step - loss: 0.3336 - acc: 0.8585 - val_loss: 0.3110 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.31093\n",
      "Epoch 77/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3336 - acc: 0.8711 - val_loss: 0.3102 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.31093 to 0.31015, saving model to ./model/model.hdf5\n",
      "Epoch 78/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3332 - acc: 0.8585 - val_loss: 0.3099 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.31015 to 0.30986, saving model to ./model/model.hdf5\n",
      "Epoch 79/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3332 - acc: 0.8585 - val_loss: 0.3093 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.30986 to 0.30933, saving model to ./model/model.hdf5\n",
      "Epoch 80/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.3328 - acc: 0.8585 - val_loss: 0.3093 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.30933 to 0.30927, saving model to ./model/model.hdf5\n",
      "Epoch 81/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3328 - acc: 0.8711 - val_loss: 0.3086 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.30927 to 0.30860, saving model to ./model/model.hdf5\n",
      "Epoch 82/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3324 - acc: 0.8585 - val_loss: 0.3083 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.30860 to 0.30828, saving model to ./model/model.hdf5\n",
      "Epoch 83/2000\n",
      "318/318 [==============================] - 0s 30us/step - loss: 0.3324 - acc: 0.8585 - val_loss: 0.3078 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.30828 to 0.30781, saving model to ./model/model.hdf5\n",
      "Epoch 84/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3320 - acc: 0.8585 - val_loss: 0.3077 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.30781 to 0.30765, saving model to ./model/model.hdf5\n",
      "Epoch 85/2000\n",
      "318/318 [==============================] - 0s 11us/step - loss: 0.3320 - acc: 0.8679 - val_loss: 0.3071 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.30765 to 0.30713, saving model to ./model/model.hdf5\n",
      "Epoch 86/2000\n",
      "318/318 [==============================] - 0s 36us/step - loss: 0.3317 - acc: 0.8585 - val_loss: 0.3069 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.30713 to 0.30689, saving model to ./model/model.hdf5\n",
      "Epoch 87/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3316 - acc: 0.8585 - val_loss: 0.3066 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.30689 to 0.30656, saving model to ./model/model.hdf5\n",
      "Epoch 88/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3313 - acc: 0.8585 - val_loss: 0.3064 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.30656 to 0.30644, saving model to ./model/model.hdf5\n",
      "Epoch 89/2000\n",
      "318/318 [==============================] - 0s 10us/step - loss: 0.3312 - acc: 0.8679 - val_loss: 0.3060 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.30644 to 0.30602, saving model to ./model/model.hdf5\n",
      "Epoch 90/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3310 - acc: 0.8585 - val_loss: 0.3058 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.30602 to 0.30579, saving model to ./model/model.hdf5\n",
      "Epoch 91/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.3309 - acc: 0.8585 - val_loss: 0.3055 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.30579 to 0.30549, saving model to ./model/model.hdf5\n",
      "Epoch 92/2000\n",
      "318/318 [==============================] - 0s 12us/step - loss: 0.3306 - acc: 0.8585 - val_loss: 0.3053 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.30549 to 0.30532, saving model to ./model/model.hdf5\n",
      "Epoch 93/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3305 - acc: 0.8679 - val_loss: 0.3050 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.30532 to 0.30496, saving model to ./model/model.hdf5\n",
      "Epoch 94/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3303 - acc: 0.8585 - val_loss: 0.3047 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.30496 to 0.30474, saving model to ./model/model.hdf5\n",
      "Epoch 95/2000\n",
      "318/318 [==============================] - 0s 10us/step - loss: 0.3302 - acc: 0.8585 - val_loss: 0.3045 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.30474 to 0.30447, saving model to ./model/model.hdf5\n",
      "Epoch 96/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3300 - acc: 0.8648 - val_loss: 0.3043 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.30447 to 0.30428, saving model to ./model/model.hdf5\n",
      "Epoch 97/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3298 - acc: 0.8679 - val_loss: 0.3040 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.30428 to 0.30397, saving model to ./model/model.hdf5\n",
      "Epoch 98/2000\n",
      "318/318 [==============================] - 0s 43us/step - loss: 0.3296 - acc: 0.8616 - val_loss: 0.3037 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.30397 to 0.30373, saving model to ./model/model.hdf5\n",
      "Epoch 99/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3295 - acc: 0.8616 - val_loss: 0.3035 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.30373 to 0.30351, saving model to ./model/model.hdf5\n",
      "Epoch 100/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3293 - acc: 0.8679 - val_loss: 0.3033 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.30351 to 0.30328, saving model to ./model/model.hdf5\n",
      "Epoch 101/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3291 - acc: 0.8711 - val_loss: 0.3030 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.30328 to 0.30301, saving model to ./model/model.hdf5\n",
      "Epoch 102/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3290 - acc: 0.8616 - val_loss: 0.3028 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.30301 to 0.30277, saving model to ./model/model.hdf5\n",
      "Epoch 103/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3288 - acc: 0.8648 - val_loss: 0.3026 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.30277 to 0.30257, saving model to ./model/model.hdf5\n",
      "Epoch 104/2000\n",
      "318/318 [==============================] - 0s 11us/step - loss: 0.3287 - acc: 0.8711 - val_loss: 0.3023 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.30257 to 0.30232, saving model to ./model/model.hdf5\n",
      "Epoch 105/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3285 - acc: 0.8679 - val_loss: 0.3021 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.30232 to 0.30207, saving model to ./model/model.hdf5\n",
      "Epoch 106/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3284 - acc: 0.8648 - val_loss: 0.3018 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.30207 to 0.30184, saving model to ./model/model.hdf5\n",
      "Epoch 107/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3282 - acc: 0.8648 - val_loss: 0.3016 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.30184 to 0.30164, saving model to ./model/model.hdf5\n",
      "Epoch 108/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3281 - acc: 0.8711 - val_loss: 0.3014 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.30164 to 0.30138, saving model to ./model/model.hdf5\n",
      "Epoch 109/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3279 - acc: 0.8679 - val_loss: 0.3012 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.30138 to 0.30115, saving model to ./model/model.hdf5\n",
      "Epoch 110/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3278 - acc: 0.8648 - val_loss: 0.3009 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.30115 to 0.30093, saving model to ./model/model.hdf5\n",
      "Epoch 111/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3276 - acc: 0.8679 - val_loss: 0.3007 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.30093 to 0.30072, saving model to ./model/model.hdf5\n",
      "Epoch 112/2000\n",
      "318/318 [==============================] - 0s 11us/step - loss: 0.3275 - acc: 0.8679 - val_loss: 0.3005 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.30072 to 0.30047, saving model to ./model/model.hdf5\n",
      "Epoch 113/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3273 - acc: 0.8679 - val_loss: 0.3002 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.30047 to 0.30025, saving model to ./model/model.hdf5\n",
      "Epoch 114/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3272 - acc: 0.8679 - val_loss: 0.3000 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.30025 to 0.30004, saving model to ./model/model.hdf5\n",
      "Epoch 115/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3270 - acc: 0.8679 - val_loss: 0.2998 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.30004 to 0.29980, saving model to ./model/model.hdf5\n",
      "Epoch 116/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3269 - acc: 0.8679 - val_loss: 0.2996 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.29980 to 0.29957, saving model to ./model/model.hdf5\n",
      "Epoch 117/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3268 - acc: 0.8679 - val_loss: 0.2994 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.29957 to 0.29935, saving model to ./model/model.hdf5\n",
      "Epoch 118/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3266 - acc: 0.8679 - val_loss: 0.2991 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.29935 to 0.29914, saving model to ./model/model.hdf5\n",
      "Epoch 119/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3265 - acc: 0.8679 - val_loss: 0.2989 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.29914 to 0.29890, saving model to ./model/model.hdf5\n",
      "Epoch 120/2000\n",
      "318/318 [==============================] - 0s 11us/step - loss: 0.3263 - acc: 0.8679 - val_loss: 0.2987 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.29890 to 0.29868, saving model to ./model/model.hdf5\n",
      "Epoch 121/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3262 - acc: 0.8679 - val_loss: 0.2985 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.29868 to 0.29846, saving model to ./model/model.hdf5\n",
      "Epoch 122/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3261 - acc: 0.8711 - val_loss: 0.2982 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.29846 to 0.29824, saving model to ./model/model.hdf5\n",
      "Epoch 123/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3260 - acc: 0.8711 - val_loss: 0.2980 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.29824 to 0.29801, saving model to ./model/model.hdf5\n",
      "Epoch 124/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3258 - acc: 0.8679 - val_loss: 0.2978 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.29801 to 0.29780, saving model to ./model/model.hdf5\n",
      "Epoch 125/2000\n",
      "318/318 [==============================] - 0s 11us/step - loss: 0.3257 - acc: 0.8711 - val_loss: 0.2976 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.29780 to 0.29758, saving model to ./model/model.hdf5\n",
      "Epoch 126/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3256 - acc: 0.8711 - val_loss: 0.2974 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.29758 to 0.29736, saving model to ./model/model.hdf5\n",
      "Epoch 127/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3254 - acc: 0.8711 - val_loss: 0.2971 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.29736 to 0.29714, saving model to ./model/model.hdf5\n",
      "Epoch 128/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3253 - acc: 0.8711 - val_loss: 0.2969 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.29714 to 0.29693, saving model to ./model/model.hdf5\n",
      "Epoch 129/2000\n",
      "318/318 [==============================] - 0s 10us/step - loss: 0.3252 - acc: 0.8711 - val_loss: 0.2967 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.29693 to 0.29671, saving model to ./model/model.hdf5\n",
      "Epoch 130/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3251 - acc: 0.8711 - val_loss: 0.2965 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.29671 to 0.29649, saving model to ./model/model.hdf5\n",
      "Epoch 131/2000\n",
      "318/318 [==============================] - 0s 41us/step - loss: 0.3250 - acc: 0.8711 - val_loss: 0.2963 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.29649 to 0.29628, saving model to ./model/model.hdf5\n",
      "Epoch 132/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3248 - acc: 0.8711 - val_loss: 0.2961 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.29628 to 0.29607, saving model to ./model/model.hdf5\n",
      "Epoch 133/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3247 - acc: 0.8711 - val_loss: 0.2959 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.29607 to 0.29586, saving model to ./model/model.hdf5\n",
      "Epoch 134/2000\n",
      "318/318 [==============================] - 0s 12us/step - loss: 0.3246 - acc: 0.8711 - val_loss: 0.2957 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.29586 to 0.29565, saving model to ./model/model.hdf5\n",
      "Epoch 135/2000\n",
      "318/318 [==============================] - 0s 30us/step - loss: 0.3245 - acc: 0.8711 - val_loss: 0.2954 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.29565 to 0.29545, saving model to ./model/model.hdf5\n",
      "Epoch 136/2000\n",
      "318/318 [==============================] - 0s 12us/step - loss: 0.3244 - acc: 0.8711 - val_loss: 0.2952 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.29545 to 0.29524, saving model to ./model/model.hdf5\n",
      "Epoch 137/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3243 - acc: 0.8711 - val_loss: 0.2950 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.29524 to 0.29504, saving model to ./model/model.hdf5\n",
      "Epoch 138/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3242 - acc: 0.8711 - val_loss: 0.2948 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.29504 to 0.29483, saving model to ./model/model.hdf5\n",
      "Epoch 139/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3240 - acc: 0.8711 - val_loss: 0.2946 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.29483 to 0.29463, saving model to ./model/model.hdf5\n",
      "Epoch 140/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.3239 - acc: 0.8711 - val_loss: 0.2944 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.29463 to 0.29443, saving model to ./model/model.hdf5\n",
      "Epoch 141/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3238 - acc: 0.8711 - val_loss: 0.2942 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.29443 to 0.29423, saving model to ./model/model.hdf5\n",
      "Epoch 142/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3237 - acc: 0.8711 - val_loss: 0.2940 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.29423 to 0.29403, saving model to ./model/model.hdf5\n",
      "Epoch 143/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.3236 - acc: 0.8711 - val_loss: 0.2938 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.29403 to 0.29382, saving model to ./model/model.hdf5\n",
      "Epoch 144/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3235 - acc: 0.8711 - val_loss: 0.2936 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.29382 to 0.29362, saving model to ./model/model.hdf5\n",
      "Epoch 145/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3234 - acc: 0.8711 - val_loss: 0.2934 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.29362 to 0.29341, saving model to ./model/model.hdf5\n",
      "Epoch 146/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.3233 - acc: 0.8711 - val_loss: 0.2932 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.29341 to 0.29320, saving model to ./model/model.hdf5\n",
      "Epoch 147/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3232 - acc: 0.8711 - val_loss: 0.2930 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.29320 to 0.29297, saving model to ./model/model.hdf5\n",
      "Epoch 148/2000\n",
      "318/318 [==============================] - 0s 12us/step - loss: 0.3230 - acc: 0.8711 - val_loss: 0.2927 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.29297 to 0.29272, saving model to ./model/model.hdf5\n",
      "Epoch 149/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3229 - acc: 0.8711 - val_loss: 0.2925 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.29272 to 0.29248, saving model to ./model/model.hdf5\n",
      "Epoch 150/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3228 - acc: 0.8679 - val_loss: 0.2922 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.29248 to 0.29224, saving model to ./model/model.hdf5\n",
      "Epoch 151/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3227 - acc: 0.8679 - val_loss: 0.2920 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.29224 to 0.29199, saving model to ./model/model.hdf5\n",
      "Epoch 152/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3226 - acc: 0.8679 - val_loss: 0.2918 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.29199 to 0.29175, saving model to ./model/model.hdf5\n",
      "Epoch 153/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3225 - acc: 0.8679 - val_loss: 0.2915 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.29175 to 0.29152, saving model to ./model/model.hdf5\n",
      "Epoch 154/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.3224 - acc: 0.8679 - val_loss: 0.2913 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.29152 to 0.29131, saving model to ./model/model.hdf5\n",
      "Epoch 155/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3223 - acc: 0.8679 - val_loss: 0.2911 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.29131 to 0.29112, saving model to ./model/model.hdf5\n",
      "Epoch 156/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3221 - acc: 0.8679 - val_loss: 0.2909 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.29112 to 0.29095, saving model to ./model/model.hdf5\n",
      "Epoch 157/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3220 - acc: 0.8711 - val_loss: 0.2908 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.29095 to 0.29078, saving model to ./model/model.hdf5\n",
      "Epoch 158/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3219 - acc: 0.8711 - val_loss: 0.2906 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.29078 to 0.29060, saving model to ./model/model.hdf5\n",
      "Epoch 159/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3218 - acc: 0.8711 - val_loss: 0.2904 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.29060 to 0.29044, saving model to ./model/model.hdf5\n",
      "Epoch 160/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3217 - acc: 0.8711 - val_loss: 0.2903 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.29044 to 0.29028, saving model to ./model/model.hdf5\n",
      "Epoch 161/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.3216 - acc: 0.8711 - val_loss: 0.2901 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.29028 to 0.29013, saving model to ./model/model.hdf5\n",
      "Epoch 162/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3215 - acc: 0.8711 - val_loss: 0.2900 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.29013 to 0.28999, saving model to ./model/model.hdf5\n",
      "Epoch 163/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3214 - acc: 0.8711 - val_loss: 0.2899 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.28999 to 0.28986, saving model to ./model/model.hdf5\n",
      "Epoch 164/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3213 - acc: 0.8711 - val_loss: 0.2897 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.28986 to 0.28973, saving model to ./model/model.hdf5\n",
      "Epoch 165/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3211 - acc: 0.8711 - val_loss: 0.2896 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.28973 to 0.28960, saving model to ./model/model.hdf5\n",
      "Epoch 166/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3210 - acc: 0.8711 - val_loss: 0.2895 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.28960 to 0.28947, saving model to ./model/model.hdf5\n",
      "Epoch 167/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.3209 - acc: 0.8679 - val_loss: 0.2893 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.28947 to 0.28934, saving model to ./model/model.hdf5\n",
      "Epoch 168/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3208 - acc: 0.8679 - val_loss: 0.2892 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.28934 to 0.28920, saving model to ./model/model.hdf5\n",
      "Epoch 169/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3206 - acc: 0.8679 - val_loss: 0.2891 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.28920 to 0.28907, saving model to ./model/model.hdf5\n",
      "Epoch 170/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3205 - acc: 0.8679 - val_loss: 0.2889 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.28907 to 0.28892, saving model to ./model/model.hdf5\n",
      "Epoch 171/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3203 - acc: 0.8679 - val_loss: 0.2888 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.28892 to 0.28877, saving model to ./model/model.hdf5\n",
      "Epoch 172/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3201 - acc: 0.8679 - val_loss: 0.2886 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.28877 to 0.28863, saving model to ./model/model.hdf5\n",
      "Epoch 173/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3199 - acc: 0.8679 - val_loss: 0.2884 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.28863 to 0.28841, saving model to ./model/model.hdf5\n",
      "Epoch 174/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3197 - acc: 0.8679 - val_loss: 0.2881 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.28841 to 0.28813, saving model to ./model/model.hdf5\n",
      "Epoch 175/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3195 - acc: 0.8679 - val_loss: 0.2878 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.28813 to 0.28785, saving model to ./model/model.hdf5\n",
      "Epoch 176/2000\n",
      "318/318 [==============================] - 0s 12us/step - loss: 0.3192 - acc: 0.8679 - val_loss: 0.2876 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.28785 to 0.28756, saving model to ./model/model.hdf5\n",
      "Epoch 177/2000\n",
      "318/318 [==============================] - 0s 43us/step - loss: 0.3189 - acc: 0.8679 - val_loss: 0.2873 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.28756 to 0.28726, saving model to ./model/model.hdf5\n",
      "Epoch 178/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3186 - acc: 0.8679 - val_loss: 0.2869 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.28726 to 0.28694, saving model to ./model/model.hdf5\n",
      "Epoch 179/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3183 - acc: 0.8711 - val_loss: 0.2866 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.28694 to 0.28661, saving model to ./model/model.hdf5\n",
      "Epoch 180/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3180 - acc: 0.8711 - val_loss: 0.2863 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.28661 to 0.28629, saving model to ./model/model.hdf5\n",
      "Epoch 181/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3178 - acc: 0.8742 - val_loss: 0.2860 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.28629 to 0.28599, saving model to ./model/model.hdf5\n",
      "Epoch 182/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3176 - acc: 0.8742 - val_loss: 0.2858 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.28599 to 0.28582, saving model to ./model/model.hdf5\n",
      "Epoch 183/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3173 - acc: 0.8742 - val_loss: 0.2857 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.28582 to 0.28574, saving model to ./model/model.hdf5\n",
      "Epoch 184/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3170 - acc: 0.8742 - val_loss: 0.2856 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.28574 to 0.28560, saving model to ./model/model.hdf5\n",
      "Epoch 185/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.3169 - acc: 0.8742 - val_loss: 0.2859 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.28560\n",
      "Epoch 186/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.3170 - acc: 0.8742 - val_loss: 0.2862 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.28560\n",
      "Epoch 187/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3170 - acc: 0.8742 - val_loss: 0.2861 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.28560\n",
      "Epoch 188/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3170 - acc: 0.8742 - val_loss: 0.2857 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.28560\n",
      "Epoch 189/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3168 - acc: 0.8742 - val_loss: 0.2852 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.28560 to 0.28518, saving model to ./model/model.hdf5\n",
      "Epoch 190/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.3165 - acc: 0.8742 - val_loss: 0.2845 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.28518 to 0.28455, saving model to ./model/model.hdf5\n",
      "Epoch 191/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3162 - acc: 0.8742 - val_loss: 0.2841 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.28455 to 0.28413, saving model to ./model/model.hdf5\n",
      "Epoch 192/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3160 - acc: 0.8742 - val_loss: 0.2840 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.28413 to 0.28396, saving model to ./model/model.hdf5\n",
      "Epoch 193/2000\n",
      "318/318 [==============================] - 0s 36us/step - loss: 0.3159 - acc: 0.8742 - val_loss: 0.2838 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.28396 to 0.28377, saving model to ./model/model.hdf5\n",
      "Epoch 194/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.3159 - acc: 0.8742 - val_loss: 0.2835 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.28377 to 0.28350, saving model to ./model/model.hdf5\n",
      "Epoch 195/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3158 - acc: 0.8742 - val_loss: 0.2833 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.28350 to 0.28332, saving model to ./model/model.hdf5\n",
      "Epoch 196/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3157 - acc: 0.8742 - val_loss: 0.2831 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.28332 to 0.28313, saving model to ./model/model.hdf5\n",
      "Epoch 197/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3156 - acc: 0.8742 - val_loss: 0.2829 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.28313 to 0.28288, saving model to ./model/model.hdf5\n",
      "Epoch 198/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3154 - acc: 0.8742 - val_loss: 0.2827 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.28288 to 0.28269, saving model to ./model/model.hdf5\n",
      "Epoch 199/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3152 - acc: 0.8742 - val_loss: 0.2824 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.28269 to 0.28244, saving model to ./model/model.hdf5\n",
      "Epoch 200/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.3150 - acc: 0.8742 - val_loss: 0.2823 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.28244 to 0.28230, saving model to ./model/model.hdf5\n",
      "Epoch 201/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3149 - acc: 0.8742 - val_loss: 0.2823 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.28230\n",
      "Epoch 202/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3149 - acc: 0.8742 - val_loss: 0.2822 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.28230 to 0.28223, saving model to ./model/model.hdf5\n",
      "Epoch 203/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3148 - acc: 0.8742 - val_loss: 0.2820 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.28223 to 0.28202, saving model to ./model/model.hdf5\n",
      "Epoch 204/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3146 - acc: 0.8742 - val_loss: 0.2819 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.28202 to 0.28185, saving model to ./model/model.hdf5\n",
      "Epoch 205/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.3145 - acc: 0.8774 - val_loss: 0.2816 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.28185 to 0.28155, saving model to ./model/model.hdf5\n",
      "Epoch 206/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3144 - acc: 0.8742 - val_loss: 0.2812 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.28155 to 0.28122, saving model to ./model/model.hdf5\n",
      "Epoch 207/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.3142 - acc: 0.8742 - val_loss: 0.2809 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.28122 to 0.28091, saving model to ./model/model.hdf5\n",
      "Epoch 208/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3141 - acc: 0.8742 - val_loss: 0.2806 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.28091 to 0.28057, saving model to ./model/model.hdf5\n",
      "Epoch 209/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3139 - acc: 0.8742 - val_loss: 0.2803 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.28057 to 0.28025, saving model to ./model/model.hdf5\n",
      "Epoch 210/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3138 - acc: 0.8742 - val_loss: 0.2801 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.28025 to 0.28012, saving model to ./model/model.hdf5\n",
      "Epoch 211/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3137 - acc: 0.8774 - val_loss: 0.2799 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00211: val_loss improved from 0.28012 to 0.27987, saving model to ./model/model.hdf5\n",
      "Epoch 212/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3136 - acc: 0.8742 - val_loss: 0.2797 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.27987 to 0.27969, saving model to ./model/model.hdf5\n",
      "Epoch 213/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.3135 - acc: 0.8774 - val_loss: 0.2795 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.27969 to 0.27954, saving model to ./model/model.hdf5\n",
      "Epoch 214/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3134 - acc: 0.8774 - val_loss: 0.2792 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.27954 to 0.27923, saving model to ./model/model.hdf5\n",
      "Epoch 215/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3133 - acc: 0.8742 - val_loss: 0.2791 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.27923 to 0.27905, saving model to ./model/model.hdf5\n",
      "Epoch 216/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3132 - acc: 0.8774 - val_loss: 0.2789 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.27905 to 0.27887, saving model to ./model/model.hdf5\n",
      "Epoch 217/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3130 - acc: 0.8774 - val_loss: 0.2787 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00217: val_loss improved from 0.27887 to 0.27867, saving model to ./model/model.hdf5\n",
      "Epoch 218/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3129 - acc: 0.8774 - val_loss: 0.2786 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.27867 to 0.27859, saving model to ./model/model.hdf5\n",
      "Epoch 219/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3128 - acc: 0.8774 - val_loss: 0.2784 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.27859 to 0.27843, saving model to ./model/model.hdf5\n",
      "Epoch 220/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.3127 - acc: 0.8774 - val_loss: 0.2782 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.27843 to 0.27822, saving model to ./model/model.hdf5\n",
      "Epoch 221/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3126 - acc: 0.8774 - val_loss: 0.2781 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.27822 to 0.27808, saving model to ./model/model.hdf5\n",
      "Epoch 222/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3125 - acc: 0.8774 - val_loss: 0.2778 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00222: val_loss improved from 0.27808 to 0.27780, saving model to ./model/model.hdf5\n",
      "Epoch 223/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3124 - acc: 0.8774 - val_loss: 0.2776 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00223: val_loss improved from 0.27780 to 0.27756, saving model to ./model/model.hdf5\n",
      "Epoch 224/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.3123 - acc: 0.8774 - val_loss: 0.2774 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.27756 to 0.27736, saving model to ./model/model.hdf5\n",
      "Epoch 225/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3122 - acc: 0.8774 - val_loss: 0.2771 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00225: val_loss improved from 0.27736 to 0.27707, saving model to ./model/model.hdf5\n",
      "Epoch 226/2000\n",
      "318/318 [==============================] - 0s 41us/step - loss: 0.3121 - acc: 0.8774 - val_loss: 0.2769 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.27707 to 0.27687, saving model to ./model/model.hdf5\n",
      "Epoch 227/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3120 - acc: 0.8742 - val_loss: 0.2766 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00227: val_loss improved from 0.27687 to 0.27664, saving model to ./model/model.hdf5\n",
      "Epoch 228/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.3119 - acc: 0.8742 - val_loss: 0.2764 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00228: val_loss improved from 0.27664 to 0.27641, saving model to ./model/model.hdf5\n",
      "Epoch 229/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3118 - acc: 0.8742 - val_loss: 0.2763 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.27641 to 0.27630, saving model to ./model/model.hdf5\n",
      "Epoch 230/2000\n",
      "318/318 [==============================] - 0s 11us/step - loss: 0.3117 - acc: 0.8742 - val_loss: 0.2761 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.27630 to 0.27605, saving model to ./model/model.hdf5\n",
      "Epoch 231/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.3116 - acc: 0.8742 - val_loss: 0.2760 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.27605 to 0.27595, saving model to ./model/model.hdf5\n",
      "Epoch 232/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3115 - acc: 0.8742 - val_loss: 0.2758 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00232: val_loss improved from 0.27595 to 0.27585, saving model to ./model/model.hdf5\n",
      "Epoch 233/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3114 - acc: 0.8742 - val_loss: 0.2757 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00233: val_loss improved from 0.27585 to 0.27570, saving model to ./model/model.hdf5\n",
      "Epoch 234/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.3113 - acc: 0.8774 - val_loss: 0.2758 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.27570\n",
      "Epoch 235/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3112 - acc: 0.8742 - val_loss: 0.2756 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.27570 to 0.27563, saving model to ./model/model.hdf5\n",
      "Epoch 236/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3111 - acc: 0.8774 - val_loss: 0.2755 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.27563 to 0.27554, saving model to ./model/model.hdf5\n",
      "Epoch 237/2000\n",
      "318/318 [==============================] - 0s 30us/step - loss: 0.3110 - acc: 0.8774 - val_loss: 0.2756 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.27554\n",
      "Epoch 238/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3109 - acc: 0.8742 - val_loss: 0.2753 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.27554 to 0.27531, saving model to ./model/model.hdf5\n",
      "Epoch 239/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3108 - acc: 0.8774 - val_loss: 0.2752 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.27531 to 0.27522, saving model to ./model/model.hdf5\n",
      "Epoch 240/2000\n",
      "318/318 [==============================] - 0s 32us/step - loss: 0.3107 - acc: 0.8742 - val_loss: 0.2750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00240: val_loss improved from 0.27522 to 0.27496, saving model to ./model/model.hdf5\n",
      "Epoch 241/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.3106 - acc: 0.8742 - val_loss: 0.2747 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00241: val_loss improved from 0.27496 to 0.27472, saving model to ./model/model.hdf5\n",
      "Epoch 242/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3105 - acc: 0.8742 - val_loss: 0.2746 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00242: val_loss improved from 0.27472 to 0.27460, saving model to ./model/model.hdf5\n",
      "Epoch 243/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3104 - acc: 0.8742 - val_loss: 0.2743 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00243: val_loss improved from 0.27460 to 0.27435, saving model to ./model/model.hdf5\n",
      "Epoch 244/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3103 - acc: 0.8774 - val_loss: 0.2743 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00244: val_loss improved from 0.27435 to 0.27426, saving model to ./model/model.hdf5\n",
      "Epoch 245/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.3102 - acc: 0.8742 - val_loss: 0.2741 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.27426 to 0.27414, saving model to ./model/model.hdf5\n",
      "Epoch 246/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3101 - acc: 0.8774 - val_loss: 0.2739 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.27414 to 0.27390, saving model to ./model/model.hdf5\n",
      "Epoch 247/2000\n",
      "318/318 [==============================] - 0s 48us/step - loss: 0.3100 - acc: 0.8774 - val_loss: 0.2738 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00247: val_loss improved from 0.27390 to 0.27380, saving model to ./model/model.hdf5\n",
      "Epoch 248/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.3099 - acc: 0.8774 - val_loss: 0.2735 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.27380 to 0.27354, saving model to ./model/model.hdf5\n",
      "Epoch 249/2000\n",
      "318/318 [==============================] - 0s 12us/step - loss: 0.3098 - acc: 0.8774 - val_loss: 0.2733 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00249: val_loss improved from 0.27354 to 0.27333, saving model to ./model/model.hdf5\n",
      "Epoch 250/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3097 - acc: 0.8774 - val_loss: 0.2733 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00250: val_loss improved from 0.27333 to 0.27328, saving model to ./model/model.hdf5\n",
      "Epoch 251/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3096 - acc: 0.8774 - val_loss: 0.2730 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00251: val_loss improved from 0.27328 to 0.27299, saving model to ./model/model.hdf5\n",
      "Epoch 252/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3096 - acc: 0.8774 - val_loss: 0.2729 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00252: val_loss improved from 0.27299 to 0.27289, saving model to ./model/model.hdf5\n",
      "Epoch 253/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3095 - acc: 0.8774 - val_loss: 0.2726 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.27289 to 0.27259, saving model to ./model/model.hdf5\n",
      "Epoch 254/2000\n",
      "318/318 [==============================] - 0s 11us/step - loss: 0.3094 - acc: 0.8774 - val_loss: 0.2724 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00254: val_loss improved from 0.27259 to 0.27242, saving model to ./model/model.hdf5\n",
      "Epoch 255/2000\n",
      "318/318 [==============================] - 0s 12us/step - loss: 0.3093 - acc: 0.8774 - val_loss: 0.2723 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.27242 to 0.27229, saving model to ./model/model.hdf5\n",
      "Epoch 256/2000\n",
      "318/318 [==============================] - 0s 12us/step - loss: 0.3092 - acc: 0.8774 - val_loss: 0.2721 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00256: val_loss improved from 0.27229 to 0.27209, saving model to ./model/model.hdf5\n",
      "Epoch 257/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3091 - acc: 0.8774 - val_loss: 0.2720 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00257: val_loss improved from 0.27209 to 0.27203, saving model to ./model/model.hdf5\n",
      "Epoch 258/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3090 - acc: 0.8774 - val_loss: 0.2718 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00258: val_loss improved from 0.27203 to 0.27182, saving model to ./model/model.hdf5\n",
      "Epoch 259/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3090 - acc: 0.8774 - val_loss: 0.2717 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00259: val_loss improved from 0.27182 to 0.27171, saving model to ./model/model.hdf5\n",
      "Epoch 260/2000\n",
      "318/318 [==============================] - 0s 36us/step - loss: 0.3089 - acc: 0.8774 - val_loss: 0.2715 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00260: val_loss improved from 0.27171 to 0.27153, saving model to ./model/model.hdf5\n",
      "Epoch 261/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3088 - acc: 0.8774 - val_loss: 0.2713 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00261: val_loss improved from 0.27153 to 0.27132, saving model to ./model/model.hdf5\n",
      "Epoch 262/2000\n",
      "318/318 [==============================] - 0s 12us/step - loss: 0.3087 - acc: 0.8774 - val_loss: 0.2712 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00262: val_loss improved from 0.27132 to 0.27122, saving model to ./model/model.hdf5\n",
      "Epoch 263/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3086 - acc: 0.8774 - val_loss: 0.2710 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.27122 to 0.27102, saving model to ./model/model.hdf5\n",
      "Epoch 264/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3086 - acc: 0.8774 - val_loss: 0.2710 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00264: val_loss improved from 0.27102 to 0.27099, saving model to ./model/model.hdf5\n",
      "Epoch 265/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3085 - acc: 0.8774 - val_loss: 0.2708 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00265: val_loss improved from 0.27099 to 0.27082, saving model to ./model/model.hdf5\n",
      "Epoch 266/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3084 - acc: 0.8774 - val_loss: 0.2708 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00266: val_loss improved from 0.27082 to 0.27075, saving model to ./model/model.hdf5\n",
      "Epoch 267/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3083 - acc: 0.8774 - val_loss: 0.2706 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00267: val_loss improved from 0.27075 to 0.27062, saving model to ./model/model.hdf5\n",
      "Epoch 268/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3082 - acc: 0.8774 - val_loss: 0.2705 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00268: val_loss improved from 0.27062 to 0.27053, saving model to ./model/model.hdf5\n",
      "Epoch 269/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3082 - acc: 0.8774 - val_loss: 0.2705 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00269: val_loss improved from 0.27053 to 0.27052, saving model to ./model/model.hdf5\n",
      "Epoch 270/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3081 - acc: 0.8774 - val_loss: 0.2704 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00270: val_loss improved from 0.27052 to 0.27043, saving model to ./model/model.hdf5\n",
      "Epoch 271/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.3080 - acc: 0.8774 - val_loss: 0.2706 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.27043\n",
      "Epoch 272/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3079 - acc: 0.8774 - val_loss: 0.2705 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.27043\n",
      "Epoch 273/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3078 - acc: 0.8774 - val_loss: 0.2705 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.27043\n",
      "Epoch 274/2000\n",
      "318/318 [==============================] - 0s 35us/step - loss: 0.3078 - acc: 0.8774 - val_loss: 0.2702 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00274: val_loss improved from 0.27043 to 0.27016, saving model to ./model/model.hdf5\n",
      "Epoch 275/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3077 - acc: 0.8774 - val_loss: 0.2699 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00275: val_loss improved from 0.27016 to 0.26987, saving model to ./model/model.hdf5\n",
      "Epoch 276/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3076 - acc: 0.8774 - val_loss: 0.2696 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00276: val_loss improved from 0.26987 to 0.26963, saving model to ./model/model.hdf5\n",
      "Epoch 277/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3075 - acc: 0.8774 - val_loss: 0.2695 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00277: val_loss improved from 0.26963 to 0.26949, saving model to ./model/model.hdf5\n",
      "Epoch 278/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.3074 - acc: 0.8805 - val_loss: 0.2694 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00278: val_loss improved from 0.26949 to 0.26940, saving model to ./model/model.hdf5\n",
      "Epoch 279/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3073 - acc: 0.8805 - val_loss: 0.2694 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.26940\n",
      "Epoch 280/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.3072 - acc: 0.8805 - val_loss: 0.2692 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00280: val_loss improved from 0.26940 to 0.26924, saving model to ./model/model.hdf5\n",
      "Epoch 281/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.3072 - acc: 0.8774 - val_loss: 0.2691 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00281: val_loss improved from 0.26924 to 0.26914, saving model to ./model/model.hdf5\n",
      "Epoch 282/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3071 - acc: 0.8805 - val_loss: 0.2688 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00282: val_loss improved from 0.26914 to 0.26882, saving model to ./model/model.hdf5\n",
      "Epoch 283/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.3070 - acc: 0.8805 - val_loss: 0.2688 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00283: val_loss improved from 0.26882 to 0.26876, saving model to ./model/model.hdf5\n",
      "Epoch 284/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3069 - acc: 0.8805 - val_loss: 0.2687 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00284: val_loss improved from 0.26876 to 0.26871, saving model to ./model/model.hdf5\n",
      "Epoch 285/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3069 - acc: 0.8805 - val_loss: 0.2685 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00285: val_loss improved from 0.26871 to 0.26848, saving model to ./model/model.hdf5\n",
      "Epoch 286/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3068 - acc: 0.8805 - val_loss: 0.2686 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.26848\n",
      "Epoch 287/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3067 - acc: 0.8805 - val_loss: 0.2680 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00287: val_loss improved from 0.26848 to 0.26797, saving model to ./model/model.hdf5\n",
      "Epoch 288/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3067 - acc: 0.8805 - val_loss: 0.2683 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.26797\n",
      "Epoch 289/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3066 - acc: 0.8805 - val_loss: 0.2679 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00289: val_loss improved from 0.26797 to 0.26791, saving model to ./model/model.hdf5\n",
      "Epoch 290/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3065 - acc: 0.8805 - val_loss: 0.2681 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.26791\n",
      "Epoch 291/2000\n",
      "318/318 [==============================] - 0s 32us/step - loss: 0.3064 - acc: 0.8805 - val_loss: 0.2680 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.26791\n",
      "Epoch 292/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3063 - acc: 0.8805 - val_loss: 0.2677 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00292: val_loss improved from 0.26791 to 0.26767, saving model to ./model/model.hdf5\n",
      "Epoch 293/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3063 - acc: 0.8805 - val_loss: 0.2678 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.26767\n",
      "Epoch 294/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3062 - acc: 0.8805 - val_loss: 0.2671 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00294: val_loss improved from 0.26767 to 0.26712, saving model to ./model/model.hdf5\n",
      "Epoch 295/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3061 - acc: 0.8805 - val_loss: 0.2674 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.26712\n",
      "Epoch 296/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3061 - acc: 0.8805 - val_loss: 0.2669 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00296: val_loss improved from 0.26712 to 0.26690, saving model to ./model/model.hdf5\n",
      "Epoch 297/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3060 - acc: 0.8805 - val_loss: 0.2673 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.26690\n",
      "Epoch 298/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3059 - acc: 0.8805 - val_loss: 0.2669 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.26690\n",
      "Epoch 299/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3058 - acc: 0.8805 - val_loss: 0.2670 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.26690\n",
      "Epoch 300/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3057 - acc: 0.8805 - val_loss: 0.2668 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00300: val_loss improved from 0.26690 to 0.26678, saving model to ./model/model.hdf5\n",
      "Epoch 301/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3057 - acc: 0.8805 - val_loss: 0.2665 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00301: val_loss improved from 0.26678 to 0.26654, saving model to ./model/model.hdf5\n",
      "Epoch 302/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.3056 - acc: 0.8805 - val_loss: 0.2665 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00302: val_loss improved from 0.26654 to 0.26649, saving model to ./model/model.hdf5\n",
      "Epoch 303/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3055 - acc: 0.8805 - val_loss: 0.2661 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00303: val_loss improved from 0.26649 to 0.26610, saving model to ./model/model.hdf5\n",
      "Epoch 304/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3055 - acc: 0.8805 - val_loss: 0.2663 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.26610\n",
      "Epoch 305/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3054 - acc: 0.8805 - val_loss: 0.2660 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00305: val_loss improved from 0.26610 to 0.26596, saving model to ./model/model.hdf5\n",
      "Epoch 306/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3053 - acc: 0.8805 - val_loss: 0.2662 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.26596\n",
      "Epoch 307/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3052 - acc: 0.8805 - val_loss: 0.2658 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00307: val_loss improved from 0.26596 to 0.26580, saving model to ./model/model.hdf5\n",
      "Epoch 308/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3052 - acc: 0.8805 - val_loss: 0.2659 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.26580\n",
      "Epoch 309/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.3051 - acc: 0.8805 - val_loss: 0.2653 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00309: val_loss improved from 0.26580 to 0.26535, saving model to ./model/model.hdf5\n",
      "Epoch 310/2000\n",
      "318/318 [==============================] - 0s 12us/step - loss: 0.3050 - acc: 0.8836 - val_loss: 0.2657 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.26535\n",
      "Epoch 311/2000\n",
      "318/318 [==============================] - 0s 11us/step - loss: 0.3050 - acc: 0.8805 - val_loss: 0.2651 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00311: val_loss improved from 0.26535 to 0.26510, saving model to ./model/model.hdf5\n",
      "Epoch 312/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3049 - acc: 0.8836 - val_loss: 0.2656 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.26510\n",
      "Epoch 313/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.3049 - acc: 0.8805 - val_loss: 0.2648 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00313: val_loss improved from 0.26510 to 0.26475, saving model to ./model/model.hdf5\n",
      "Epoch 314/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.3048 - acc: 0.8836 - val_loss: 0.2654 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.26475\n",
      "Epoch 315/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.3047 - acc: 0.8805 - val_loss: 0.2644 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00315: val_loss improved from 0.26475 to 0.26439, saving model to ./model/model.hdf5\n",
      "Epoch 316/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3047 - acc: 0.8836 - val_loss: 0.2656 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.26439\n",
      "Epoch 317/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3047 - acc: 0.8805 - val_loss: 0.2641 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00317: val_loss improved from 0.26439 to 0.26413, saving model to ./model/model.hdf5\n",
      "Epoch 318/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3048 - acc: 0.8836 - val_loss: 0.2659 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.26413\n",
      "Epoch 319/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3048 - acc: 0.8805 - val_loss: 0.2638 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00319: val_loss improved from 0.26413 to 0.26377, saving model to ./model/model.hdf5\n",
      "Epoch 320/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3049 - acc: 0.8805 - val_loss: 0.2663 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.26377\n",
      "Epoch 321/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3050 - acc: 0.8805 - val_loss: 0.2635 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00321: val_loss improved from 0.26377 to 0.26354, saving model to ./model/model.hdf5\n",
      "Epoch 322/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3052 - acc: 0.8805 - val_loss: 0.2674 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.26354\n",
      "Epoch 323/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3056 - acc: 0.8868 - val_loss: 0.2639 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.26354\n",
      "Epoch 324/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3062 - acc: 0.8805 - val_loss: 0.2702 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.26354\n",
      "Epoch 325/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3071 - acc: 0.8774 - val_loss: 0.2657 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.26354\n",
      "Epoch 326/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3084 - acc: 0.8774 - val_loss: 0.2753 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.26354\n",
      "Epoch 327/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3104 - acc: 0.8648 - val_loss: 0.2691 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.26354\n",
      "Epoch 328/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3125 - acc: 0.8774 - val_loss: 0.2820 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.26354\n",
      "Epoch 329/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3150 - acc: 0.8679 - val_loss: 0.2730 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.26354\n",
      "Epoch 330/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.3171 - acc: 0.8805 - val_loss: 0.2873 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.26354\n",
      "Epoch 331/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3189 - acc: 0.8679 - val_loss: 0.2742 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.26354\n",
      "Epoch 332/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3191 - acc: 0.8742 - val_loss: 0.2867 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.26354\n",
      "Epoch 333/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.3185 - acc: 0.8679 - val_loss: 0.2709 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.26354\n",
      "Epoch 334/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3157 - acc: 0.8805 - val_loss: 0.2773 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.26354\n",
      "Epoch 335/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3119 - acc: 0.8679 - val_loss: 0.2639 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.26354\n",
      "Epoch 336/2000\n",
      "318/318 [==============================] - 0s 12us/step - loss: 0.3077 - acc: 0.8805 - val_loss: 0.2656 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.26354\n",
      "Epoch 337/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3047 - acc: 0.8836 - val_loss: 0.2618 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00337: val_loss improved from 0.26354 to 0.26179, saving model to ./model/model.hdf5\n",
      "Epoch 338/2000\n",
      "318/318 [==============================] - 0s 33us/step - loss: 0.3034 - acc: 0.8805 - val_loss: 0.2609 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00338: val_loss improved from 0.26179 to 0.26093, saving model to ./model/model.hdf5\n",
      "Epoch 339/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3039 - acc: 0.8805 - val_loss: 0.2666 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.26093\n",
      "Epoch 340/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3056 - acc: 0.8774 - val_loss: 0.2625 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.26093\n",
      "Epoch 341/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.3075 - acc: 0.8805 - val_loss: 0.2725 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.26093\n",
      "Epoch 342/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.3092 - acc: 0.8648 - val_loss: 0.2647 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.26093\n",
      "Epoch 343/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3102 - acc: 0.8774 - val_loss: 0.2748 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.26093\n",
      "Epoch 344/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3105 - acc: 0.8679 - val_loss: 0.2646 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.26093\n",
      "Epoch 345/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3097 - acc: 0.8774 - val_loss: 0.2720 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.26093\n",
      "Epoch 346/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3084 - acc: 0.8679 - val_loss: 0.2626 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.26093\n",
      "Epoch 347/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3065 - acc: 0.8805 - val_loss: 0.2663 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.26093\n",
      "Epoch 348/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.3047 - acc: 0.8805 - val_loss: 0.2612 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.26093\n",
      "Epoch 349/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3034 - acc: 0.8836 - val_loss: 0.2623 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.26093\n",
      "Epoch 350/2000\n",
      "318/318 [==============================] - 0s 31us/step - loss: 0.3027 - acc: 0.8836 - val_loss: 0.2627 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.26093\n",
      "Epoch 351/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.3027 - acc: 0.8836 - val_loss: 0.2614 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.26093\n",
      "Epoch 352/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3032 - acc: 0.8836 - val_loss: 0.2656 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.26093\n",
      "Epoch 353/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3039 - acc: 0.8868 - val_loss: 0.2623 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.26093\n",
      "Epoch 354/2000\n",
      "318/318 [==============================] - 0s 38us/step - loss: 0.3050 - acc: 0.8805 - val_loss: 0.2696 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.26093\n",
      "Epoch 355/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.3063 - acc: 0.8711 - val_loss: 0.2642 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.26093\n",
      "Epoch 356/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.3078 - acc: 0.8805 - val_loss: 0.2749 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.26093\n",
      "Epoch 357/2000\n",
      "318/318 [==============================] - 0s 54us/step - loss: 0.3097 - acc: 0.8679 - val_loss: 0.2674 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.26093\n",
      "Epoch 358/2000\n",
      "318/318 [==============================] - 0s 37us/step - loss: 0.3120 - acc: 0.8774 - val_loss: 0.2821 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.26093\n",
      "Epoch 359/2000\n",
      "318/318 [==============================] - 0s 32us/step - loss: 0.3147 - acc: 0.8679 - val_loss: 0.2715 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.26093\n",
      "Epoch 360/2000\n",
      "318/318 [==============================] - 0s 48us/step - loss: 0.3173 - acc: 0.8742 - val_loss: 0.2885 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.26093\n",
      "Epoch 361/2000\n",
      "318/318 [==============================] - 0s 58us/step - loss: 0.3195 - acc: 0.8679 - val_loss: 0.2735 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.26093\n",
      "Epoch 362/2000\n",
      "318/318 [==============================] - 0s 31us/step - loss: 0.3201 - acc: 0.8742 - val_loss: 0.2876 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.26093\n",
      "Epoch 363/2000\n",
      "318/318 [==============================] - 0s 44us/step - loss: 0.3190 - acc: 0.8679 - val_loss: 0.2690 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.26093\n",
      "Epoch 364/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3155 - acc: 0.8805 - val_loss: 0.2763 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.26093\n",
      "Epoch 365/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.3109 - acc: 0.8679 - val_loss: 0.2614 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.26093\n",
      "Epoch 366/2000\n",
      "318/318 [==============================] - 0s 47us/step - loss: 0.3063 - acc: 0.8805 - val_loss: 0.2635 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.26093\n",
      "Epoch 367/2000\n",
      "318/318 [==============================] - 0s 38us/step - loss: 0.3030 - acc: 0.8805 - val_loss: 0.2602 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00367: val_loss improved from 0.26093 to 0.26022, saving model to ./model/model.hdf5\n",
      "Epoch 368/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3018 - acc: 0.8805 - val_loss: 0.2593 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00368: val_loss improved from 0.26022 to 0.25931, saving model to ./model/model.hdf5\n",
      "Epoch 369/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3026 - acc: 0.8805 - val_loss: 0.2663 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.25931\n",
      "Epoch 370/2000\n",
      "318/318 [==============================] - 0s 31us/step - loss: 0.3045 - acc: 0.8679 - val_loss: 0.2610 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.25931\n",
      "Epoch 371/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3065 - acc: 0.8805 - val_loss: 0.2716 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.25931\n",
      "Epoch 372/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.3079 - acc: 0.8679 - val_loss: 0.2622 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.25931\n",
      "Epoch 373/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3083 - acc: 0.8774 - val_loss: 0.2719 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.25931\n",
      "Epoch 374/2000\n",
      "318/318 [==============================] - 0s 31us/step - loss: 0.3080 - acc: 0.8679 - val_loss: 0.2614 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.25931\n",
      "Epoch 375/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3070 - acc: 0.8805 - val_loss: 0.2685 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.25931\n",
      "Epoch 376/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3057 - acc: 0.8679 - val_loss: 0.2598 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.25931\n",
      "Epoch 377/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3042 - acc: 0.8805 - val_loss: 0.2643 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.25931\n",
      "Epoch 378/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.3030 - acc: 0.8805 - val_loss: 0.2590 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00378: val_loss improved from 0.25931 to 0.25897, saving model to ./model/model.hdf5\n",
      "Epoch 379/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3020 - acc: 0.8836 - val_loss: 0.2612 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.25897\n",
      "Epoch 380/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3013 - acc: 0.8836 - val_loss: 0.2596 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.25897\n",
      "Epoch 381/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3010 - acc: 0.8868 - val_loss: 0.2597 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.25897\n",
      "Epoch 382/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3009 - acc: 0.8868 - val_loss: 0.2608 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.25897\n",
      "Epoch 383/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3010 - acc: 0.8836 - val_loss: 0.2590 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.25897\n",
      "Epoch 384/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.3012 - acc: 0.8836 - val_loss: 0.2624 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.25897\n",
      "Epoch 385/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3017 - acc: 0.8836 - val_loss: 0.2590 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00385: val_loss improved from 0.25897 to 0.25896, saving model to ./model/model.hdf5\n",
      "Epoch 386/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3023 - acc: 0.8805 - val_loss: 0.2653 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.25896\n",
      "Epoch 387/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.3034 - acc: 0.8805 - val_loss: 0.2603 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.25896\n",
      "Epoch 388/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3049 - acc: 0.8805 - val_loss: 0.2716 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.25896\n",
      "Epoch 389/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3072 - acc: 0.8679 - val_loss: 0.2648 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.25896\n",
      "Epoch 390/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3108 - acc: 0.8774 - val_loss: 0.2851 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.25896\n",
      "Epoch 391/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3164 - acc: 0.8711 - val_loss: 0.2764 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.25896\n",
      "Epoch 392/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3242 - acc: 0.8711 - val_loss: 0.3102 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.25896\n",
      "Epoch 393/2000\n",
      "318/318 [==============================] - 0s 39us/step - loss: 0.3350 - acc: 0.8459 - val_loss: 0.2949 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.25896\n",
      "Epoch 394/2000\n",
      "318/318 [==============================] - 0s 30us/step - loss: 0.3440 - acc: 0.8648 - val_loss: 0.3318 - val_acc: 0.8375\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.25896\n",
      "Epoch 395/2000\n",
      "318/318 [==============================] - 0s 62us/step - loss: 0.3511 - acc: 0.8428 - val_loss: 0.2965 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.25896\n",
      "Epoch 396/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3454 - acc: 0.8648 - val_loss: 0.3062 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.25896\n",
      "Epoch 397/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3308 - acc: 0.8522 - val_loss: 0.2659 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.25896\n",
      "Epoch 398/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.3119 - acc: 0.8805 - val_loss: 0.2623 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.25896\n",
      "Epoch 399/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3012 - acc: 0.8868 - val_loss: 0.2684 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.25896\n",
      "Epoch 400/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.3042 - acc: 0.8711 - val_loss: 0.2678 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.25896\n",
      "Epoch 401/2000\n",
      "318/318 [==============================] - 0s 44us/step - loss: 0.3145 - acc: 0.8805 - val_loss: 0.2926 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.25896\n",
      "Epoch 402/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3203 - acc: 0.8679 - val_loss: 0.2690 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.25896\n",
      "Epoch 403/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.3163 - acc: 0.8805 - val_loss: 0.2724 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.25896\n",
      "Epoch 404/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3068 - acc: 0.8711 - val_loss: 0.2586 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00404: val_loss improved from 0.25896 to 0.25862, saving model to ./model/model.hdf5\n",
      "Epoch 405/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3008 - acc: 0.8836 - val_loss: 0.2583 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00405: val_loss improved from 0.25862 to 0.25835, saving model to ./model/model.hdf5\n",
      "Epoch 406/2000\n",
      "318/318 [==============================] - 0s 12us/step - loss: 0.3024 - acc: 0.8805 - val_loss: 0.2742 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.25835\n",
      "Epoch 407/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.3082 - acc: 0.8711 - val_loss: 0.2645 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.25835\n",
      "Epoch 408/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3119 - acc: 0.8805 - val_loss: 0.2772 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.25835\n",
      "Epoch 409/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3103 - acc: 0.8679 - val_loss: 0.2593 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.25835\n",
      "Epoch 410/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3054 - acc: 0.8805 - val_loss: 0.2613 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.25835\n",
      "Epoch 411/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3009 - acc: 0.8805 - val_loss: 0.2577 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00411: val_loss improved from 0.25835 to 0.25771, saving model to ./model/model.hdf5\n",
      "Epoch 412/2000\n",
      "318/318 [==============================] - 0s 42us/step - loss: 0.2995 - acc: 0.8868 - val_loss: 0.2564 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00412: val_loss improved from 0.25771 to 0.25645, saving model to ./model/model.hdf5\n",
      "Epoch 413/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3013 - acc: 0.8836 - val_loss: 0.2678 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.25645\n",
      "Epoch 414/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3045 - acc: 0.8711 - val_loss: 0.2610 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.25645\n",
      "Epoch 415/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3075 - acc: 0.8805 - val_loss: 0.2750 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.25645\n",
      "Epoch 416/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.3091 - acc: 0.8742 - val_loss: 0.2627 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.25645\n",
      "Epoch 417/2000\n",
      "318/318 [==============================] - 0s 30us/step - loss: 0.3090 - acc: 0.8774 - val_loss: 0.2730 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.25645\n",
      "Epoch 418/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.3078 - acc: 0.8742 - val_loss: 0.2601 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.25645\n",
      "Epoch 419/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3056 - acc: 0.8805 - val_loss: 0.2663 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.25645\n",
      "Epoch 420/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.3033 - acc: 0.8742 - val_loss: 0.2571 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.25645\n",
      "Epoch 421/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.3012 - acc: 0.8805 - val_loss: 0.2599 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.25645\n",
      "Epoch 422/2000\n",
      "318/318 [==============================] - 0s 33us/step - loss: 0.2997 - acc: 0.8836 - val_loss: 0.2565 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.25645\n",
      "Epoch 423/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2989 - acc: 0.8836 - val_loss: 0.2564 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00423: val_loss improved from 0.25645 to 0.25637, saving model to ./model/model.hdf5\n",
      "Epoch 424/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2988 - acc: 0.8836 - val_loss: 0.2585 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.25637\n",
      "Epoch 425/2000\n",
      "318/318 [==============================] - 0s 38us/step - loss: 0.2992 - acc: 0.8805 - val_loss: 0.2555 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00425: val_loss improved from 0.25637 to 0.25549, saving model to ./model/model.hdf5\n",
      "Epoch 426/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2999 - acc: 0.8805 - val_loss: 0.2614 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.25549\n",
      "Epoch 427/2000\n",
      "318/318 [==============================] - 0s 12us/step - loss: 0.3006 - acc: 0.8836 - val_loss: 0.2559 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.25549\n",
      "Epoch 428/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3011 - acc: 0.8836 - val_loss: 0.2633 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.25549\n",
      "Epoch 429/2000\n",
      "318/318 [==============================] - 0s 43us/step - loss: 0.3015 - acc: 0.8805 - val_loss: 0.2562 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.25549\n",
      "Epoch 430/2000\n",
      "318/318 [==============================] - 0s 36us/step - loss: 0.3014 - acc: 0.8805 - val_loss: 0.2627 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.25549\n",
      "Epoch 431/2000\n",
      "318/318 [==============================] - 0s 68us/step - loss: 0.3010 - acc: 0.8774 - val_loss: 0.2557 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.25549\n",
      "Epoch 432/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.3003 - acc: 0.8836 - val_loss: 0.2601 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.25549\n",
      "Epoch 433/2000\n",
      "318/318 [==============================] - 0s 39us/step - loss: 0.2995 - acc: 0.8836 - val_loss: 0.2554 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00433: val_loss improved from 0.25549 to 0.25539, saving model to ./model/model.hdf5\n",
      "Epoch 434/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2989 - acc: 0.8805 - val_loss: 0.2574 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.25539\n",
      "Epoch 435/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2984 - acc: 0.8836 - val_loss: 0.2561 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.25539\n",
      "Epoch 436/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2982 - acc: 0.8868 - val_loss: 0.2557 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.25539\n",
      "Epoch 437/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2982 - acc: 0.8836 - val_loss: 0.2574 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.25539\n",
      "Epoch 438/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.2983 - acc: 0.8836 - val_loss: 0.2550 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00438: val_loss improved from 0.25539 to 0.25504, saving model to ./model/model.hdf5\n",
      "Epoch 439/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2986 - acc: 0.8805 - val_loss: 0.2588 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.25504\n",
      "Epoch 440/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2988 - acc: 0.8836 - val_loss: 0.2549 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00440: val_loss improved from 0.25504 to 0.25494, saving model to ./model/model.hdf5\n",
      "Epoch 441/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2990 - acc: 0.8805 - val_loss: 0.2600 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.25494\n",
      "Epoch 442/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.2992 - acc: 0.8774 - val_loss: 0.2551 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.25494\n",
      "Epoch 443/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.2993 - acc: 0.8836 - val_loss: 0.2608 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.25494\n",
      "Epoch 444/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2994 - acc: 0.8742 - val_loss: 0.2551 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.25494\n",
      "Epoch 445/2000\n",
      "318/318 [==============================] - 0s 49us/step - loss: 0.2995 - acc: 0.8836 - val_loss: 0.2611 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.25494\n",
      "Epoch 446/2000\n",
      "318/318 [==============================] - 0s 39us/step - loss: 0.2995 - acc: 0.8774 - val_loss: 0.2551 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.25494\n",
      "Epoch 447/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2995 - acc: 0.8836 - val_loss: 0.2612 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.25494\n",
      "Epoch 448/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2995 - acc: 0.8774 - val_loss: 0.2551 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.25494\n",
      "Epoch 449/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.2996 - acc: 0.8836 - val_loss: 0.2618 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.25494\n",
      "Epoch 450/2000\n",
      "318/318 [==============================] - -0s -29us/step - loss: 0.2998 - acc: 0.8774 - val_loss: 0.2553 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.25494\n",
      "Epoch 451/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3000 - acc: 0.8836 - val_loss: 0.2633 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.25494\n",
      "Epoch 452/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.3005 - acc: 0.8774 - val_loss: 0.2558 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.25494\n",
      "Epoch 453/2000\n",
      "318/318 [==============================] - 0s 31us/step - loss: 0.3011 - acc: 0.8805 - val_loss: 0.2663 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.25494\n",
      "Epoch 454/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3022 - acc: 0.8774 - val_loss: 0.2573 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.25494\n",
      "Epoch 455/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3037 - acc: 0.8805 - val_loss: 0.2723 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.25494\n",
      "Epoch 456/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3060 - acc: 0.8711 - val_loss: 0.2614 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.25494\n",
      "Epoch 457/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.3090 - acc: 0.8805 - val_loss: 0.2838 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.25494\n",
      "Epoch 458/2000\n",
      "318/318 [==============================] - 0s 42us/step - loss: 0.3136 - acc: 0.8711 - val_loss: 0.2705 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.25494\n",
      "Epoch 459/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3196 - acc: 0.8711 - val_loss: 0.3032 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.25494\n",
      "Epoch 460/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3274 - acc: 0.8585 - val_loss: 0.2849 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.25494\n",
      "Epoch 461/2000\n",
      "318/318 [==============================] - 0s 30us/step - loss: 0.3354 - acc: 0.8742 - val_loss: 0.3244 - val_acc: 0.8375\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.25494\n",
      "Epoch 462/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3430 - acc: 0.8428 - val_loss: 0.2933 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.25494\n",
      "Epoch 463/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.3444 - acc: 0.8648 - val_loss: 0.3195 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.25494\n",
      "Epoch 464/2000\n",
      "318/318 [==============================] - 0s 31us/step - loss: 0.3386 - acc: 0.8428 - val_loss: 0.2746 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.25494\n",
      "Epoch 465/2000\n",
      "318/318 [==============================] - 0s 45us/step - loss: 0.3240 - acc: 0.8742 - val_loss: 0.2761 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.25494\n",
      "Epoch 466/2000\n",
      "318/318 [==============================] - 0s 32us/step - loss: 0.3072 - acc: 0.8742 - val_loss: 0.2558 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.25494\n",
      "Epoch 467/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2979 - acc: 0.8836 - val_loss: 0.2558 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.25494\n",
      "Epoch 468/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2998 - acc: 0.8805 - val_loss: 0.2780 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.25494\n",
      "Epoch 469/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3083 - acc: 0.8742 - val_loss: 0.2664 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.25494\n",
      "Epoch 470/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3151 - acc: 0.8805 - val_loss: 0.2866 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.25494\n",
      "Epoch 471/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3142 - acc: 0.8679 - val_loss: 0.2602 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.25494\n",
      "Epoch 472/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3073 - acc: 0.8805 - val_loss: 0.2638 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.25494\n",
      "Epoch 473/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2998 - acc: 0.8742 - val_loss: 0.2568 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.25494\n",
      "Epoch 474/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2970 - acc: 0.8868 - val_loss: 0.2556 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.25494\n",
      "Epoch 475/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2995 - acc: 0.8805 - val_loss: 0.2718 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.25494\n",
      "Epoch 476/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3041 - acc: 0.8742 - val_loss: 0.2606 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.25494\n",
      "Epoch 477/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.3068 - acc: 0.8805 - val_loss: 0.2748 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.25494\n",
      "Epoch 478/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3058 - acc: 0.8679 - val_loss: 0.2577 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.25494\n",
      "Epoch 479/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.3024 - acc: 0.8805 - val_loss: 0.2630 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.25494\n",
      "Epoch 480/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.2986 - acc: 0.8711 - val_loss: 0.2561 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.25494\n",
      "Epoch 481/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.2965 - acc: 0.8805 - val_loss: 0.2560 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.25494\n",
      "Epoch 482/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.2965 - acc: 0.8805 - val_loss: 0.2623 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.25494\n",
      "Epoch 483/2000\n",
      "318/318 [==============================] - 0s 54us/step - loss: 0.2980 - acc: 0.8805 - val_loss: 0.2570 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.25494\n",
      "Epoch 484/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.3002 - acc: 0.8805 - val_loss: 0.2697 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.25494\n",
      "Epoch 485/2000\n",
      "318/318 [==============================] - 0s 30us/step - loss: 0.3024 - acc: 0.8679 - val_loss: 0.2604 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.25494\n",
      "Epoch 486/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3051 - acc: 0.8805 - val_loss: 0.2775 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.25494\n",
      "Epoch 487/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.3078 - acc: 0.8742 - val_loss: 0.2658 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.25494\n",
      "Epoch 488/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.3116 - acc: 0.8805 - val_loss: 0.2883 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.25494\n",
      "Epoch 489/2000\n",
      "318/318 [==============================] - 0s 33us/step - loss: 0.3153 - acc: 0.8679 - val_loss: 0.2728 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.25494\n",
      "Epoch 490/2000\n",
      "318/318 [==============================] - 0s 34us/step - loss: 0.3192 - acc: 0.8742 - val_loss: 0.2966 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.25494\n",
      "Epoch 491/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.3202 - acc: 0.8648 - val_loss: 0.2723 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.25494\n",
      "Epoch 492/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3184 - acc: 0.8805 - val_loss: 0.2847 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.25494\n",
      "Epoch 493/2000\n",
      "318/318 [==============================] - 0s 30us/step - loss: 0.3108 - acc: 0.8742 - val_loss: 0.2590 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.25494\n",
      "Epoch 494/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.3023 - acc: 0.8805 - val_loss: 0.2603 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.25494\n",
      "Epoch 495/2000\n",
      "318/318 [==============================] - 0s 33us/step - loss: 0.2965 - acc: 0.8805 - val_loss: 0.2602 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.25494\n",
      "Epoch 496/2000\n",
      "318/318 [==============================] - 0s 32us/step - loss: 0.2966 - acc: 0.8805 - val_loss: 0.2573 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.25494\n",
      "Epoch 497/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3008 - acc: 0.8774 - val_loss: 0.2743 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.25494\n",
      "Epoch 498/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3044 - acc: 0.8679 - val_loss: 0.2602 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.25494\n",
      "Epoch 499/2000\n",
      "318/318 [==============================] - 0s 30us/step - loss: 0.3050 - acc: 0.8805 - val_loss: 0.2704 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.25494\n",
      "Epoch 500/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.3019 - acc: 0.8711 - val_loss: 0.2557 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.25494\n",
      "Epoch 501/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2981 - acc: 0.8805 - val_loss: 0.2584 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 0.25494\n",
      "Epoch 502/2000\n",
      "318/318 [==============================] - 0s 30us/step - loss: 0.2955 - acc: 0.8836 - val_loss: 0.2568 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 0.25494\n",
      "Epoch 503/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.2951 - acc: 0.8868 - val_loss: 0.2548 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00503: val_loss improved from 0.25494 to 0.25477, saving model to ./model/model.hdf5\n",
      "Epoch 504/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.2964 - acc: 0.8805 - val_loss: 0.2645 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 0.25477\n",
      "Epoch 505/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2986 - acc: 0.8774 - val_loss: 0.2574 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 0.25477\n",
      "Epoch 506/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.3012 - acc: 0.8774 - val_loss: 0.2725 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 0.25477\n",
      "Epoch 507/2000\n",
      "318/318 [==============================] - 0s 31us/step - loss: 0.3035 - acc: 0.8679 - val_loss: 0.2609 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 0.25477\n",
      "Epoch 508/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3055 - acc: 0.8805 - val_loss: 0.2777 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 0.25477\n",
      "Epoch 509/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3064 - acc: 0.8774 - val_loss: 0.2620 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 0.25477\n",
      "Epoch 510/2000\n",
      "318/318 [==============================] - 0s 38us/step - loss: 0.3064 - acc: 0.8805 - val_loss: 0.2756 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 0.25477\n",
      "Epoch 511/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3045 - acc: 0.8679 - val_loss: 0.2583 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 0.25477\n",
      "Epoch 512/2000\n",
      "318/318 [==============================] - 0s 55us/step - loss: 0.3016 - acc: 0.8805 - val_loss: 0.2652 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 0.25477\n",
      "Epoch 513/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2981 - acc: 0.8774 - val_loss: 0.2550 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 0.25477\n",
      "Epoch 514/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2955 - acc: 0.8805 - val_loss: 0.2564 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 0.25477\n",
      "Epoch 515/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2945 - acc: 0.8836 - val_loss: 0.2585 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 0.25477\n",
      "Epoch 516/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2950 - acc: 0.8836 - val_loss: 0.2547 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00516: val_loss improved from 0.25477 to 0.25468, saving model to ./model/model.hdf5\n",
      "Epoch 517/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2963 - acc: 0.8805 - val_loss: 0.2643 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 0.25468\n",
      "Epoch 518/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2977 - acc: 0.8836 - val_loss: 0.2557 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 0.25468\n",
      "Epoch 519/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2985 - acc: 0.8805 - val_loss: 0.2663 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 0.25468\n",
      "Epoch 520/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2987 - acc: 0.8774 - val_loss: 0.2558 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 0.25468\n",
      "Epoch 521/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2984 - acc: 0.8805 - val_loss: 0.2648 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 0.25468\n",
      "Epoch 522/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2977 - acc: 0.8805 - val_loss: 0.2551 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 0.25468\n",
      "Epoch 523/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2970 - acc: 0.8805 - val_loss: 0.2621 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 0.25468\n",
      "Epoch 524/2000\n",
      "318/318 [==============================] - 0s 30us/step - loss: 0.2963 - acc: 0.8836 - val_loss: 0.2545 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00524: val_loss improved from 0.25468 to 0.25454, saving model to ./model/model.hdf5\n",
      "Epoch 525/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2957 - acc: 0.8805 - val_loss: 0.2598 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 0.25454\n",
      "Epoch 526/2000\n",
      "318/318 [==============================] - 0s 34us/step - loss: 0.2951 - acc: 0.8836 - val_loss: 0.2543 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00526: val_loss improved from 0.25454 to 0.25430, saving model to ./model/model.hdf5\n",
      "Epoch 527/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2947 - acc: 0.8836 - val_loss: 0.2582 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 0.25430\n",
      "Epoch 528/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.2944 - acc: 0.8805 - val_loss: 0.2543 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00528: val_loss improved from 0.25430 to 0.25428, saving model to ./model/model.hdf5\n",
      "Epoch 529/2000\n",
      "318/318 [==============================] - 0s 11us/step - loss: 0.2942 - acc: 0.8805 - val_loss: 0.2573 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 0.25428\n",
      "Epoch 530/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2940 - acc: 0.8836 - val_loss: 0.2543 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 0.25428\n",
      "Epoch 531/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2939 - acc: 0.8805 - val_loss: 0.2567 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 0.25428\n",
      "Epoch 532/2000\n",
      "318/318 [==============================] - 0s 52us/step - loss: 0.2938 - acc: 0.8836 - val_loss: 0.2544 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 0.25428\n",
      "Epoch 533/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2937 - acc: 0.8805 - val_loss: 0.2565 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 0.25428\n",
      "Epoch 534/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.2937 - acc: 0.8836 - val_loss: 0.2544 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 0.25428\n",
      "Epoch 535/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2936 - acc: 0.8805 - val_loss: 0.2565 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 0.25428\n",
      "Epoch 536/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2936 - acc: 0.8836 - val_loss: 0.2543 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00536: val_loss improved from 0.25428 to 0.25428, saving model to ./model/model.hdf5\n",
      "Epoch 537/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2935 - acc: 0.8805 - val_loss: 0.2566 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 0.25428\n",
      "Epoch 538/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.2935 - acc: 0.8836 - val_loss: 0.2541 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00538: val_loss improved from 0.25428 to 0.25407, saving model to ./model/model.hdf5\n",
      "Epoch 539/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2935 - acc: 0.8805 - val_loss: 0.2570 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 0.25407\n",
      "Epoch 540/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2936 - acc: 0.8836 - val_loss: 0.2538 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00540: val_loss improved from 0.25407 to 0.25378, saving model to ./model/model.hdf5\n",
      "Epoch 541/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2937 - acc: 0.8805 - val_loss: 0.2580 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 0.25378\n",
      "Epoch 542/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.2939 - acc: 0.8805 - val_loss: 0.2536 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00542: val_loss improved from 0.25378 to 0.25355, saving model to ./model/model.hdf5\n",
      "Epoch 543/2000\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2943 - acc: 0.8836 - val_loss: 0.2605 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 0.25355\n",
      "Epoch 544/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2949 - acc: 0.8805 - val_loss: 0.2541 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 0.25355\n",
      "Epoch 545/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.2962 - acc: 0.8805 - val_loss: 0.2668 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 0.25355\n",
      "Epoch 546/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2983 - acc: 0.8805 - val_loss: 0.2579 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 0.25355\n",
      "Epoch 547/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3022 - acc: 0.8805 - val_loss: 0.2827 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 0.25355\n",
      "Epoch 548/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3083 - acc: 0.8742 - val_loss: 0.2718 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 0.25355\n",
      "Epoch 549/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.3191 - acc: 0.8774 - val_loss: 0.3185 - val_acc: 0.8375\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 0.25355\n",
      "Epoch 550/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3336 - acc: 0.8553 - val_loss: 0.3041 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 0.25355\n",
      "Epoch 551/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3544 - acc: 0.8585 - val_loss: 0.3662 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 0.25355\n",
      "Epoch 552/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3689 - acc: 0.8333 - val_loss: 0.3201 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 0.25355\n",
      "Epoch 553/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3713 - acc: 0.8522 - val_loss: 0.3310 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 0.25355\n",
      "Epoch 554/2000\n",
      "318/318 [==============================] - 0s 44us/step - loss: 0.3427 - acc: 0.8459 - val_loss: 0.2609 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 0.25355\n",
      "Epoch 555/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3082 - acc: 0.8836 - val_loss: 0.2538 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 0.25355\n",
      "Epoch 556/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.2955 - acc: 0.8868 - val_loss: 0.2874 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 0.25355\n",
      "Epoch 557/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3126 - acc: 0.8679 - val_loss: 0.2795 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 0.25355\n",
      "Epoch 558/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.3308 - acc: 0.8805 - val_loss: 0.2997 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 0.25355\n",
      "Epoch 559/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3207 - acc: 0.8679 - val_loss: 0.2536 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 0.25355\n",
      "Epoch 560/2000\n",
      "318/318 [==============================] - 0s 31us/step - loss: 0.3006 - acc: 0.8805 - val_loss: 0.2519 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00560: val_loss improved from 0.25355 to 0.25189, saving model to ./model/model.hdf5\n",
      "Epoch 561/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.2950 - acc: 0.8868 - val_loss: 0.2804 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 0.25189\n",
      "Epoch 562/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3068 - acc: 0.8774 - val_loss: 0.2663 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 0.25189\n",
      "Epoch 563/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.3184 - acc: 0.8805 - val_loss: 0.2937 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 0.25189\n",
      "Epoch 564/2000\n",
      "318/318 [==============================] - 0s 42us/step - loss: 0.3156 - acc: 0.8679 - val_loss: 0.2547 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 0.25189\n",
      "Epoch 565/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3038 - acc: 0.8774 - val_loss: 0.2571 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 0.25189\n",
      "Epoch 566/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2943 - acc: 0.8679 - val_loss: 0.2572 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 0.25189\n",
      "Epoch 567/2000\n",
      "318/318 [==============================] - 0s 49us/step - loss: 0.2944 - acc: 0.8679 - val_loss: 0.2529 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 0.25189\n",
      "Epoch 568/2000\n",
      "318/318 [==============================] - 0s 59us/step - loss: 0.3013 - acc: 0.8805 - val_loss: 0.2804 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 0.25189\n",
      "Epoch 569/2000\n",
      "318/318 [==============================] - 0s 37us/step - loss: 0.3079 - acc: 0.8742 - val_loss: 0.2584 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 0.25189\n",
      "Epoch 570/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.3095 - acc: 0.8774 - val_loss: 0.2760 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 0.25189\n",
      "Epoch 571/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3053 - acc: 0.8805 - val_loss: 0.2508 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00571: val_loss improved from 0.25189 to 0.25082, saving model to ./model/model.hdf5\n",
      "Epoch 572/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.2988 - acc: 0.8805 - val_loss: 0.2553 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 0.25082\n",
      "Epoch 573/2000\n",
      "318/318 [==============================] - 0s 10us/step - loss: 0.2939 - acc: 0.8742 - val_loss: 0.2515 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 0.25082\n",
      "Epoch 574/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2927 - acc: 0.8805 - val_loss: 0.2483 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00574: val_loss improved from 0.25082 to 0.24835, saving model to ./model/model.hdf5\n",
      "Epoch 575/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2950 - acc: 0.8868 - val_loss: 0.2638 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 0.24835\n",
      "Epoch 576/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.2986 - acc: 0.8774 - val_loss: 0.2513 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 0.24835\n",
      "Epoch 577/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.3017 - acc: 0.8805 - val_loss: 0.2706 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 0.24835\n",
      "Epoch 578/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.3029 - acc: 0.8836 - val_loss: 0.2514 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 0.24835\n",
      "Epoch 579/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.3020 - acc: 0.8805 - val_loss: 0.2652 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 0.24835\n",
      "Epoch 580/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2996 - acc: 0.8805 - val_loss: 0.2482 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00580: val_loss improved from 0.24835 to 0.24818, saving model to ./model/model.hdf5\n",
      "Epoch 581/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2965 - acc: 0.8868 - val_loss: 0.2548 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 0.24818\n",
      "Epoch 582/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.2939 - acc: 0.8774 - val_loss: 0.2484 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 0.24818\n",
      "Epoch 583/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2925 - acc: 0.8836 - val_loss: 0.2487 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 0.24818\n",
      "Epoch 584/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2923 - acc: 0.8836 - val_loss: 0.2539 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 0.24818\n",
      "Epoch 585/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2932 - acc: 0.8742 - val_loss: 0.2482 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00585: val_loss improved from 0.24818 to 0.24817, saving model to ./model/model.hdf5\n",
      "Epoch 586/2000\n",
      "318/318 [==============================] - 0s 40us/step - loss: 0.2945 - acc: 0.8868 - val_loss: 0.2590 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 0.24817\n",
      "Epoch 587/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2954 - acc: 0.8742 - val_loss: 0.2486 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 0.24817\n",
      "Epoch 588/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.2955 - acc: 0.8868 - val_loss: 0.2579 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 0.24817\n",
      "Epoch 589/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2948 - acc: 0.8742 - val_loss: 0.2481 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00589: val_loss improved from 0.24817 to 0.24814, saving model to ./model/model.hdf5\n",
      "Epoch 590/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2937 - acc: 0.8868 - val_loss: 0.2530 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 0.24814\n",
      "Epoch 591/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2927 - acc: 0.8774 - val_loss: 0.2487 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 0.24814\n",
      "Epoch 592/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2920 - acc: 0.8836 - val_loss: 0.2489 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 0.24814\n",
      "Epoch 593/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2919 - acc: 0.8836 - val_loss: 0.2513 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 0.24814\n",
      "Epoch 594/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2923 - acc: 0.8774 - val_loss: 0.2473 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00594: val_loss improved from 0.24814 to 0.24733, saving model to ./model/model.hdf5\n",
      "Epoch 595/2000\n",
      "318/318 [==============================] - 0s 30us/step - loss: 0.2928 - acc: 0.8868 - val_loss: 0.2542 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 0.24733\n",
      "Epoch 596/2000\n",
      "318/318 [==============================] - 0s 33us/step - loss: 0.2933 - acc: 0.8774 - val_loss: 0.2471 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00596: val_loss improved from 0.24733 to 0.24712, saving model to ./model/model.hdf5\n",
      "Epoch 597/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.2937 - acc: 0.8868 - val_loss: 0.2554 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 0.24712\n",
      "Epoch 598/2000\n",
      "318/318 [==============================] - 0s 31us/step - loss: 0.2938 - acc: 0.8774 - val_loss: 0.2472 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 0.24712\n",
      "Epoch 599/2000\n",
      "318/318 [==============================] - 0s 48us/step - loss: 0.2936 - acc: 0.8868 - val_loss: 0.2544 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 0.24712\n",
      "Epoch 600/2000\n",
      "318/318 [==============================] - 0s 33us/step - loss: 0.2932 - acc: 0.8742 - val_loss: 0.2474 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 0.24712\n",
      "Epoch 601/2000\n",
      "318/318 [==============================] - 0s 47us/step - loss: 0.2927 - acc: 0.8868 - val_loss: 0.2522 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 0.24712\n",
      "Epoch 602/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.2922 - acc: 0.8774 - val_loss: 0.2483 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 0.24712\n",
      "Epoch 603/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.2918 - acc: 0.8868 - val_loss: 0.2500 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 0.24712\n",
      "Epoch 604/2000\n",
      "318/318 [==============================] - 0s 35us/step - loss: 0.2916 - acc: 0.8868 - val_loss: 0.2494 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 0.24712\n",
      "Epoch 605/2000\n",
      "318/318 [==============================] - 0s 43us/step - loss: 0.2915 - acc: 0.8836 - val_loss: 0.2482 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 0.24712\n",
      "Epoch 606/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.2916 - acc: 0.8836 - val_loss: 0.2508 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 0.24712\n",
      "Epoch 607/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2917 - acc: 0.8774 - val_loss: 0.2473 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 0.24712\n",
      "Epoch 608/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.2919 - acc: 0.8868 - val_loss: 0.2522 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 0.24712\n",
      "Epoch 609/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2921 - acc: 0.8774 - val_loss: 0.2471 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00609: val_loss improved from 0.24712 to 0.24706, saving model to ./model/model.hdf5\n",
      "Epoch 610/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2923 - acc: 0.8868 - val_loss: 0.2532 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 0.24706\n",
      "Epoch 611/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2924 - acc: 0.8805 - val_loss: 0.2470 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00611: val_loss improved from 0.24706 to 0.24703, saving model to ./model/model.hdf5\n",
      "Epoch 612/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2925 - acc: 0.8868 - val_loss: 0.2537 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 0.24703\n",
      "Epoch 613/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2925 - acc: 0.8774 - val_loss: 0.2472 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 0.24703\n",
      "Epoch 614/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2925 - acc: 0.8868 - val_loss: 0.2538 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 0.24703\n",
      "Epoch 615/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.2924 - acc: 0.8742 - val_loss: 0.2474 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 0.24703\n",
      "Epoch 616/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2923 - acc: 0.8868 - val_loss: 0.2535 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 0.24703\n",
      "Epoch 617/2000\n",
      "318/318 [==============================] - 0s 41us/step - loss: 0.2922 - acc: 0.8742 - val_loss: 0.2473 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 0.24703\n",
      "Epoch 618/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2921 - acc: 0.8899 - val_loss: 0.2530 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 0.24703\n",
      "Epoch 619/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2920 - acc: 0.8742 - val_loss: 0.2470 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00619: val_loss improved from 0.24703 to 0.24700, saving model to ./model/model.hdf5\n",
      "Epoch 620/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2920 - acc: 0.8899 - val_loss: 0.2528 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 0.24700\n",
      "Epoch 621/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2920 - acc: 0.8774 - val_loss: 0.2468 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00621: val_loss improved from 0.24700 to 0.24677, saving model to ./model/model.hdf5\n",
      "Epoch 622/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2920 - acc: 0.8899 - val_loss: 0.2531 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 0.24677\n",
      "Epoch 623/2000\n",
      "318/318 [==============================] - 0s 12us/step - loss: 0.2921 - acc: 0.8805 - val_loss: 0.2466 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00623: val_loss improved from 0.24677 to 0.24662, saving model to ./model/model.hdf5\n",
      "Epoch 624/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2923 - acc: 0.8868 - val_loss: 0.2542 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 0.24662\n",
      "Epoch 625/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.2925 - acc: 0.8742 - val_loss: 0.2466 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 0.24662\n",
      "Epoch 626/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.2928 - acc: 0.8868 - val_loss: 0.2561 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 0.24662\n",
      "Epoch 627/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2933 - acc: 0.8774 - val_loss: 0.2470 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 0.24662\n",
      "Epoch 628/2000\n",
      "318/318 [==============================] - 0s 11us/step - loss: 0.2939 - acc: 0.8868 - val_loss: 0.2592 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 0.24662\n",
      "Epoch 629/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2946 - acc: 0.8805 - val_loss: 0.2479 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 0.24662\n",
      "Epoch 630/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2957 - acc: 0.8836 - val_loss: 0.2638 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 0.24662\n",
      "Epoch 631/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2970 - acc: 0.8774 - val_loss: 0.2497 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 0.24662\n",
      "Epoch 632/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2989 - acc: 0.8805 - val_loss: 0.2707 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 0.24662\n",
      "Epoch 633/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3012 - acc: 0.8805 - val_loss: 0.2533 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 0.24662\n",
      "Epoch 634/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3043 - acc: 0.8805 - val_loss: 0.2815 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 0.24662\n",
      "Epoch 635/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3082 - acc: 0.8805 - val_loss: 0.2598 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 0.24662\n",
      "Epoch 636/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3131 - acc: 0.8711 - val_loss: 0.2968 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 0.24662\n",
      "Epoch 637/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3186 - acc: 0.8679 - val_loss: 0.2688 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 0.24662\n",
      "Epoch 638/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3241 - acc: 0.8774 - val_loss: 0.3100 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 0.24662\n",
      "Epoch 639/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3278 - acc: 0.8553 - val_loss: 0.2719 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 0.24662\n",
      "Epoch 640/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3279 - acc: 0.8742 - val_loss: 0.3031 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 0.24662\n",
      "Epoch 641/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3226 - acc: 0.8553 - val_loss: 0.2592 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 0.24662\n",
      "Epoch 642/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3127 - acc: 0.8774 - val_loss: 0.2708 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 0.24662\n",
      "Epoch 643/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3009 - acc: 0.8774 - val_loss: 0.2467 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 0.24662\n",
      "Epoch 644/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2927 - acc: 0.8868 - val_loss: 0.2474 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 0.24662\n",
      "Epoch 645/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2907 - acc: 0.8868 - val_loss: 0.2590 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 0.24662\n",
      "Epoch 646/2000\n",
      "318/318 [==============================] - 0s 32us/step - loss: 0.2945 - acc: 0.8774 - val_loss: 0.2496 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 0.24662\n",
      "Epoch 647/2000\n",
      "318/318 [==============================] - 0s 31us/step - loss: 0.3009 - acc: 0.8805 - val_loss: 0.2777 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 0.24662\n",
      "Epoch 648/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3064 - acc: 0.8805 - val_loss: 0.2557 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 0.24662\n",
      "Epoch 649/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3097 - acc: 0.8742 - val_loss: 0.2824 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 0.24662\n",
      "Epoch 650/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.3099 - acc: 0.8711 - val_loss: 0.2543 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 0.24662\n",
      "Epoch 651/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.3078 - acc: 0.8774 - val_loss: 0.2728 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 0.24662\n",
      "Epoch 652/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3034 - acc: 0.8836 - val_loss: 0.2477 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 0.24662\n",
      "Epoch 653/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2983 - acc: 0.8836 - val_loss: 0.2567 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 0.24662\n",
      "Epoch 654/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2936 - acc: 0.8774 - val_loss: 0.2452 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00654: val_loss improved from 0.24662 to 0.24520, saving model to ./model/model.hdf5\n",
      "Epoch 655/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.2906 - acc: 0.8899 - val_loss: 0.2463 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 0.24520\n",
      "Epoch 656/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.2899 - acc: 0.8836 - val_loss: 0.2516 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 0.24520\n",
      "Epoch 657/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2911 - acc: 0.8742 - val_loss: 0.2450 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00657: val_loss improved from 0.24520 to 0.24498, saving model to ./model/model.hdf5\n",
      "Epoch 658/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2931 - acc: 0.8868 - val_loss: 0.2591 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 0.24498\n",
      "Epoch 659/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2948 - acc: 0.8805 - val_loss: 0.2459 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 0.24498\n",
      "Epoch 660/2000\n",
      "318/318 [==============================] - 0s 32us/step - loss: 0.2957 - acc: 0.8868 - val_loss: 0.2600 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 0.24498\n",
      "Epoch 661/2000\n",
      "318/318 [==============================] - 0s 33us/step - loss: 0.2954 - acc: 0.8805 - val_loss: 0.2451 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 0.24498\n",
      "Epoch 662/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2943 - acc: 0.8868 - val_loss: 0.2553 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 0.24498\n",
      "Epoch 663/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2928 - acc: 0.8774 - val_loss: 0.2444 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00663: val_loss improved from 0.24498 to 0.24440, saving model to ./model/model.hdf5\n",
      "Epoch 664/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.2914 - acc: 0.8868 - val_loss: 0.2499 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 0.24440\n",
      "Epoch 665/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2904 - acc: 0.8805 - val_loss: 0.2452 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 0.24440\n",
      "Epoch 666/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.2897 - acc: 0.8836 - val_loss: 0.2465 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 0.24440\n",
      "Epoch 667/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2895 - acc: 0.8774 - val_loss: 0.2474 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 0.24440\n",
      "Epoch 668/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.2895 - acc: 0.8836 - val_loss: 0.2449 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 0.24440\n",
      "Epoch 669/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2898 - acc: 0.8836 - val_loss: 0.2502 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 0.24440\n",
      "Epoch 670/2000\n",
      "318/318 [==============================] - 0s 31us/step - loss: 0.2903 - acc: 0.8805 - val_loss: 0.2444 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00670: val_loss improved from 0.24440 to 0.24439, saving model to ./model/model.hdf5\n",
      "Epoch 671/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2907 - acc: 0.8868 - val_loss: 0.2523 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 0.24439\n",
      "Epoch 672/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2911 - acc: 0.8742 - val_loss: 0.2443 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00672: val_loss improved from 0.24439 to 0.24432, saving model to ./model/model.hdf5\n",
      "Epoch 673/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2913 - acc: 0.8868 - val_loss: 0.2528 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 0.24432\n",
      "Epoch 674/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2913 - acc: 0.8742 - val_loss: 0.2442 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00674: val_loss improved from 0.24432 to 0.24423, saving model to ./model/model.hdf5\n",
      "Epoch 675/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.2912 - acc: 0.8868 - val_loss: 0.2522 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 0.24423\n",
      "Epoch 676/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2910 - acc: 0.8742 - val_loss: 0.2442 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00676: val_loss improved from 0.24423 to 0.24416, saving model to ./model/model.hdf5\n",
      "Epoch 677/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.2907 - acc: 0.8868 - val_loss: 0.2511 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 0.24416\n",
      "Epoch 678/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.2905 - acc: 0.8805 - val_loss: 0.2442 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 0.24416\n",
      "Epoch 679/2000\n",
      "318/318 [==============================] - 0s 30us/step - loss: 0.2902 - acc: 0.8899 - val_loss: 0.2502 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 0.24416\n",
      "Epoch 680/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2900 - acc: 0.8805 - val_loss: 0.2443 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 0.24416\n",
      "Epoch 681/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2899 - acc: 0.8899 - val_loss: 0.2494 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 0.24416\n",
      "Epoch 682/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.2897 - acc: 0.8805 - val_loss: 0.2444 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 0.24416\n",
      "Epoch 683/2000\n",
      "318/318 [==============================] - 0s 39us/step - loss: 0.2895 - acc: 0.8868 - val_loss: 0.2489 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 0.24416\n",
      "Epoch 684/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2894 - acc: 0.8774 - val_loss: 0.2445 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 0.24416\n",
      "Epoch 685/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2893 - acc: 0.8868 - val_loss: 0.2485 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 0.24416\n",
      "Epoch 686/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.2893 - acc: 0.8805 - val_loss: 0.2446 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 0.24416\n",
      "Epoch 687/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2892 - acc: 0.8836 - val_loss: 0.2484 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 0.24416\n",
      "Epoch 688/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.2892 - acc: 0.8805 - val_loss: 0.2445 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 0.24416\n",
      "Epoch 689/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2892 - acc: 0.8836 - val_loss: 0.2487 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 0.24416\n",
      "Epoch 690/2000\n",
      "318/318 [==============================] - 0s 35us/step - loss: 0.2892 - acc: 0.8774 - val_loss: 0.2444 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 0.24416\n",
      "Epoch 691/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2893 - acc: 0.8836 - val_loss: 0.2495 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 0.24416\n",
      "Epoch 692/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2894 - acc: 0.8805 - val_loss: 0.2441 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00692: val_loss improved from 0.24416 to 0.24407, saving model to ./model/model.hdf5\n",
      "Epoch 693/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.2896 - acc: 0.8899 - val_loss: 0.2510 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 0.24407\n",
      "Epoch 694/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2900 - acc: 0.8805 - val_loss: 0.2438 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00694: val_loss improved from 0.24407 to 0.24381, saving model to ./model/model.hdf5\n",
      "Epoch 695/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2905 - acc: 0.8868 - val_loss: 0.2540 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 0.24381\n",
      "Epoch 696/2000\n",
      "318/318 [==============================] - 0s 37us/step - loss: 0.2913 - acc: 0.8774 - val_loss: 0.2441 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 0.24381\n",
      "Epoch 697/2000\n",
      "318/318 [==============================] - 0s 34us/step - loss: 0.2924 - acc: 0.8868 - val_loss: 0.2591 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 0.24381\n",
      "Epoch 698/2000\n",
      "318/318 [==============================] - 0s 39us/step - loss: 0.2940 - acc: 0.8805 - val_loss: 0.2460 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 0.24381\n",
      "Epoch 699/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2964 - acc: 0.8836 - val_loss: 0.2688 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 0.24381\n",
      "Epoch 700/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.2998 - acc: 0.8868 - val_loss: 0.2519 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 0.24381\n",
      "Epoch 701/2000\n",
      "318/318 [==============================] - 0s 36us/step - loss: 0.3051 - acc: 0.8774 - val_loss: 0.2881 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 0.24381\n",
      "Epoch 702/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3126 - acc: 0.8679 - val_loss: 0.2663 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 0.24381\n",
      "Epoch 703/2000\n",
      "318/318 [==============================] - 0s 36us/step - loss: 0.3234 - acc: 0.8742 - val_loss: 0.3208 - val_acc: 0.8375\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 0.24381\n",
      "Epoch 704/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3359 - acc: 0.8459 - val_loss: 0.2877 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 0.24381\n",
      "Epoch 705/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3482 - acc: 0.8616 - val_loss: 0.3457 - val_acc: 0.8375\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 0.24381\n",
      "Epoch 706/2000\n",
      "318/318 [==============================] - 0s 42us/step - loss: 0.3538 - acc: 0.8396 - val_loss: 0.2854 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 0.24381\n",
      "Epoch 707/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3461 - acc: 0.8679 - val_loss: 0.3050 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 0.24381\n",
      "Epoch 708/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3232 - acc: 0.8585 - val_loss: 0.2479 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 0.24381\n",
      "Epoch 709/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.2992 - acc: 0.8805 - val_loss: 0.2470 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 0.24381\n",
      "Epoch 710/2000\n",
      "318/318 [==============================] - 0s 44us/step - loss: 0.2900 - acc: 0.8868 - val_loss: 0.2694 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 0.24381\n",
      "Epoch 711/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2996 - acc: 0.8774 - val_loss: 0.2582 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 0.24381\n",
      "Epoch 712/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3141 - acc: 0.8805 - val_loss: 0.2965 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 0.24381\n",
      "Epoch 713/2000\n",
      "318/318 [==============================] - 0s 32us/step - loss: 0.3178 - acc: 0.8616 - val_loss: 0.2547 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 0.24381\n",
      "Epoch 714/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3103 - acc: 0.8774 - val_loss: 0.2648 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 0.24381\n",
      "Epoch 715/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.2976 - acc: 0.8805 - val_loss: 0.2425 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00715: val_loss improved from 0.24381 to 0.24249, saving model to ./model/model.hdf5\n",
      "Epoch 716/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2894 - acc: 0.8899 - val_loss: 0.2425 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 0.24249\n",
      "Epoch 717/2000\n",
      "318/318 [==============================] - 0s 10us/step - loss: 0.2888 - acc: 0.8899 - val_loss: 0.2586 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 0.24249\n",
      "Epoch 718/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2942 - acc: 0.8836 - val_loss: 0.2500 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 0.24249\n",
      "Epoch 719/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3027 - acc: 0.8805 - val_loss: 0.2848 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 0.24249\n",
      "Epoch 720/2000\n",
      "318/318 [==============================] - 0s 37us/step - loss: 0.3119 - acc: 0.8679 - val_loss: 0.2671 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 0.24249\n",
      "Epoch 721/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3223 - acc: 0.8742 - val_loss: 0.3095 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 0.24249\n",
      "Epoch 722/2000\n",
      "318/318 [==============================] - 0s 39us/step - loss: 0.3296 - acc: 0.8553 - val_loss: 0.2776 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 0.24249\n",
      "Epoch 723/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.3345 - acc: 0.8711 - val_loss: 0.3077 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 0.24249\n",
      "Epoch 724/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3267 - acc: 0.8585 - val_loss: 0.2573 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 0.24249\n",
      "Epoch 725/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3114 - acc: 0.8836 - val_loss: 0.2595 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 0.24249\n",
      "Epoch 726/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2939 - acc: 0.8774 - val_loss: 0.2484 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 0.24249\n",
      "Epoch 727/2000\n",
      "318/318 [==============================] - 0s 35us/step - loss: 0.2891 - acc: 0.8774 - val_loss: 0.2461 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 0.24249\n",
      "Epoch 728/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2973 - acc: 0.8805 - val_loss: 0.2780 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 0.24249\n",
      "Epoch 729/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3053 - acc: 0.8774 - val_loss: 0.2499 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 0.24249\n",
      "Epoch 730/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3042 - acc: 0.8805 - val_loss: 0.2600 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 0.24249\n",
      "Epoch 731/2000\n",
      "318/318 [==============================] - 0s 31us/step - loss: 0.2948 - acc: 0.8774 - val_loss: 0.2421 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00731: val_loss improved from 0.24249 to 0.24205, saving model to ./model/model.hdf5\n",
      "Epoch 732/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2883 - acc: 0.8868 - val_loss: 0.2414 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00732: val_loss improved from 0.24205 to 0.24136, saving model to ./model/model.hdf5\n",
      "Epoch 733/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2896 - acc: 0.8868 - val_loss: 0.2604 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 0.24136\n",
      "Epoch 734/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2954 - acc: 0.8836 - val_loss: 0.2467 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 0.24136\n",
      "Epoch 735/2000\n",
      "318/318 [==============================] - 0s 30us/step - loss: 0.3001 - acc: 0.8836 - val_loss: 0.2670 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 0.24136\n",
      "Epoch 736/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2997 - acc: 0.8836 - val_loss: 0.2442 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 0.24136\n",
      "Epoch 737/2000\n",
      "318/318 [==============================] - 0s 35us/step - loss: 0.2957 - acc: 0.8868 - val_loss: 0.2518 - val_acc: 0.9125\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 0.24136\n",
      "Epoch 738/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2905 - acc: 0.8805 - val_loss: 0.2427 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 0.24136\n",
      "Epoch 739/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2877 - acc: 0.8774 - val_loss: 0.2420 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 0.24136\n",
      "Epoch 740/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2882 - acc: 0.8868 - val_loss: 0.2533 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 0.24136\n",
      "Epoch 741/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2908 - acc: 0.8836 - val_loss: 0.2435 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 0.24136\n",
      "Epoch 742/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.2929 - acc: 0.8868 - val_loss: 0.2573 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 0.24136\n",
      "Epoch 743/2000\n",
      "318/318 [==============================] - 0s 47us/step - loss: 0.2926 - acc: 0.8805 - val_loss: 0.2429 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 0.24136\n",
      "Epoch 744/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2905 - acc: 0.8868 - val_loss: 0.2486 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 0.24136\n",
      "Epoch 745/2000\n",
      "318/318 [==============================] - 0s 44us/step - loss: 0.2882 - acc: 0.8774 - val_loss: 0.2450 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 0.24136\n",
      "Epoch 746/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2873 - acc: 0.8868 - val_loss: 0.2432 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 0.24136\n",
      "Epoch 747/2000\n",
      "318/318 [==============================] - 0s 41us/step - loss: 0.2880 - acc: 0.8899 - val_loss: 0.2517 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 0.24136\n",
      "Epoch 748/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2894 - acc: 0.8774 - val_loss: 0.2432 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 0.24136\n",
      "Epoch 749/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2904 - acc: 0.8868 - val_loss: 0.2535 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 0.24136\n",
      "Epoch 750/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.2904 - acc: 0.8805 - val_loss: 0.2428 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 0.24136\n",
      "Epoch 751/2000\n",
      "318/318 [==============================] - 0s 49us/step - loss: 0.2894 - acc: 0.8868 - val_loss: 0.2489 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 0.24136\n",
      "Epoch 752/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.2882 - acc: 0.8805 - val_loss: 0.2430 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 0.24136\n",
      "Epoch 753/2000\n",
      "318/318 [==============================] - 0s 33us/step - loss: 0.2872 - acc: 0.8805 - val_loss: 0.2440 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 0.24136\n",
      "Epoch 754/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2869 - acc: 0.8836 - val_loss: 0.2462 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 0.24136\n",
      "Epoch 755/2000\n",
      "318/318 [==============================] - 0s 34us/step - loss: 0.2872 - acc: 0.8836 - val_loss: 0.2422 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 0.24136\n",
      "Epoch 756/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2878 - acc: 0.8899 - val_loss: 0.2489 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 0.24136\n",
      "Epoch 757/2000\n",
      "318/318 [==============================] - 0s 47us/step - loss: 0.2881 - acc: 0.8805 - val_loss: 0.2420 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 0.24136\n",
      "Epoch 758/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2882 - acc: 0.8868 - val_loss: 0.2482 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 0.24136\n",
      "Epoch 759/2000\n",
      "318/318 [==============================] - 0s 44us/step - loss: 0.2878 - acc: 0.8805 - val_loss: 0.2424 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 0.24136\n",
      "Epoch 760/2000\n",
      "318/318 [==============================] - 0s 31us/step - loss: 0.2874 - acc: 0.8899 - val_loss: 0.2458 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 0.24136\n",
      "Epoch 761/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.2869 - acc: 0.8836 - val_loss: 0.2437 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 0.24136\n",
      "Epoch 762/2000\n",
      "318/318 [==============================] - 0s 38us/step - loss: 0.2867 - acc: 0.8805 - val_loss: 0.2439 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 0.24136\n",
      "Epoch 763/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.2866 - acc: 0.8836 - val_loss: 0.2459 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 0.24136\n",
      "Epoch 764/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2867 - acc: 0.8805 - val_loss: 0.2431 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 0.24136\n",
      "Epoch 765/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.2869 - acc: 0.8836 - val_loss: 0.2474 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 0.24136\n",
      "Epoch 766/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2871 - acc: 0.8774 - val_loss: 0.2429 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 0.24136\n",
      "Epoch 767/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.2872 - acc: 0.8899 - val_loss: 0.2479 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 0.24136\n",
      "Epoch 768/2000\n",
      "318/318 [==============================] - 0s 39us/step - loss: 0.2872 - acc: 0.8805 - val_loss: 0.2428 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 0.24136\n",
      "Epoch 769/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2871 - acc: 0.8899 - val_loss: 0.2471 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 0.24136\n",
      "Epoch 770/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2869 - acc: 0.8805 - val_loss: 0.2430 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 0.24136\n",
      "Epoch 771/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2867 - acc: 0.8836 - val_loss: 0.2458 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 0.24136\n",
      "Epoch 772/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.2865 - acc: 0.8805 - val_loss: 0.2434 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 0.24136\n",
      "Epoch 773/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2864 - acc: 0.8805 - val_loss: 0.2446 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 0.24136\n",
      "Epoch 774/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2863 - acc: 0.8836 - val_loss: 0.2441 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 0.24136\n",
      "Epoch 775/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.2862 - acc: 0.8836 - val_loss: 0.2438 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 0.24136\n",
      "Epoch 776/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2862 - acc: 0.8836 - val_loss: 0.2448 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 0.24136\n",
      "Epoch 777/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2862 - acc: 0.8836 - val_loss: 0.2434 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 0.24136\n",
      "Epoch 778/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2862 - acc: 0.8805 - val_loss: 0.2454 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 0.24136\n",
      "Epoch 779/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2863 - acc: 0.8805 - val_loss: 0.2431 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 0.24136\n",
      "Epoch 780/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2863 - acc: 0.8836 - val_loss: 0.2460 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 0.24136\n",
      "Epoch 781/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2863 - acc: 0.8805 - val_loss: 0.2429 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 0.24136\n",
      "Epoch 782/2000\n",
      "318/318 [==============================] - 0s 31us/step - loss: 0.2863 - acc: 0.8836 - val_loss: 0.2464 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 0.24136\n",
      "Epoch 783/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2864 - acc: 0.8774 - val_loss: 0.2428 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 0.24136\n",
      "Epoch 784/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.2864 - acc: 0.8836 - val_loss: 0.2467 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 0.24136\n",
      "Epoch 785/2000\n",
      "318/318 [==============================] - 0s 31us/step - loss: 0.2864 - acc: 0.8774 - val_loss: 0.2427 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 0.24136\n",
      "Epoch 786/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.2864 - acc: 0.8836 - val_loss: 0.2467 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 0.24136\n",
      "Epoch 787/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2863 - acc: 0.8774 - val_loss: 0.2425 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 0.24136\n",
      "Epoch 788/2000\n",
      "318/318 [==============================] - 0s 33us/step - loss: 0.2863 - acc: 0.8836 - val_loss: 0.2466 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 0.24136\n",
      "Epoch 789/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.2863 - acc: 0.8774 - val_loss: 0.2425 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 0.24136\n",
      "Epoch 790/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2863 - acc: 0.8836 - val_loss: 0.2466 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 0.24136\n",
      "Epoch 791/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.2862 - acc: 0.8774 - val_loss: 0.2424 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 0.24136\n",
      "Epoch 792/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2862 - acc: 0.8836 - val_loss: 0.2469 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 0.24136\n",
      "Epoch 793/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2863 - acc: 0.8774 - val_loss: 0.2423 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 0.24136\n",
      "Epoch 794/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2863 - acc: 0.8836 - val_loss: 0.2473 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 0.24136\n",
      "Epoch 795/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.2863 - acc: 0.8805 - val_loss: 0.2422 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 0.24136\n",
      "Epoch 796/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.2864 - acc: 0.8899 - val_loss: 0.2481 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 0.24136\n",
      "Epoch 797/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2865 - acc: 0.8805 - val_loss: 0.2420 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 0.24136\n",
      "Epoch 798/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2868 - acc: 0.8899 - val_loss: 0.2496 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 0.24136\n",
      "Epoch 799/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2871 - acc: 0.8805 - val_loss: 0.2419 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 0.24136\n",
      "Epoch 800/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.2876 - acc: 0.8868 - val_loss: 0.2522 - val_acc: 0.9125\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 0.24136\n",
      "Epoch 801/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.2882 - acc: 0.8774 - val_loss: 0.2421 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 0.24136\n",
      "Epoch 802/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2893 - acc: 0.8868 - val_loss: 0.2570 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 0.24136\n",
      "Epoch 803/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2908 - acc: 0.8774 - val_loss: 0.2438 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 0.24136\n",
      "Epoch 804/2000\n",
      "318/318 [==============================] - 0s 34us/step - loss: 0.2932 - acc: 0.8868 - val_loss: 0.2665 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 0.24136\n",
      "Epoch 805/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.2964 - acc: 0.8868 - val_loss: 0.2489 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 0.24136\n",
      "Epoch 806/2000\n",
      "318/318 [==============================] - 0s 30us/step - loss: 0.3014 - acc: 0.8836 - val_loss: 0.2834 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 0.24136\n",
      "Epoch 807/2000\n",
      "318/318 [==============================] - 0s 42us/step - loss: 0.3074 - acc: 0.8711 - val_loss: 0.2595 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 0.24136\n",
      "Epoch 808/2000\n",
      "318/318 [==============================] - 0s 37us/step - loss: 0.3157 - acc: 0.8774 - val_loss: 0.3060 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 0.24136\n",
      "Epoch 809/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3229 - acc: 0.8648 - val_loss: 0.2700 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 0.24136\n",
      "Epoch 810/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.3294 - acc: 0.8742 - val_loss: 0.3137 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 0.24136\n",
      "Epoch 811/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.3281 - acc: 0.8585 - val_loss: 0.2614 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 0.24136\n",
      "Epoch 812/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3204 - acc: 0.8774 - val_loss: 0.2800 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 0.24136\n",
      "Epoch 813/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.3049 - acc: 0.8742 - val_loss: 0.2413 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00813: val_loss improved from 0.24136 to 0.24126, saving model to ./model/model.hdf5\n",
      "Epoch 814/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2914 - acc: 0.8868 - val_loss: 0.2434 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 0.24126\n",
      "Epoch 815/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2860 - acc: 0.8836 - val_loss: 0.2549 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 0.24126\n",
      "Epoch 816/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.2898 - acc: 0.8774 - val_loss: 0.2456 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 0.24126\n",
      "Epoch 817/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2985 - acc: 0.8805 - val_loss: 0.2815 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 0.24126\n",
      "Epoch 818/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3065 - acc: 0.8742 - val_loss: 0.2575 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 0.24126\n",
      "Epoch 819/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3134 - acc: 0.8774 - val_loss: 0.2947 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 0.24126\n",
      "Epoch 820/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.3163 - acc: 0.8648 - val_loss: 0.2637 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 0.24126\n",
      "Epoch 821/2000\n",
      "318/318 [==============================] - 0s 30us/step - loss: 0.3200 - acc: 0.8774 - val_loss: 0.3002 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 0.24126\n",
      "Epoch 822/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.3205 - acc: 0.8679 - val_loss: 0.2663 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 0.24126\n",
      "Epoch 823/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.3230 - acc: 0.8742 - val_loss: 0.2990 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 0.24126\n",
      "Epoch 824/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3189 - acc: 0.8679 - val_loss: 0.2579 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 0.24126\n",
      "Epoch 825/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3133 - acc: 0.8774 - val_loss: 0.2734 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 0.24126\n",
      "Epoch 826/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.3005 - acc: 0.8805 - val_loss: 0.2417 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 0.24126\n",
      "Epoch 827/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2897 - acc: 0.8868 - val_loss: 0.2434 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 0.24126\n",
      "Epoch 828/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2855 - acc: 0.8836 - val_loss: 0.2541 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 0.24126\n",
      "Epoch 829/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2891 - acc: 0.8774 - val_loss: 0.2446 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 0.24126\n",
      "Epoch 830/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.2959 - acc: 0.8805 - val_loss: 0.2715 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 0.24126\n",
      "Epoch 831/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2998 - acc: 0.8836 - val_loss: 0.2482 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 0.24126\n",
      "Epoch 832/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3010 - acc: 0.8805 - val_loss: 0.2672 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 0.24126\n",
      "Epoch 833/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.2976 - acc: 0.8868 - val_loss: 0.2434 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 0.24126\n",
      "Epoch 834/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2939 - acc: 0.8836 - val_loss: 0.2539 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 0.24126\n",
      "Epoch 835/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.2896 - acc: 0.8774 - val_loss: 0.2398 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00835: val_loss improved from 0.24126 to 0.23979, saving model to ./model/model.hdf5\n",
      "Epoch 836/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2865 - acc: 0.8868 - val_loss: 0.2437 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 0.23979\n",
      "Epoch 837/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2847 - acc: 0.8899 - val_loss: 0.2418 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 0.23979\n",
      "Epoch 838/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.2843 - acc: 0.8836 - val_loss: 0.2397 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00838: val_loss improved from 0.23979 to 0.23974, saving model to ./model/model.hdf5\n",
      "Epoch 839/2000\n",
      "318/318 [==============================] - 0s 36us/step - loss: 0.2849 - acc: 0.8899 - val_loss: 0.2477 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 0.23974\n",
      "Epoch 840/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2862 - acc: 0.8742 - val_loss: 0.2397 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00840: val_loss improved from 0.23974 to 0.23974, saving model to ./model/model.hdf5\n",
      "Epoch 841/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.2876 - acc: 0.8868 - val_loss: 0.2526 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 0.23974\n",
      "Epoch 842/2000\n",
      "318/318 [==============================] - 0s 30us/step - loss: 0.2886 - acc: 0.8774 - val_loss: 0.2406 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 0.23974\n",
      "Epoch 843/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.2895 - acc: 0.8868 - val_loss: 0.2545 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 0.23974\n",
      "Epoch 844/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2895 - acc: 0.8774 - val_loss: 0.2407 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 0.23974\n",
      "Epoch 845/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2894 - acc: 0.8868 - val_loss: 0.2532 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 0.23974\n",
      "Epoch 846/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2887 - acc: 0.8774 - val_loss: 0.2402 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 0.23974\n",
      "Epoch 847/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2881 - acc: 0.8868 - val_loss: 0.2506 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 0.23974\n",
      "Epoch 848/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2873 - acc: 0.8774 - val_loss: 0.2398 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 0.23974\n",
      "Epoch 849/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.2867 - acc: 0.8868 - val_loss: 0.2485 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 0.23974\n",
      "Epoch 850/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2862 - acc: 0.8805 - val_loss: 0.2396 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00850: val_loss improved from 0.23974 to 0.23959, saving model to ./model/model.hdf5\n",
      "Epoch 851/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2858 - acc: 0.8868 - val_loss: 0.2470 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 0.23959\n",
      "Epoch 852/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.2855 - acc: 0.8805 - val_loss: 0.2393 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00852: val_loss improved from 0.23959 to 0.23930, saving model to ./model/model.hdf5\n",
      "Epoch 853/2000\n",
      "318/318 [==============================] - 0s 12us/step - loss: 0.2852 - acc: 0.8868 - val_loss: 0.2459 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 0.23930\n",
      "Epoch 854/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.2850 - acc: 0.8836 - val_loss: 0.2392 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00854: val_loss improved from 0.23930 to 0.23916, saving model to ./model/model.hdf5\n",
      "Epoch 855/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.2849 - acc: 0.8868 - val_loss: 0.2453 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 0.23916\n",
      "Epoch 856/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2847 - acc: 0.8836 - val_loss: 0.2391 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00856: val_loss improved from 0.23916 to 0.23914, saving model to ./model/model.hdf5\n",
      "Epoch 857/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2846 - acc: 0.8868 - val_loss: 0.2447 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 0.23914\n",
      "Epoch 858/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.2845 - acc: 0.8868 - val_loss: 0.2391 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 0.23914\n",
      "Epoch 859/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.2844 - acc: 0.8899 - val_loss: 0.2441 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 0.23914\n",
      "Epoch 860/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2842 - acc: 0.8868 - val_loss: 0.2392 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 0.23914\n",
      "Epoch 861/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.2841 - acc: 0.8899 - val_loss: 0.2438 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 0.23914\n",
      "Epoch 862/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2841 - acc: 0.8868 - val_loss: 0.2392 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 0.23914\n",
      "Epoch 863/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.2840 - acc: 0.8899 - val_loss: 0.2439 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 0.23914\n",
      "Epoch 864/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.2840 - acc: 0.8868 - val_loss: 0.2392 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 0.23914\n",
      "Epoch 865/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2841 - acc: 0.8899 - val_loss: 0.2445 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 0.23914\n",
      "Epoch 866/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2842 - acc: 0.8868 - val_loss: 0.2390 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00866: val_loss improved from 0.23914 to 0.23900, saving model to ./model/model.hdf5\n",
      "Epoch 867/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2844 - acc: 0.8868 - val_loss: 0.2460 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 0.23900\n",
      "Epoch 868/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2847 - acc: 0.8836 - val_loss: 0.2389 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00868: val_loss improved from 0.23900 to 0.23891, saving model to ./model/model.hdf5\n",
      "Epoch 869/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.2853 - acc: 0.8868 - val_loss: 0.2488 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 0.23891\n",
      "Epoch 870/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.2860 - acc: 0.8742 - val_loss: 0.2392 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 0.23891\n",
      "Epoch 871/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2872 - acc: 0.8868 - val_loss: 0.2539 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 0.23891\n",
      "Epoch 872/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.2887 - acc: 0.8774 - val_loss: 0.2410 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 0.23891\n",
      "Epoch 873/2000\n",
      "318/318 [==============================] - 0s 38us/step - loss: 0.2912 - acc: 0.8836 - val_loss: 0.2625 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 0.23891\n",
      "Epoch 874/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2939 - acc: 0.8836 - val_loss: 0.2456 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 0.23891\n",
      "Epoch 875/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2984 - acc: 0.8868 - val_loss: 0.2748 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 0.23891\n",
      "Epoch 876/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3020 - acc: 0.8805 - val_loss: 0.2528 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 0.23891\n",
      "Epoch 877/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.3080 - acc: 0.8805 - val_loss: 0.2868 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 0.23891\n",
      "Epoch 878/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3103 - acc: 0.8648 - val_loss: 0.2568 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 0.23891\n",
      "Epoch 879/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.3132 - acc: 0.8774 - val_loss: 0.2846 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 0.23891\n",
      "Epoch 880/2000\n",
      "318/318 [==============================] - 0s 35us/step - loss: 0.3083 - acc: 0.8679 - val_loss: 0.2480 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 0.23891\n",
      "Epoch 881/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3022 - acc: 0.8868 - val_loss: 0.2606 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 0.23891\n",
      "Epoch 882/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2923 - acc: 0.8774 - val_loss: 0.2385 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00882: val_loss improved from 0.23891 to 0.23847, saving model to ./model/model.hdf5\n",
      "Epoch 883/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.2854 - acc: 0.8899 - val_loss: 0.2396 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 0.23847\n",
      "Epoch 884/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2836 - acc: 0.8899 - val_loss: 0.2499 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 0.23847\n",
      "Epoch 885/2000\n",
      "318/318 [==============================] - 0s 36us/step - loss: 0.2867 - acc: 0.8774 - val_loss: 0.2402 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 0.23847\n",
      "Epoch 886/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2919 - acc: 0.8836 - val_loss: 0.2652 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 0.23847\n",
      "Epoch 887/2000\n",
      "318/318 [==============================] - 0s 40us/step - loss: 0.2957 - acc: 0.8868 - val_loss: 0.2450 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 0.23847\n",
      "Epoch 888/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.2984 - acc: 0.8805 - val_loss: 0.2682 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 0.23847\n",
      "Epoch 889/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.2978 - acc: 0.8868 - val_loss: 0.2439 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 0.23847\n",
      "Epoch 890/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.2964 - acc: 0.8836 - val_loss: 0.2612 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 0.23847\n",
      "Epoch 891/2000\n",
      "318/318 [==============================] - 0s 33us/step - loss: 0.2930 - acc: 0.8868 - val_loss: 0.2398 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 0.23847\n",
      "Epoch 892/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.2898 - acc: 0.8868 - val_loss: 0.2500 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 0.23847\n",
      "Epoch 893/2000\n",
      "318/318 [==============================] - 0s 40us/step - loss: 0.2864 - acc: 0.8774 - val_loss: 0.2382 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00893: val_loss improved from 0.23847 to 0.23819, saving model to ./model/model.hdf5\n",
      "Epoch 894/2000\n",
      "318/318 [==============================] - 0s 31us/step - loss: 0.2840 - acc: 0.8899 - val_loss: 0.2413 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 0.23819\n",
      "Epoch 895/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2829 - acc: 0.8899 - val_loss: 0.2425 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 0.23819\n",
      "Epoch 896/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2830 - acc: 0.8836 - val_loss: 0.2385 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 0.23819\n",
      "Epoch 897/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2841 - acc: 0.8868 - val_loss: 0.2489 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 0.23819\n",
      "Epoch 898/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.2856 - acc: 0.8742 - val_loss: 0.2392 - val_acc: 0.9125\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 0.23819\n",
      "Epoch 899/2000\n",
      "318/318 [==============================] - 0s 40us/step - loss: 0.2871 - acc: 0.8868 - val_loss: 0.2538 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 0.23819\n",
      "Epoch 900/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2882 - acc: 0.8805 - val_loss: 0.2403 - val_acc: 0.9125\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 0.23819\n",
      "Epoch 901/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.2891 - acc: 0.8868 - val_loss: 0.2560 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 0.23819\n",
      "Epoch 902/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.2892 - acc: 0.8774 - val_loss: 0.2404 - val_acc: 0.9125\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 0.23819\n",
      "Epoch 903/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.2893 - acc: 0.8868 - val_loss: 0.2551 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 0.23819\n",
      "Epoch 904/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2885 - acc: 0.8774 - val_loss: 0.2392 - val_acc: 0.9125\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 0.23819\n",
      "Epoch 905/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2875 - acc: 0.8868 - val_loss: 0.2504 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 0.23819\n",
      "Epoch 906/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2860 - acc: 0.8774 - val_loss: 0.2382 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00906: val_loss improved from 0.23819 to 0.23817, saving model to ./model/model.hdf5\n",
      "Epoch 907/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2847 - acc: 0.8868 - val_loss: 0.2449 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 0.23817\n",
      "Epoch 908/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2835 - acc: 0.8774 - val_loss: 0.2387 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 0.23817\n",
      "Epoch 909/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.2827 - acc: 0.8836 - val_loss: 0.2408 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 0.23817\n",
      "Epoch 910/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2824 - acc: 0.8899 - val_loss: 0.2408 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 0.23817\n",
      "Epoch 911/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2823 - acc: 0.8899 - val_loss: 0.2387 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 0.23817\n",
      "Epoch 912/2000\n",
      "318/318 [==============================] - 0s 49us/step - loss: 0.2826 - acc: 0.8836 - val_loss: 0.2436 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 0.23817\n",
      "Epoch 913/2000\n",
      "318/318 [==============================] - 0s 50us/step - loss: 0.2830 - acc: 0.8774 - val_loss: 0.2381 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00913: val_loss improved from 0.23817 to 0.23806, saving model to ./model/model.hdf5\n",
      "Epoch 914/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.2835 - acc: 0.8868 - val_loss: 0.2468 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 0.23806\n",
      "Epoch 915/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.2842 - acc: 0.8805 - val_loss: 0.2381 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 0.23806\n",
      "Epoch 916/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.2850 - acc: 0.8868 - val_loss: 0.2504 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 0.23806\n",
      "Epoch 917/2000\n",
      "318/318 [==============================] - 0s 35us/step - loss: 0.2859 - acc: 0.8742 - val_loss: 0.2387 - val_acc: 0.9125\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 0.23806\n",
      "Epoch 918/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2870 - acc: 0.8868 - val_loss: 0.2547 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 0.23806\n",
      "Epoch 919/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2881 - acc: 0.8805 - val_loss: 0.2397 - val_acc: 0.9125\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 0.23806\n",
      "Epoch 920/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.2894 - acc: 0.8868 - val_loss: 0.2586 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 0.23806\n",
      "Epoch 921/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.2902 - acc: 0.8836 - val_loss: 0.2408 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 0.23806\n",
      "Epoch 922/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2913 - acc: 0.8868 - val_loss: 0.2610 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 0.23806\n",
      "Epoch 923/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2916 - acc: 0.8805 - val_loss: 0.2413 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 0.23806\n",
      "Epoch 924/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2922 - acc: 0.8868 - val_loss: 0.2614 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 0.23806\n",
      "Epoch 925/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2918 - acc: 0.8805 - val_loss: 0.2409 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 0.23806\n",
      "Epoch 926/2000\n",
      "318/318 [==============================] - 0s 30us/step - loss: 0.2919 - acc: 0.8868 - val_loss: 0.2600 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 0.23806\n",
      "Epoch 927/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2909 - acc: 0.8805 - val_loss: 0.2398 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 0.23806\n",
      "Epoch 928/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2904 - acc: 0.8868 - val_loss: 0.2565 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 0.23806\n",
      "Epoch 929/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2889 - acc: 0.8805 - val_loss: 0.2382 - val_acc: 0.9125\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 0.23806\n",
      "Epoch 930/2000\n",
      "318/318 [==============================] - 0s 35us/step - loss: 0.2877 - acc: 0.8868 - val_loss: 0.2513 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 0.23806\n",
      "Epoch 931/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2861 - acc: 0.8774 - val_loss: 0.2371 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00931: val_loss improved from 0.23806 to 0.23713, saving model to ./model/model.hdf5\n",
      "Epoch 932/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.2848 - acc: 0.8868 - val_loss: 0.2461 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 0.23713\n",
      "Epoch 933/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2836 - acc: 0.8742 - val_loss: 0.2372 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 0.23713\n",
      "Epoch 934/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.2828 - acc: 0.8899 - val_loss: 0.2424 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 0.23713\n",
      "Epoch 935/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2822 - acc: 0.8868 - val_loss: 0.2381 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 0.23713\n",
      "Epoch 936/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.2818 - acc: 0.8868 - val_loss: 0.2402 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 0.23713\n",
      "Epoch 937/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2816 - acc: 0.8899 - val_loss: 0.2393 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 0.23713\n",
      "Epoch 938/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2816 - acc: 0.8868 - val_loss: 0.2390 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 0.23713\n",
      "Epoch 939/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.2816 - acc: 0.8836 - val_loss: 0.2406 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 0.23713\n",
      "Epoch 940/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.2816 - acc: 0.8899 - val_loss: 0.2382 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 0.23713\n",
      "Epoch 941/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2817 - acc: 0.8836 - val_loss: 0.2422 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 0.23713\n",
      "Epoch 942/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2819 - acc: 0.8868 - val_loss: 0.2375 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 0.23713\n",
      "Epoch 943/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2822 - acc: 0.8899 - val_loss: 0.2445 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 0.23713\n",
      "Epoch 944/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2826 - acc: 0.8836 - val_loss: 0.2371 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00944: val_loss improved from 0.23713 to 0.23712, saving model to ./model/model.hdf5\n",
      "Epoch 945/2000\n",
      "318/318 [==============================] - 0s 30us/step - loss: 0.2832 - acc: 0.8899 - val_loss: 0.2479 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 0.23712\n",
      "Epoch 946/2000\n",
      "318/318 [==============================] - 0s 43us/step - loss: 0.2839 - acc: 0.8742 - val_loss: 0.2374 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 0.23712\n",
      "Epoch 947/2000\n",
      "318/318 [==============================] - 0s 44us/step - loss: 0.2852 - acc: 0.8868 - val_loss: 0.2534 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 0.23712\n",
      "Epoch 948/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.2867 - acc: 0.8805 - val_loss: 0.2391 - val_acc: 0.9125\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 0.23712\n",
      "Epoch 949/2000\n",
      "318/318 [==============================] - 0s 31us/step - loss: 0.2893 - acc: 0.8868 - val_loss: 0.2631 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 0.23712\n",
      "Epoch 950/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2924 - acc: 0.8868 - val_loss: 0.2443 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 0.23712\n",
      "Epoch 951/2000\n",
      "318/318 [==============================] - 0s 35us/step - loss: 0.2978 - acc: 0.8868 - val_loss: 0.2804 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 0.23712\n",
      "Epoch 952/2000\n",
      "318/318 [==============================] - 0s 46us/step - loss: 0.3036 - acc: 0.8774 - val_loss: 0.2559 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 0.23712\n",
      "Epoch 953/2000\n",
      "318/318 [==============================] - 0s 63us/step - loss: 0.3138 - acc: 0.8774 - val_loss: 0.3065 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 0.23712\n",
      "Epoch 954/2000\n",
      "318/318 [==============================] - 0s 11us/step - loss: 0.3217 - acc: 0.8679 - val_loss: 0.2721 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 0.23712\n",
      "Epoch 955/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3336 - acc: 0.8742 - val_loss: 0.3237 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 0.23712\n",
      "Epoch 956/2000\n",
      "318/318 [==============================] - 0s 36us/step - loss: 0.3338 - acc: 0.8491 - val_loss: 0.2689 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 0.23712\n",
      "Epoch 957/2000\n",
      "318/318 [==============================] - 0s 38us/step - loss: 0.3305 - acc: 0.8774 - val_loss: 0.2921 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 0.23712\n",
      "Epoch 958/2000\n",
      "318/318 [==============================] - 0s 31us/step - loss: 0.3106 - acc: 0.8774 - val_loss: 0.2398 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 0.23712\n",
      "Epoch 959/2000\n",
      "318/318 [==============================] - 0s 59us/step - loss: 0.2923 - acc: 0.8868 - val_loss: 0.2419 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 0.23712\n",
      "Epoch 960/2000\n",
      "318/318 [==============================] - 0s 50us/step - loss: 0.2828 - acc: 0.8868 - val_loss: 0.2541 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 0.23712\n",
      "Epoch 961/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2874 - acc: 0.8774 - val_loss: 0.2438 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 0.23712\n",
      "Epoch 962/2000\n",
      "318/318 [==============================] - 0s 36us/step - loss: 0.2995 - acc: 0.8836 - val_loss: 0.2860 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 0.23712\n",
      "Epoch 963/2000\n",
      "318/318 [==============================] - 0s 49us/step - loss: 0.3077 - acc: 0.8679 - val_loss: 0.2541 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 0.23712\n",
      "Epoch 964/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3125 - acc: 0.8774 - val_loss: 0.2872 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 0.23712\n",
      "Epoch 965/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.3094 - acc: 0.8679 - val_loss: 0.2506 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 0.23712\n",
      "Epoch 966/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.3071 - acc: 0.8774 - val_loss: 0.2745 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 0.23712\n",
      "Epoch 967/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.3007 - acc: 0.8774 - val_loss: 0.2419 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 0.23712\n",
      "Epoch 968/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2956 - acc: 0.8805 - val_loss: 0.2567 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 0.23712\n",
      "Epoch 969/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2889 - acc: 0.8805 - val_loss: 0.2357 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00969: val_loss improved from 0.23712 to 0.23566, saving model to ./model/model.hdf5\n",
      "Epoch 970/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.2839 - acc: 0.8868 - val_loss: 0.2404 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 0.23566\n",
      "Epoch 971/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.2812 - acc: 0.8868 - val_loss: 0.2411 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 0.23566\n",
      "Epoch 972/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.2813 - acc: 0.8868 - val_loss: 0.2357 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 0.23566\n",
      "Epoch 973/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.2835 - acc: 0.8899 - val_loss: 0.2518 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 0.23566\n",
      "Epoch 974/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2858 - acc: 0.8774 - val_loss: 0.2374 - val_acc: 0.9125\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 0.23566\n",
      "Epoch 975/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.2876 - acc: 0.8868 - val_loss: 0.2550 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 0.23566\n",
      "Epoch 976/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2876 - acc: 0.8805 - val_loss: 0.2375 - val_acc: 0.9125\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 0.23566\n",
      "Epoch 977/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2870 - acc: 0.8868 - val_loss: 0.2511 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 0.23566\n",
      "Epoch 978/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2854 - acc: 0.8805 - val_loss: 0.2365 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 0.23566\n",
      "Epoch 979/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2839 - acc: 0.8868 - val_loss: 0.2451 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 0.23566\n",
      "Epoch 980/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.2823 - acc: 0.8805 - val_loss: 0.2369 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 0.23566\n",
      "Epoch 981/2000\n",
      "318/318 [==============================] - 0s 37us/step - loss: 0.2813 - acc: 0.8868 - val_loss: 0.2402 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 0.23566\n",
      "Epoch 982/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.2806 - acc: 0.8836 - val_loss: 0.2391 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 0.23566\n",
      "Epoch 983/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.2805 - acc: 0.8899 - val_loss: 0.2376 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 0.23566\n",
      "Epoch 984/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2806 - acc: 0.8868 - val_loss: 0.2419 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 0.23566\n",
      "Epoch 985/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2810 - acc: 0.8836 - val_loss: 0.2366 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 0.23566\n",
      "Epoch 986/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2815 - acc: 0.8899 - val_loss: 0.2444 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 0.23566\n",
      "Epoch 987/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2820 - acc: 0.8805 - val_loss: 0.2364 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 0.23566\n",
      "Epoch 988/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2823 - acc: 0.8868 - val_loss: 0.2458 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 0.23566\n",
      "Epoch 989/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2824 - acc: 0.8774 - val_loss: 0.2364 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 0.23566\n",
      "Epoch 990/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.2825 - acc: 0.8868 - val_loss: 0.2456 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 0.23566\n",
      "Epoch 991/2000\n",
      "318/318 [==============================] - 0s 36us/step - loss: 0.2823 - acc: 0.8774 - val_loss: 0.2363 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 0.23566\n",
      "Epoch 992/2000\n",
      "318/318 [==============================] - 0s 44us/step - loss: 0.2820 - acc: 0.8899 - val_loss: 0.2443 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 0.23566\n",
      "Epoch 993/2000\n",
      "318/318 [==============================] - 0s 38us/step - loss: 0.2817 - acc: 0.8836 - val_loss: 0.2365 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 0.23566\n",
      "Epoch 994/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2813 - acc: 0.8868 - val_loss: 0.2424 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 0.23566\n",
      "Epoch 995/2000\n",
      "318/318 [==============================] - 0s 33us/step - loss: 0.2809 - acc: 0.8836 - val_loss: 0.2369 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 0.23566\n",
      "Epoch 996/2000\n",
      "318/318 [==============================] - 0s 35us/step - loss: 0.2806 - acc: 0.8836 - val_loss: 0.2407 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 0.23566\n",
      "Epoch 997/2000\n",
      "318/318 [==============================] - 0s 37us/step - loss: 0.2804 - acc: 0.8836 - val_loss: 0.2374 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 0.23566\n",
      "Epoch 998/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.2803 - acc: 0.8868 - val_loss: 0.2398 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 0.23566\n",
      "Epoch 999/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2802 - acc: 0.8868 - val_loss: 0.2378 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 0.23566\n",
      "Epoch 1000/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2801 - acc: 0.8868 - val_loss: 0.2392 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 0.23566\n",
      "Epoch 1001/2000\n",
      "318/318 [==============================] - 0s 40us/step - loss: 0.2800 - acc: 0.8899 - val_loss: 0.2382 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01001: val_loss did not improve from 0.23566\n",
      "Epoch 1002/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.2800 - acc: 0.8899 - val_loss: 0.2389 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01002: val_loss did not improve from 0.23566\n",
      "Epoch 1003/2000\n",
      "318/318 [==============================] - 0s 33us/step - loss: 0.2800 - acc: 0.8899 - val_loss: 0.2385 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01003: val_loss did not improve from 0.23566\n",
      "Epoch 1004/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2799 - acc: 0.8899 - val_loss: 0.2386 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01004: val_loss did not improve from 0.23566\n",
      "Epoch 1005/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.2799 - acc: 0.8899 - val_loss: 0.2387 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01005: val_loss did not improve from 0.23566\n",
      "Epoch 1006/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.2799 - acc: 0.8899 - val_loss: 0.2384 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01006: val_loss did not improve from 0.23566\n",
      "Epoch 1007/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.2799 - acc: 0.8899 - val_loss: 0.2389 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01007: val_loss did not improve from 0.23566\n",
      "Epoch 1008/2000\n",
      "318/318 [==============================] - 0s 30us/step - loss: 0.2798 - acc: 0.8899 - val_loss: 0.2381 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01008: val_loss did not improve from 0.23566\n",
      "Epoch 1009/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.2798 - acc: 0.8899 - val_loss: 0.2391 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01009: val_loss did not improve from 0.23566\n",
      "Epoch 1010/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.2798 - acc: 0.8868 - val_loss: 0.2378 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01010: val_loss did not improve from 0.23566\n",
      "Epoch 1011/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.2798 - acc: 0.8868 - val_loss: 0.2394 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01011: val_loss did not improve from 0.23566\n",
      "Epoch 1012/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2798 - acc: 0.8868 - val_loss: 0.2376 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01012: val_loss did not improve from 0.23566\n",
      "Epoch 1013/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2798 - acc: 0.8868 - val_loss: 0.2400 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01013: val_loss did not improve from 0.23566\n",
      "Epoch 1014/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.2798 - acc: 0.8836 - val_loss: 0.2372 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01014: val_loss did not improve from 0.23566\n",
      "Epoch 1015/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.2799 - acc: 0.8868 - val_loss: 0.2407 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01015: val_loss did not improve from 0.23566\n",
      "Epoch 1016/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2799 - acc: 0.8836 - val_loss: 0.2367 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01016: val_loss did not improve from 0.23566\n",
      "Epoch 1017/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2801 - acc: 0.8868 - val_loss: 0.2421 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01017: val_loss did not improve from 0.23566\n",
      "Epoch 1018/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2803 - acc: 0.8836 - val_loss: 0.2362 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01018: val_loss did not improve from 0.23566\n",
      "Epoch 1019/2000\n",
      "318/318 [==============================] - 0s 30us/step - loss: 0.2807 - acc: 0.8868 - val_loss: 0.2450 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01019: val_loss did not improve from 0.23566\n",
      "Epoch 1020/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2814 - acc: 0.8805 - val_loss: 0.2361 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01020: val_loss did not improve from 0.23566\n",
      "Epoch 1021/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2826 - acc: 0.8868 - val_loss: 0.2513 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01021: val_loss did not improve from 0.23566\n",
      "Epoch 1022/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.2844 - acc: 0.8774 - val_loss: 0.2379 - val_acc: 0.9125\n",
      "\n",
      "Epoch 01022: val_loss did not improve from 0.23566\n",
      "Epoch 1023/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2877 - acc: 0.8868 - val_loss: 0.2650 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01023: val_loss did not improve from 0.23566\n",
      "Epoch 1024/2000\n",
      "318/318 [==============================] - 0s 32us/step - loss: 0.2924 - acc: 0.8868 - val_loss: 0.2465 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01024: val_loss did not improve from 0.23566\n",
      "Epoch 1025/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3012 - acc: 0.8805 - val_loss: 0.2939 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01025: val_loss did not improve from 0.23566\n",
      "Epoch 1026/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.3120 - acc: 0.8679 - val_loss: 0.2712 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01026: val_loss did not improve from 0.23566\n",
      "Epoch 1027/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.3326 - acc: 0.8742 - val_loss: 0.3455 - val_acc: 0.8375\n",
      "\n",
      "Epoch 01027: val_loss did not improve from 0.23566\n",
      "Epoch 1028/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3494 - acc: 0.8491 - val_loss: 0.3037 - val_acc: 0.8625\n",
      "\n",
      "Epoch 01028: val_loss did not improve from 0.23566\n",
      "Epoch 1029/2000\n",
      "318/318 [==============================] - 0s 30us/step - loss: 0.3712 - acc: 0.8648 - val_loss: 0.3633 - val_acc: 0.8250\n",
      "\n",
      "Epoch 01029: val_loss did not improve from 0.23566\n",
      "Epoch 1030/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.3611 - acc: 0.8491 - val_loss: 0.2707 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01030: val_loss did not improve from 0.23566\n",
      "Epoch 1031/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.3351 - acc: 0.8742 - val_loss: 0.2666 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01031: val_loss did not improve from 0.23566\n",
      "Epoch 1032/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2933 - acc: 0.8774 - val_loss: 0.2520 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01032: val_loss did not improve from 0.23566\n",
      "Epoch 1033/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.2865 - acc: 0.8774 - val_loss: 0.2500 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01033: val_loss did not improve from 0.23566\n",
      "Epoch 1034/2000\n",
      "318/318 [==============================] - 0s 32us/step - loss: 0.3112 - acc: 0.8899 - val_loss: 0.3100 - val_acc: 0.8625\n",
      "\n",
      "Epoch 01034: val_loss did not improve from 0.23566\n",
      "Epoch 1035/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.3211 - acc: 0.8553 - val_loss: 0.2461 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01035: val_loss did not improve from 0.23566\n",
      "Epoch 1036/2000\n",
      "318/318 [==============================] - 0s 31us/step - loss: 0.3074 - acc: 0.8868 - val_loss: 0.2498 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01036: val_loss did not improve from 0.23566\n",
      "Epoch 1037/2000\n",
      "318/318 [==============================] - 0s 35us/step - loss: 0.2853 - acc: 0.8774 - val_loss: 0.2436 - val_acc: 0.9125\n",
      "\n",
      "Epoch 01037: val_loss did not improve from 0.23566\n",
      "Epoch 1038/2000\n",
      "318/318 [==============================] - 0s 34us/step - loss: 0.2825 - acc: 0.8742 - val_loss: 0.2400 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01038: val_loss did not improve from 0.23566\n",
      "Epoch 1039/2000\n",
      "318/318 [==============================] - 0s 42us/step - loss: 0.2972 - acc: 0.8899 - val_loss: 0.2911 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01039: val_loss did not improve from 0.23566\n",
      "Epoch 1040/2000\n",
      "318/318 [==============================] - 0s 42us/step - loss: 0.3131 - acc: 0.8679 - val_loss: 0.2635 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01040: val_loss did not improve from 0.23566\n",
      "Epoch 1041/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3257 - acc: 0.8711 - val_loss: 0.3063 - val_acc: 0.8625\n",
      "\n",
      "Epoch 01041: val_loss did not improve from 0.23566\n",
      "Epoch 1042/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3244 - acc: 0.8742 - val_loss: 0.2579 - val_acc: 0.8625\n",
      "\n",
      "Epoch 01042: val_loss did not improve from 0.23566\n",
      "Epoch 1043/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.3186 - acc: 0.8742 - val_loss: 0.2775 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01043: val_loss did not improve from 0.23566\n",
      "Epoch 1044/2000\n",
      "318/318 [==============================] - 0s 42us/step - loss: 0.3016 - acc: 0.8774 - val_loss: 0.2349 - val_acc: 0.9125\n",
      "\n",
      "Epoch 01044: val_loss improved from 0.23566 to 0.23491, saving model to ./model/model.hdf5\n",
      "Epoch 1045/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.2866 - acc: 0.8868 - val_loss: 0.2375 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01045: val_loss did not improve from 0.23491\n",
      "Epoch 1046/2000\n",
      "318/318 [==============================] - 0s 54us/step - loss: 0.2796 - acc: 0.8868 - val_loss: 0.2501 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01046: val_loss did not improve from 0.23491\n",
      "Epoch 1047/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2841 - acc: 0.8774 - val_loss: 0.2397 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01047: val_loss did not improve from 0.23491\n",
      "Epoch 1048/2000\n",
      "318/318 [==============================] - 0s 37us/step - loss: 0.2937 - acc: 0.8868 - val_loss: 0.2730 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01048: val_loss did not improve from 0.23491\n",
      "Epoch 1049/2000\n",
      "318/318 [==============================] - 0s 33us/step - loss: 0.2977 - acc: 0.8868 - val_loss: 0.2423 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01049: val_loss did not improve from 0.23491\n",
      "Epoch 1050/2000\n",
      "318/318 [==============================] - 0s 47us/step - loss: 0.2969 - acc: 0.8868 - val_loss: 0.2589 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01050: val_loss did not improve from 0.23491\n",
      "Epoch 1051/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.2891 - acc: 0.8774 - val_loss: 0.2343 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01051: val_loss improved from 0.23491 to 0.23433, saving model to ./model/model.hdf5\n",
      "Epoch 1052/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.2823 - acc: 0.8868 - val_loss: 0.2385 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01052: val_loss did not improve from 0.23433\n",
      "Epoch 1053/2000\n",
      "318/318 [==============================] - 0s 34us/step - loss: 0.2789 - acc: 0.8868 - val_loss: 0.2401 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01053: val_loss did not improve from 0.23433\n",
      "Epoch 1054/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.2793 - acc: 0.8868 - val_loss: 0.2350 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01054: val_loss did not improve from 0.23433\n",
      "Epoch 1055/2000\n",
      "318/318 [==============================] - 0s 42us/step - loss: 0.2827 - acc: 0.8868 - val_loss: 0.2570 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01055: val_loss did not improve from 0.23433\n",
      "Epoch 1056/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2881 - acc: 0.8868 - val_loss: 0.2455 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01056: val_loss did not improve from 0.23433\n",
      "Epoch 1057/2000\n",
      "318/318 [==============================] - 0s 29us/step - loss: 0.2978 - acc: 0.8805 - val_loss: 0.2883 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01057: val_loss did not improve from 0.23433\n",
      "Epoch 1058/2000\n",
      "318/318 [==============================] - 0s 31us/step - loss: 0.3103 - acc: 0.8774 - val_loss: 0.2810 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01058: val_loss did not improve from 0.23433\n",
      "Epoch 1059/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3391 - acc: 0.8616 - val_loss: 0.3575 - val_acc: 0.8250\n",
      "\n",
      "Epoch 01059: val_loss did not improve from 0.23433\n",
      "Epoch 1060/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.3614 - acc: 0.8333 - val_loss: 0.3332 - val_acc: 0.8500\n",
      "\n",
      "Epoch 01060: val_loss did not improve from 0.23433\n",
      "Epoch 1061/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.3961 - acc: 0.8522 - val_loss: 0.3601 - val_acc: 0.8250\n",
      "\n",
      "Epoch 01061: val_loss did not improve from 0.23433\n",
      "Epoch 1062/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.3569 - acc: 0.8522 - val_loss: 0.2544 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01062: val_loss did not improve from 0.23433\n",
      "Epoch 1063/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3085 - acc: 0.8805 - val_loss: 0.2446 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01063: val_loss did not improve from 0.23433\n",
      "Epoch 1064/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.2858 - acc: 0.8868 - val_loss: 0.3162 - val_acc: 0.8625\n",
      "\n",
      "Epoch 01064: val_loss did not improve from 0.23433\n",
      "Epoch 1065/2000\n",
      "318/318 [==============================] - 0s 47us/step - loss: 0.3205 - acc: 0.8616 - val_loss: 0.2762 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01065: val_loss did not improve from 0.23433\n",
      "Epoch 1066/2000\n",
      "318/318 [==============================] - 0s 72us/step - loss: 0.3369 - acc: 0.8836 - val_loss: 0.2688 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01066: val_loss did not improve from 0.23433\n",
      "Epoch 1067/2000\n",
      "318/318 [==============================] - 0s 34us/step - loss: 0.2928 - acc: 0.8774 - val_loss: 0.2698 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01067: val_loss did not improve from 0.23433\n",
      "Epoch 1068/2000\n",
      "318/318 [==============================] - 0s 35us/step - loss: 0.2936 - acc: 0.8711 - val_loss: 0.2649 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01068: val_loss did not improve from 0.23433\n",
      "Epoch 1069/2000\n",
      "318/318 [==============================] - 0s 30us/step - loss: 0.3259 - acc: 0.8836 - val_loss: 0.2930 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01069: val_loss did not improve from 0.23433\n",
      "Epoch 1070/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3102 - acc: 0.8711 - val_loss: 0.2341 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01070: val_loss improved from 0.23433 to 0.23407, saving model to ./model/model.hdf5\n",
      "Epoch 1071/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2845 - acc: 0.8868 - val_loss: 0.2322 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01071: val_loss improved from 0.23407 to 0.23218, saving model to ./model/model.hdf5\n",
      "Epoch 1072/2000\n",
      "318/318 [==============================] - 0s 11us/step - loss: 0.2843 - acc: 0.8836 - val_loss: 0.2765 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01072: val_loss did not improve from 0.23218\n",
      "Epoch 1073/2000\n",
      "318/318 [==============================] - 0s 38us/step - loss: 0.3044 - acc: 0.8742 - val_loss: 0.2604 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01073: val_loss did not improve from 0.23218\n",
      "Epoch 1074/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.3232 - acc: 0.8774 - val_loss: 0.2922 - val_acc: 0.8625\n",
      "\n",
      "Epoch 01074: val_loss did not improve from 0.23218\n",
      "Epoch 1075/2000\n",
      "318/318 [==============================] - 0s 15us/step - loss: 0.3178 - acc: 0.8648 - val_loss: 0.2453 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01075: val_loss did not improve from 0.23218\n",
      "Epoch 1076/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.3051 - acc: 0.8742 - val_loss: 0.2491 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01076: val_loss did not improve from 0.23218\n",
      "Epoch 1077/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2872 - acc: 0.8774 - val_loss: 0.2315 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01077: val_loss improved from 0.23218 to 0.23148, saving model to ./model/model.hdf5\n",
      "Epoch 1078/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2792 - acc: 0.8931 - val_loss: 0.2306 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01078: val_loss improved from 0.23148 to 0.23064, saving model to ./model/model.hdf5\n",
      "Epoch 1079/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2826 - acc: 0.8868 - val_loss: 0.2579 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01079: val_loss did not improve from 0.23064\n",
      "Epoch 1080/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2901 - acc: 0.8774 - val_loss: 0.2377 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01080: val_loss did not improve from 0.23064\n",
      "Epoch 1081/2000\n",
      "318/318 [==============================] - 0s 45us/step - loss: 0.2935 - acc: 0.8836 - val_loss: 0.2538 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01081: val_loss did not improve from 0.23064\n",
      "Epoch 1082/2000\n",
      "318/318 [==============================] - 0s 44us/step - loss: 0.2864 - acc: 0.8805 - val_loss: 0.2332 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01082: val_loss did not improve from 0.23064\n",
      "Epoch 1083/2000\n",
      "318/318 [==============================] - 0s 34us/step - loss: 0.2796 - acc: 0.8931 - val_loss: 0.2332 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01083: val_loss did not improve from 0.23064\n",
      "Epoch 1084/2000\n",
      "318/318 [==============================] - 0s 33us/step - loss: 0.2799 - acc: 0.8899 - val_loss: 0.2517 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01084: val_loss did not improve from 0.23064\n",
      "Epoch 1085/2000\n",
      "318/318 [==============================] - 0s 34us/step - loss: 0.2848 - acc: 0.8805 - val_loss: 0.2354 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01085: val_loss did not improve from 0.23064\n",
      "Epoch 1086/2000\n",
      "318/318 [==============================] - 0s 41us/step - loss: 0.2873 - acc: 0.8899 - val_loss: 0.2496 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01086: val_loss did not improve from 0.23064\n",
      "Epoch 1087/2000\n",
      "318/318 [==============================] - 0s 39us/step - loss: 0.2837 - acc: 0.8774 - val_loss: 0.2332 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01087: val_loss did not improve from 0.23064\n",
      "Epoch 1088/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.2794 - acc: 0.8899 - val_loss: 0.2349 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01088: val_loss did not improve from 0.23064\n",
      "Epoch 1089/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.2780 - acc: 0.8868 - val_loss: 0.2430 - val_acc: 0.9125\n",
      "\n",
      "Epoch 01089: val_loss did not improve from 0.23064\n",
      "Epoch 1090/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2799 - acc: 0.8836 - val_loss: 0.2351 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01090: val_loss did not improve from 0.23064\n",
      "Epoch 1091/2000\n",
      "318/318 [==============================] - 0s 13us/step - loss: 0.2832 - acc: 0.8868 - val_loss: 0.2526 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01091: val_loss did not improve from 0.23064\n",
      "Epoch 1092/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2857 - acc: 0.8868 - val_loss: 0.2375 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01092: val_loss did not improve from 0.23064\n",
      "Epoch 1093/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.2878 - acc: 0.8899 - val_loss: 0.2553 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01093: val_loss did not improve from 0.23064\n",
      "Epoch 1094/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2879 - acc: 0.8836 - val_loss: 0.2375 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01094: val_loss did not improve from 0.23064\n",
      "Epoch 1095/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2881 - acc: 0.8868 - val_loss: 0.2533 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01095: val_loss did not improve from 0.23064\n",
      "Epoch 1096/2000\n",
      "318/318 [==============================] - 0s 43us/step - loss: 0.2859 - acc: 0.8868 - val_loss: 0.2351 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01096: val_loss did not improve from 0.23064\n",
      "Epoch 1097/2000\n",
      "318/318 [==============================] - 0s 34us/step - loss: 0.2836 - acc: 0.8868 - val_loss: 0.2451 - val_acc: 0.9125\n",
      "\n",
      "Epoch 01097: val_loss did not improve from 0.23064\n",
      "Epoch 1098/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.2799 - acc: 0.8836 - val_loss: 0.2353 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01098: val_loss did not improve from 0.23064\n",
      "Epoch 1099/2000\n",
      "318/318 [==============================] - 0s 36us/step - loss: 0.2775 - acc: 0.8868 - val_loss: 0.2366 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01099: val_loss did not improve from 0.23064\n",
      "Epoch 1100/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2770 - acc: 0.8931 - val_loss: 0.2431 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01100: val_loss did not improve from 0.23064\n",
      "Epoch 1101/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2785 - acc: 0.8805 - val_loss: 0.2352 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01101: val_loss did not improve from 0.23064\n",
      "Epoch 1102/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2802 - acc: 0.8899 - val_loss: 0.2473 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01102: val_loss did not improve from 0.23064\n",
      "Epoch 1103/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.2807 - acc: 0.8805 - val_loss: 0.2347 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01103: val_loss did not improve from 0.23064\n",
      "Epoch 1104/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2804 - acc: 0.8899 - val_loss: 0.2430 - val_acc: 0.9125\n",
      "\n",
      "Epoch 01104: val_loss did not improve from 0.23064\n",
      "Epoch 1105/2000\n",
      "318/318 [==============================] - 0s 35us/step - loss: 0.2791 - acc: 0.8836 - val_loss: 0.2337 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01105: val_loss did not improve from 0.23064\n",
      "Epoch 1106/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2781 - acc: 0.8868 - val_loss: 0.2380 - val_acc: 0.9125\n",
      "\n",
      "Epoch 01106: val_loss did not improve from 0.23064\n",
      "Epoch 1107/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2772 - acc: 0.8931 - val_loss: 0.2346 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01107: val_loss did not improve from 0.23064\n",
      "Epoch 1108/2000\n",
      "318/318 [==============================] - 0s 44us/step - loss: 0.2766 - acc: 0.8868 - val_loss: 0.2351 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01108: val_loss did not improve from 0.23064\n",
      "Epoch 1109/2000\n",
      "318/318 [==============================] - 0s 33us/step - loss: 0.2765 - acc: 0.8899 - val_loss: 0.2374 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01109: val_loss did not improve from 0.23064\n",
      "Epoch 1110/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2767 - acc: 0.8962 - val_loss: 0.2340 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01110: val_loss did not improve from 0.23064\n",
      "Epoch 1111/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2770 - acc: 0.8868 - val_loss: 0.2398 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01111: val_loss did not improve from 0.23064\n",
      "Epoch 1112/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2773 - acc: 0.8931 - val_loss: 0.2341 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01112: val_loss did not improve from 0.23064\n",
      "Epoch 1113/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2773 - acc: 0.8868 - val_loss: 0.2392 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01113: val_loss did not improve from 0.23064\n",
      "Epoch 1114/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2770 - acc: 0.8931 - val_loss: 0.2343 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01114: val_loss did not improve from 0.23064\n",
      "Epoch 1115/2000\n",
      "318/318 [==============================] - 0s 37us/step - loss: 0.2766 - acc: 0.8868 - val_loss: 0.2364 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01115: val_loss did not improve from 0.23064\n",
      "Epoch 1116/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2763 - acc: 0.8931 - val_loss: 0.2352 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01116: val_loss did not improve from 0.23064\n",
      "Epoch 1117/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2762 - acc: 0.8931 - val_loss: 0.2345 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01117: val_loss did not improve from 0.23064\n",
      "Epoch 1118/2000\n",
      "318/318 [==============================] - 0s 16us/step - loss: 0.2762 - acc: 0.8931 - val_loss: 0.2370 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01118: val_loss did not improve from 0.23064\n",
      "Epoch 1119/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2764 - acc: 0.8931 - val_loss: 0.2340 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01119: val_loss did not improve from 0.23064\n",
      "Epoch 1120/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2766 - acc: 0.8868 - val_loss: 0.2386 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01120: val_loss did not improve from 0.23064\n",
      "Epoch 1121/2000\n",
      "318/318 [==============================] - 0s 34us/step - loss: 0.2767 - acc: 0.8931 - val_loss: 0.2340 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01121: val_loss did not improve from 0.23064\n",
      "Epoch 1122/2000\n",
      "318/318 [==============================] - 0s 39us/step - loss: 0.2768 - acc: 0.8868 - val_loss: 0.2391 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01122: val_loss did not improve from 0.23064\n",
      "Epoch 1123/2000\n",
      "318/318 [==============================] - 0s 35us/step - loss: 0.2767 - acc: 0.8931 - val_loss: 0.2341 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01123: val_loss did not improve from 0.23064\n",
      "Epoch 1124/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2766 - acc: 0.8868 - val_loss: 0.2384 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01124: val_loss did not improve from 0.23064\n",
      "Epoch 1125/2000\n",
      "318/318 [==============================] - 0s 31us/step - loss: 0.2764 - acc: 0.8931 - val_loss: 0.2344 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01125: val_loss did not improve from 0.23064\n",
      "Epoch 1126/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.2763 - acc: 0.8868 - val_loss: 0.2371 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01126: val_loss did not improve from 0.23064\n",
      "Epoch 1127/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2761 - acc: 0.8931 - val_loss: 0.2349 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01127: val_loss did not improve from 0.23064\n",
      "Epoch 1128/2000\n",
      "318/318 [==============================] - 0s 43us/step - loss: 0.2759 - acc: 0.8931 - val_loss: 0.2356 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01128: val_loss did not improve from 0.23064\n",
      "Epoch 1129/2000\n",
      "318/318 [==============================] - 0s 34us/step - loss: 0.2759 - acc: 0.8931 - val_loss: 0.2354 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01129: val_loss did not improve from 0.23064\n",
      "Epoch 1130/2000\n",
      "318/318 [==============================] - 0s 42us/step - loss: 0.2758 - acc: 0.8931 - val_loss: 0.2347 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01130: val_loss did not improve from 0.23064\n",
      "Epoch 1131/2000\n",
      "318/318 [==============================] - 0s 12us/step - loss: 0.2758 - acc: 0.8931 - val_loss: 0.2362 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01131: val_loss did not improve from 0.23064\n",
      "Epoch 1132/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2758 - acc: 0.8962 - val_loss: 0.2346 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01132: val_loss did not improve from 0.23064\n",
      "Epoch 1133/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2759 - acc: 0.8868 - val_loss: 0.2371 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01133: val_loss did not improve from 0.23064\n",
      "Epoch 1134/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2759 - acc: 0.8962 - val_loss: 0.2346 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01134: val_loss did not improve from 0.23064\n",
      "Epoch 1135/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2759 - acc: 0.8868 - val_loss: 0.2379 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01135: val_loss did not improve from 0.23064\n",
      "Epoch 1136/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.2760 - acc: 0.8931 - val_loss: 0.2342 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01136: val_loss did not improve from 0.23064\n",
      "Epoch 1137/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2761 - acc: 0.8868 - val_loss: 0.2385 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01137: val_loss did not improve from 0.23064\n",
      "Epoch 1138/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.2762 - acc: 0.8931 - val_loss: 0.2336 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01138: val_loss did not improve from 0.23064\n",
      "Epoch 1139/2000\n",
      "318/318 [==============================] - 0s 14us/step - loss: 0.2763 - acc: 0.8868 - val_loss: 0.2389 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01139: val_loss did not improve from 0.23064\n",
      "Epoch 1140/2000\n",
      "318/318 [==============================] - 0s 33us/step - loss: 0.2764 - acc: 0.8931 - val_loss: 0.2332 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01140: val_loss did not improve from 0.23064\n",
      "Epoch 1141/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2766 - acc: 0.8868 - val_loss: 0.2401 - val_acc: 0.9125\n",
      "\n",
      "Epoch 01141: val_loss did not improve from 0.23064\n",
      "Epoch 1142/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.2769 - acc: 0.8899 - val_loss: 0.2332 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01142: val_loss did not improve from 0.23064\n",
      "Epoch 1143/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.2773 - acc: 0.8899 - val_loss: 0.2424 - val_acc: 0.9125\n",
      "\n",
      "Epoch 01143: val_loss did not improve from 0.23064\n",
      "Epoch 1144/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2777 - acc: 0.8899 - val_loss: 0.2336 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01144: val_loss did not improve from 0.23064\n",
      "Epoch 1145/2000\n",
      "318/318 [==============================] - 0s 17us/step - loss: 0.2784 - acc: 0.8899 - val_loss: 0.2453 - val_acc: 0.9125\n",
      "\n",
      "Epoch 01145: val_loss did not improve from 0.23064\n",
      "Epoch 1146/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2790 - acc: 0.8774 - val_loss: 0.2342 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01146: val_loss did not improve from 0.23064\n",
      "Epoch 1147/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.2799 - acc: 0.8899 - val_loss: 0.2483 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01147: val_loss did not improve from 0.23064\n",
      "Epoch 1148/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2805 - acc: 0.8805 - val_loss: 0.2350 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01148: val_loss did not improve from 0.23064\n",
      "Epoch 1149/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2818 - acc: 0.8868 - val_loss: 0.2521 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01149: val_loss did not improve from 0.23064\n",
      "Epoch 1150/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2828 - acc: 0.8805 - val_loss: 0.2365 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01150: val_loss did not improve from 0.23064\n",
      "Epoch 1151/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2848 - acc: 0.8868 - val_loss: 0.2570 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01151: val_loss did not improve from 0.23064\n",
      "Epoch 1152/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2860 - acc: 0.8868 - val_loss: 0.2389 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01152: val_loss did not improve from 0.23064\n",
      "Epoch 1153/2000\n",
      "318/318 [==============================] - 0s 20us/step - loss: 0.2887 - acc: 0.8868 - val_loss: 0.2625 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01153: val_loss did not improve from 0.23064\n",
      "Epoch 1154/2000\n",
      "318/318 [==============================] - 0s 31us/step - loss: 0.2894 - acc: 0.8868 - val_loss: 0.2407 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01154: val_loss did not improve from 0.23064\n",
      "Epoch 1155/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2915 - acc: 0.8868 - val_loss: 0.2639 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01155: val_loss did not improve from 0.23064\n",
      "Epoch 1156/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2899 - acc: 0.8868 - val_loss: 0.2392 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01156: val_loss did not improve from 0.23064\n",
      "Epoch 1157/2000\n",
      "318/318 [==============================] - 0s 38us/step - loss: 0.2893 - acc: 0.8868 - val_loss: 0.2570 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01157: val_loss did not improve from 0.23064\n",
      "Epoch 1158/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2853 - acc: 0.8868 - val_loss: 0.2347 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01158: val_loss did not improve from 0.23064\n",
      "Epoch 1159/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.2821 - acc: 0.8868 - val_loss: 0.2447 - val_acc: 0.9125\n",
      "\n",
      "Epoch 01159: val_loss did not improve from 0.23064\n",
      "Epoch 1160/2000\n",
      "318/318 [==============================] - 0s 22us/step - loss: 0.2786 - acc: 0.8805 - val_loss: 0.2325 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01160: val_loss did not improve from 0.23064\n",
      "Epoch 1161/2000\n",
      "318/318 [==============================] - 0s 24us/step - loss: 0.2763 - acc: 0.8868 - val_loss: 0.2353 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01161: val_loss did not improve from 0.23064\n",
      "Epoch 1162/2000\n",
      "318/318 [==============================] - 0s 30us/step - loss: 0.2752 - acc: 0.8931 - val_loss: 0.2350 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01162: val_loss did not improve from 0.23064\n",
      "Epoch 1163/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.2751 - acc: 0.8931 - val_loss: 0.2320 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01163: val_loss did not improve from 0.23064\n",
      "Epoch 1164/2000\n",
      "318/318 [==============================] - 0s 32us/step - loss: 0.2759 - acc: 0.8868 - val_loss: 0.2410 - val_acc: 0.9125\n",
      "\n",
      "Epoch 01164: val_loss did not improve from 0.23064\n",
      "Epoch 1165/2000\n",
      "318/318 [==============================] - 0s 44us/step - loss: 0.2773 - acc: 0.8836 - val_loss: 0.2326 - val_acc: 0.9125\n",
      "\n",
      "Epoch 01165: val_loss did not improve from 0.23064\n",
      "Epoch 1166/2000\n",
      "318/318 [==============================] - 0s 41us/step - loss: 0.2797 - acc: 0.8899 - val_loss: 0.2509 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01166: val_loss did not improve from 0.23064\n",
      "Epoch 1167/2000\n",
      "318/318 [==============================] - 0s 26us/step - loss: 0.2827 - acc: 0.8899 - val_loss: 0.2373 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01167: val_loss did not improve from 0.23064\n",
      "Epoch 1168/2000\n",
      "318/318 [==============================] - 0s 21us/step - loss: 0.2876 - acc: 0.8868 - val_loss: 0.2660 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01168: val_loss did not improve from 0.23064\n",
      "Epoch 1169/2000\n",
      "318/318 [==============================] - 0s 23us/step - loss: 0.2918 - acc: 0.8836 - val_loss: 0.2459 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01169: val_loss did not improve from 0.23064\n",
      "Epoch 1170/2000\n",
      "318/318 [==============================] - 0s 18us/step - loss: 0.2988 - acc: 0.8805 - val_loss: 0.2782 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01170: val_loss did not improve from 0.23064\n",
      "Epoch 1171/2000\n",
      "318/318 [==============================] - 0s 31us/step - loss: 0.2991 - acc: 0.8774 - val_loss: 0.2475 - val_acc: 0.8875\n",
      "\n",
      "Epoch 01171: val_loss did not improve from 0.23064\n",
      "Epoch 1172/2000\n",
      "318/318 [==============================] - 0s 19us/step - loss: 0.3005 - acc: 0.8805 - val_loss: 0.2686 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01172: val_loss did not improve from 0.23064\n",
      "Epoch 1173/2000\n",
      "318/318 [==============================] - 0s 28us/step - loss: 0.2916 - acc: 0.8836 - val_loss: 0.2357 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01173: val_loss did not improve from 0.23064\n",
      "Epoch 1174/2000\n",
      "318/318 [==============================] - 0s 35us/step - loss: 0.2837 - acc: 0.8868 - val_loss: 0.2417 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01174: val_loss did not improve from 0.23064\n",
      "Epoch 1175/2000\n",
      "318/318 [==============================] - 0s 27us/step - loss: 0.2768 - acc: 0.8836 - val_loss: 0.2364 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01175: val_loss did not improve from 0.23064\n",
      "Epoch 1176/2000\n",
      "318/318 [==============================] - 0s 25us/step - loss: 0.2752 - acc: 0.8931 - val_loss: 0.2316 - val_acc: 0.9000\n",
      "\n",
      "Epoch 01176: val_loss did not improve from 0.23064\n",
      "Epoch 1177/2000\n",
      "318/318 [==============================] - 0s 31us/step - loss: 0.2782 - acc: 0.8899 - val_loss: 0.2525 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01177: val_loss did not improve from 0.23064\n",
      "Epoch 1178/2000\n",
      "318/318 [==============================] - 0s 30us/step - loss: 0.2840 - acc: 0.8868 - val_loss: 0.2404 - val_acc: 0.8750\n",
      "\n",
      "Epoch 01178: val_loss did not improve from 0.23064\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, validation_split=0.2, epochs=2000, batch_size=500, \n",
    "                    callbacks=[checkpointer, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vloss = history.history['val_loss']\n",
    "y_acc = history.history['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAFyBJREFUeJzt3XuQXGWdxvHnN7dkSCJhwmi4hYCFlhHLAYcUI1Rqlig3sxgvRYFx0V22xkitpbtbFUjxh+VaxW0tl724OlPGXd0FxYXoStasspEppGa4TMItASIBkYuQDCYSCJDJTH77x3tOunvSM3N60j1z3uH7qerq7nNOv/2+5z3z9On3nOlj7i4AQDzqprsCAIDKENwAEBmCGwAiQ3ADQGQIbgCIDMENAJEhuAEgMgQ3AESG4AaAyDTUotBjjz3WFy9eXIuiAWBG2rx58yvu3ppl2ZoE9+LFizUwMFCLogFgRjKz32VdlqESAIgMwQ0AkSG4ASAyBDcARIbgBoDIENwAEJl8BXd/v3T99eEeAFBWTc7jnpT+fmn5cmloSGpqkjZtkjo6prtWAJA7+dnj7u0NoT0yEu57e6e7RgCQS/kJ7s7OsKddXx/uOzunu0YAkEv5GSrp6AjDI729IbQZJgGAsvIT3FIIawIbAMaVn6ESAEAmBDcARIbgBoDIENwAEBmCGwAiQ3ADQGQIbgCIDMENAJEhuAEgMgQ3AESG4AaAyBDcABAZghsAIkNwA0BkCG4AiAzBDQCRIbgBIDIENwBEJtOly8zsWUmvSRqRNOzu7bWsFABgbJVcc/JP3P2VmtUEAJAJQyUAEJmswe2Sfmlmm82sq5YVAgCML+tQybnu/qKZvVPSXWb2pLvfU7xAEuhdkrRo0aIqVxMAkMq0x+3uLyb3uyT9RNLSMsv0uHu7u7e3trZWt5YAgEMmDG4zm2Nm89LHks6XtLXWFQMAlJdlqORdkn5iZunyt7r7/9a0VgCAMU0Y3O7+jKQPTkFdAAAZcDogAESG4AaAyBDcABAZghsAIkNwA0BkCG4AiAzBDQCRIbgBIDIENwBEhuAGgMgQ3AAQGYIbACJDcANAZAhuAIgMwQ0AkSG4ASAyBDcARIbgBoDIENwAEBmCGwAiQ3ADQGQIbgCIDMENAJEhuAEgMgQ3AESG4AaAyBDcABCZzMFtZvVm9pCZbahlhQAA46tkj/vLkp6oVUUAANlkCm4zO1HSxyR9t7bVAQBMJOse982S1kg6WMO6AAAymDC4zWyFpF3uvnmC5brMbMDMBgYHB6tWQQBAqSx73OdIusTMnpX0I0nnmdl/jl7I3Xvcvd3d21tbW6tcTQBAasLgdve17n6iuy+WdJmkX7n7Z2teMwBAWZzHDQCRaahkYXfvldRbk5oAADJhjxsAIkNwA0BkCG4AiAzBDQCRIbgBIDIENwBEhuAGgMgQ3AAQGYIbACJDcANAZAhuAIgMwQ0AkSG4ASAyBDcARIbgBoDIENwAEBmCGwAiQ3ADQGQIbgCIDMENAJEhuAEgMgQ3AESG4AaAyBDcABAZghsAIkNwA0BkCG4AiMyEwW1ms83sATN7xMy2mdnXpqJiAIDyGjIss1/See7+upk1SrrXzDa6+301rhsAoIwJg9vdXdLrydPG5Oa1rBQAYGyZxrjNrN7MHpa0S9Jd7n5/basFABhLpuB29xF3b5N0oqSlZnb66GXMrMvMBsxsYHBwsNr1BAAkKjqrxN3/KOluSReWmdfj7u3u3t7a2lqt+gEARslyVkmrmc1PHjdL+qikJ2tdMQBAeVnOKjlO0vfNrF4h6H/s7htqWy0AwFiynFXyqKQzpqAuAIAM+M9JAIgMwQ0AkSG4ASAyBDcARIbgBoDIENwAEBmCGwAiQ3ADQGQIbgCIDMENAJEhuAEgMgQ3AESG4AaAyBDcABAZghsAIkNwA0BkCG4AiAzBDQCRIbgBIDIENwBEhuAGgMgQ3AAQGYIbACJDcANAZAhuAIgMwQ0AkSG4ASAyEwa3mZ1kZneb2eNmts3MvjwVFQMAlNeQYZlhSX/r7lvMbJ6kzWZ2l7s/XuO6AQDKmHCP291fcvctyePXJD0h6YRaVwwAUF5FY9xmtljSGZLur0VlAAATyxzcZjZX0h2SvuLue8vM7zKzATMbGBwcrGYdAQBFMgW3mTUqhPYt7r6+3DLu3uPu7e7e3traWs06AgCKZDmrxCStk/SEu3+z9lUCAIwnyx73OZL+TNJ5ZvZwcru4xvUCAIxhwtMB3f1eSTYFdQEAZMB/TgJAZAhuAIgMwQ0AkSG4ASAyBDcARIbgBoDIENwAEBmCGwAiQ3ADQGQIbgCIDMENAJEhuAEgMgQ3AESG4AaAyBDcABAZghsAIkNwA0BkCG4AiAzBDQCRIbgBIDIENwBEhuAGgMgQ3AAQGYIbACJDcANAZAhuAIgMwQ0AkZkwuM3se2a2y8y21rw2PT3SBReEewBAWQ0Zlvl3Sf8i6Qc1rUlPj/SFL4THv/xluO/qqulbAkCMJtzjdvd7JO2ueU3uuGP85wAASXka4/7Up8Z/DgCQlG2oJBMz65LUJUmLFi2qvIB0WOSOO0JoM0wCAGWZu0+8kNliSRvc/fQshba3t/vAwMCR1QwA3kbMbLO7t2dZtmp73Kit/n7pmmukBx+UhobCNPdwK2YWbu5SQ4PU2SkddZR0333S/v3SySdL+/ZJzz4b5p91lnTSSdKGDeF173mPtG2b9Oab4Xlrq3TFFdJvfiM99FAoY98+6cABacGCsNyrr4b3bmiQhofD46OOkubMCY9nz5ba2qSLLpI2bgx12bevUOc5c6SFC6U9e6TXXgtlNzZKTU1huf37Q12Gh6WDB0vbW1cX6rh8ubRlSyhj9mxp/vxCeW+9FdZHc3N4TVp++jiVTnvrrfBeTU3SmWdKZ58t3XlnKE8KdXrzzfDes2aFaSMjoV/S/jAL80eX8cILoQ3nnFPolz17wvulr5HCMgcPhvKK+9Q9LJOu36GhUJ+RkdC+k06SzjhDeuop6fjjQ3/edps0OBiWTdfXyEh43wMHSuvc0BBuBw8W+nfv3kI9mpvD+6brIJXOr6sr1D0ts7ExfIk+4QRp/XqppUXavj2UkS5XV1f62uLtOm2/FNbnWWeF9dnbW9rGl18O7XrHO6RFi0L7zMLjvXvDexb/7aRll/s7Gv23NLofil+bPpbC+rnqKunGGw8vr5om3OM2sx9K6pR0rKSdkr7q7uvGe0019rh7eqR168IfoRQ2vFmzpOeeC0GRrsiJOmD0yj+8ffl/bYYvRQByZM2aysO7qnvc7n55ZW9/ZNI9y3vuqU55E4XeePPz+loA+bZ+fW33unM1VNLfH77ap19nACBGn/xkbcvPVXD39paOOWJs6VjoWMMsIyNTX6epko4nzvRvJsVDgTPpvbJK+zm9H318oxrl19VlG84c/d7F4/nFr21ulr70pdqPcecquBcsmNzGM14H5GWc+khfaxbG+M86S7rhBqmjY/x1Ikmf/ax0++2hjJYWad68cKxg/nzp0kvDgbEHHwwhv2iRNHeu9OKL0vveF97jpz+VbrqptMzTTpOOOSaUsWVLeM3atdIHPhA+eDs7pccek26+ufRg3muvHV5OQ0No05494QCkVHpgcf/+UO8VK8K0zs7D293TI61eXboO580L942N4aDnihWFg6tmhfLnzCkcyBsaKj2Y1dIi7d4tbd0qvf56qGd6sDWt09694YDY7t3hgGZnZyh7wQLpD38I99/4Rig/tXBhKH/rVqm+PiyTHrh96KFQ3sKF4YBw2tb+/sK67egIz2+6qdCetrZwgO7OO6WdOws7P0NDhYN1p58uLVkSHt95Zziw2NYWxmKL12lPT+F/39L+vfTSQrs2bgzvO2dOYR1Iob6S9IMflD7v7Q3b0QMPFN5j9mzpxBOlj3yk0M7+/tLXltu+y/V1U1NYj0cfHQ5Yzpkj3XJLYX5LS7h3D9voqlWhb8ptSxMZ3Q8TTa+lTKcDVmqyByevv1669tqxw3vevLDxjA6KqVpZb0dZ/qCmspypLrsa0jB8O/97QrXCLUtfT0eQVkMlBydzFdyFMe7iOpnq6qRvf/vtu9EDmPkqCe78/Mu7wqdj7z8/piX2RDLFJLkuuYTQBoBUrsa4e3qkdX/Xot2jvgQsXDg99QGAPMpNcBd+1fX4oqmuxnrXFVfYNNUKAPInN8EdjmS7wvBIyvWxP62L6gADANRabsa4C7/i6sktWKiXpqM6AJBbuQnuri5p5eKHk2fhoGS9hnXF/1wWTjcBAEjKUXBL0pq1DWrWm6rTsBp0QP+qq9Qx/OtwUiYAQFKOxrglqeMDr2tT/QXqHTlXnepVh+6TmmaFk7sBAJJyFtzq7VWH96lD94bn73yn9PWvx/XvTwBQY7kaKun/4/t0/cE16tfZYcKuXeHHCa6+enorBgA5kps97v5+afk/fExDWqEmDWmTloehEvfCLx3V+ie3ACACudnj7u2VhkYaNKIGDalRveosXeCmm6RPfIIzTHC4/v7wC2VsG28P4/V3f7/0xS+G20zeHty96rcPfehDXqm+PvfmZvd6G/Zm7fM+nV185a7SW1OT+9y57suWhReidvr63K+7Lr/rOd1w6urcGxrcu7unu0aHy/s6jEl3t3tjY+jv5ubSddrXF7IhzYnGxqjWuaQBz5ixuQlu92T7Xv2s99WfO3Zol7uZudfXh840K73V1Y09b/T8+nr3hQvz+cc/Hfr63GfNCuu4vn7y66Wvz33lSvelS6u/bq+7rnRbqKur7nt0d7uff/7ky+zuDuvOLIRKX9+RlzmWvj731avDrZbvU6lq1aOvL6zL4r/7lSsLH4rXXRemFW8PK1dWpw1TINrgPmTp0sqCu1a3SkL/SD4wssxraHD/4Aez70GkYdnSEgJj4UL3VavCul25Msxfs8b96KPDt5e2tjA9/aN3D99oRq+TtrZQxqpV2f4Yu7sPL6OlJZS9enWYv2yZ+/z5oY7LloV6LFuWLejH2lba2grlT2Zvt7vb/eSTS8tcuLCw7rLo6wt9OLqM0cFSjb3Cvr6whznWtlzJt9Pu7tLt5EisWVNaj9NOm/wH+OrVY7evoSHUd3RwS+5LltT2w6tK36gqCe5c/R73IYVfnEI5E13yR6r+dZ4mUld3+GV7qrlt1dcf3t7JlG9W+rjcZYiylluuzcVll5s+lvr60vdOr003+tpY5eqdLpvlenUtLeESPMWXHSpW7nJFTU3hEkALFhQuH1R8KaH9+0svXZReWujee6Xf/W7suqSXQXrvewuX8Bnr0jz9/dKFFxYuuTMZ6ZVYRrehqUm68srxfzu6+LJD6XpraQntvPXWQh+tXHl43TOK9kIKJXp6Sq9/tXev9MYbR145APGY6guMlvtgL/fhOZ76eunXv644vKO9kEKJri7p8cell14Kt337pO5uaelSadmy8Knc3Bz2NurqwspK94CKb+PNK54PIH8q+dZSzfdzD4E9MlL5+4+MHH6x1irLzXncmXR11fZSOD090le/Gq6oe/Bgfq4WPJUbLoAj9/vf17T4uIK71mr9wXAkrr5a+ta3whigNHHoS+GbxPHHS+ecI23aFC5HPmtWmPfGG2G5xYvDpbx37Ah7CmZhDO/gwfBtpLVVWr48jFfu3h0unX7gQFjGTBoeHv9Dzkw66ijpqqukd787DH/t3Bkuiz40FJZtbpY+/GFp+/bwoTk8HOqe1sV97PbW10vvf790wQVhjHTPnlBuWn66TFrPcuuqXLl1daHeCxaENg8OhvUzNBTan9Zpon4wk449NowrP/JIKCMdD961q1DHiYw1Nj/VxzLyoqUlbI9jmTWr/Bj+VLnyypoWn98xbuDtIL0EfFtbCPP00uSVXKq8p0daty58SK9ZE6alr00fL1ggbdxYenBttPQDJT1Y9/TT0vr10qmnSs8/XzjeJBU+wBobw/LF9u0L75Ee0Gxrky66KLy3FA5+btgQymhoKAxHNDcXXp9+OEqFIc3Zs6Uzz5RuuKF0HW3bFu7TD7fPfCb8l3W6bltbpS1bCh/qo+u9b1/YkRnvg10KdZ07N7xu9uywM7BzZ2jnkiXSM8+E+WvXTmoHcGYcnASAt5GZcXASAFBWpuA2swvNbLuZ7TCza2pdKQDA2CYMbjOrl/QtSRdJWiLpcjNbUuuKAQDKy7LHvVTSDnd/xt2HJP1I0sdrWy0AwFiyBPcJkp4vev5CMq2EmXWZ2YCZDQwODlarfgCAUap2cNLde9y93d3bW1tbq1UsAGCULP+A86Kkk4qen5hMG9PmzZtfMbNxfl1mXMdKemWSr82jmdSemdQWifbk2Uxqi5StPSdnLWzC87jNrEHSbyQtVwjsByV9xt23ZX2TSpjZQNZzGWMwk9ozk9oi0Z48m0ltkarfngn3uN192Mz+StIvJNVL+l6tQhsAMLFMv1Xi7j+X9PMa1wUAkEEe/3OyZ7orUGUzqT0zqS0S7cmzmdQWqcrtqclvlQAAaiePe9wAgHHkJrhj/D0UMzvJzO42s8fNbJuZfTmZ3mJmd5nZU8n9Mcl0M7N/Str4qJmdOb0tOJyZ1ZvZQ2a2IXl+ipndn9T5NjNrSqbPSp7vSOYvns56l2Nm883sdjN70syeMLOOyPvmr5PtbKuZ/dDMZsfUP2b2PTPbZWZbi6ZV3B9m9rlk+afM7HPT0ZakHuXa8/fJ9vaomf3EzOYXzVubtGe7mV1QNL3y7Mt6VeFa3hTOVnla0qmSmiQ9ImnJdNcrQ72Pk3Rm8niewmmTSyTdJOmaZPo1km5MHl8saaMkk3S2pPunuw1l2vQ3km6VtCF5/mNJlyWPvyPpi8njqyR9J3l8maTbprvuZdryfUl/mTxukjQ/1r5R+G/l30pqLuqXz8fUP5KWSTpT0taiaRX1h6QWSc8k98ckj4/JUXvOl9SQPL6xqD1LklybJemUJO/qJ5t9075BJo3qkPSLoudrJa2d7npNoh3/LemjkrZLOi6Zdpyk7cnjbkmXFy1/aLk83BT+uWqTpPMkbUj+aF4p2hAP9ZPC6aEdyeOGZDmb7jYUteXoJOhs1PRY+yb96YmWZH1vkHRBbP0jafGooKuoPyRdLqm7aHrJctPdnlHzPiHpluRxSaal/TPZ7MvLUEmm30PJs+Sr6BmS7pf0Lnd/KZn1sqR3JY/z3s6bJa2RlF4KZIGkP7r7cPK8uL6H2pLMfzVZPi9OkTQo6d+SoZ/vmtkcRdo37v6ipG9Iek7SSwrre7Pi7Z9Upf2R634a5S8UvjVIVW5PXoI7amY2V9Idkr7i7nuL53n4GM39qTtmtkLSLnffPN11qZIGha+x33b3MyTtU/gqfkgsfSNJydjvxxU+kI6XNEfShdNaqSqLqT8mYmbXShqWdEstys9LcFf8eyh5YWaNCqF9i7uvTybvNLPjkvnHSdqVTM9zO8+RdImZPavw073nSfpHSfOTnz2QSut7qC3J/KMl/WEqKzyBFyS94O73J89vVwjyGPtGkj4i6bfuPujuByStV+izWPsnVWl/5L2fZGafl7RC0qrkw0iqcnvyEtwPSjotOULepHAw5WfTXKcJmZlJWifpCXf/ZtGsn0lKj3Z/TmHsO51+RXLE/GxJrxZ9TZxW7r7W3U9098UK6/9X7r5K0t2SPp0sNrotaRs/nSyfm70ld39Z0vNm9t5k0nJJjyvCvkk8J+lsMzsq2e7S9kTZP0Uq7Y9fSDrfzI5JvoWcn0zLBTO7UGG48RJ3f6No1s8kXZac7XOKpNMkPaDJZt90H6woGpS/WOGsjKclXTvd9clY53MVvto9Kunh5HaxwljiJklPSfo/SS3J8qZwNaGnJT0mqX262zBGuzpVOKvk1GQD2yHpvyTNSqbPTp7vSOafOt31LtOONkkDSf/8VOEshGj7RtLXJD0paauk/1A4QyGa/pH0Q4Xx+QMK34iunEx/KIwd70huf56z9uxQGLNO8+A7Rctfm7Rnu6SLiqZXnH385yQARCYvQyUAgIwIbgCIDMENAJEhuAEgMgQ3AESG4AaAyBDcABAZghsAIvP/ouLoLlFGaeYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x_len = np.arange(len(y_acc))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=3)\n",
    "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model/model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171/171 [==============================] - 0s 594us/step\n",
      "Accuracy: 0.9064\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.4f\" % (model.evaluate(X_test, Y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
